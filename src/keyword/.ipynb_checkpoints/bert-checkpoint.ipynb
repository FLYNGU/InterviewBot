{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import torch \n",
    "import jieba\n",
    "import json\n",
    "import os\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.set_device(0)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "# 确定模型训练方式，GPU训练或CPU训练\n",
    "parameter_copy = {\n",
    "    # 此处embedding维度为768\n",
    "    'd_model':768, \n",
    "    # rnn的隐层维度为300\n",
    "    'hid_dim':300,\n",
    "    # 训练的批次为100轮\n",
    "    'epoch':100,\n",
    "    # 单次训练的batch_size为100条数据\n",
    "    'batch_size':10,\n",
    "    # 设置序列的最大长度为100\n",
    "    'n_layers':2,\n",
    "    # 设置dropout，为防止过拟合\n",
    "    'dropout':0.1,\n",
    "    # 配置cpu、gpu\n",
    "    'device':device,\n",
    "    # 设置训练学习率\n",
    "    'lr':0.001,\n",
    "    # 优化器的参数，动量主要用于随机梯度下降\n",
    "    'momentum':0.99,\n",
    "}\n",
    "\n",
    "def build_dataSet(parameter):\n",
    "    data_name = ['train','dev']\n",
    "    data_set = {}\n",
    "    key_table = defaultdict(int)\n",
    "    vocab_table = defaultdict(int)\n",
    "    vocab_table['<PAD>'] = 0\n",
    "    vocab_table['<UNK>'] = 0\n",
    "    for i in data_name:\n",
    "        data_set[i] = []\n",
    "        data_src = open('data/'+i+'.json','r',encoding = 'utf-8').readlines()\n",
    "        for data in data_src:\n",
    "            data = json.loads(data)\n",
    "            text = list(data['text'])\n",
    "            label = data['label']\n",
    "            label_new = ['O']*len(text)\n",
    "            key_table['O']\n",
    "            for keys in label:\n",
    "                inds = label[keys].values()\n",
    "                for id_list in inds:\n",
    "                    for ind in id_list:\n",
    "                        if ind[1] - ind[0] == 0:\n",
    "                            keys_list = ['S-'+keys]\n",
    "                            label_new[ind[0]] = keys_list[0]\n",
    "                        if ind[1] - ind[0] == 1:\n",
    "                            keys_list = ['B-'+keys,'E-'+keys]\n",
    "                            label_new[ind[0]] = keys_list[0]\n",
    "                            label_new[ind[1]] = keys_list[1]\n",
    "                        if ind[1] - ind[0] > 1:\n",
    "                            keys_list = ['B-'+keys,'I-'+keys,'E-'+keys]\n",
    "                            label_new[ind[0]] = keys_list[0]\n",
    "                            label_new[ind[0]+1:ind[1]] = [keys_list[1]]*(ind[1]-1-ind[0])\n",
    "                            label_new[ind[1]] = keys_list[2]\n",
    "                        for key in keys_list:\n",
    "                            key_table[key] += 1\n",
    "            for j in text:\n",
    "                vocab_table[j] += 1\n",
    "            data_set[i].append([text,label_new])\n",
    "    key2ind = dict(zip(key_table.keys(),range(len(key_table))))\n",
    "    ind2key = dict(zip(range(len(key_table)),key_table.keys()))\n",
    "    word2ind = dict(zip(vocab_table.keys(),range(len(vocab_table))))\n",
    "    ind2word = dict(zip(range(len(vocab_table)),vocab_table.keys()))\n",
    "    parameter['key2ind'] = key2ind\n",
    "    parameter['ind2key'] = ind2key\n",
    "    parameter['word2ind'] = word2ind\n",
    "    parameter['ind2word'] = ind2word\n",
    "    parameter['data_set'] = data_set\n",
    "    parameter['output_size'] = len(key2ind)\n",
    "    parameter['word_size'] = len(word2ind)\n",
    "    return parameter\n",
    "\n",
    "\n",
    "def batch_yield_bert(parameter,shuffle = True,isTrain = True):\n",
    "    data_set = parameter['data_set']['train'] if isTrain else parameter['data_set']['dev']\n",
    "    Epoch = parameter['epoch'] if isTrain else 1\n",
    "    for epoch in range(Epoch):\n",
    "        # 每轮对原始数据进行随机化\n",
    "        if shuffle:\n",
    "            random.shuffle(data_set)\n",
    "        inputs,targets = [],[]\n",
    "        max_len = 0\n",
    "        for items in tqdm(data_set):\n",
    "            input = tokenizer.convert_tokens_to_ids(items[0])\n",
    "            target = itemgetter(*items[1])(parameter['key2ind'])\n",
    "            target = target if type(target) == type(()) else (target,0)\n",
    "            if len(input) > max_len:\n",
    "                max_len = len(input)\n",
    "            inputs.append(list(input))\n",
    "            targets.append(list(target))\n",
    "            if len(inputs) >= parameter['batch_size']:\n",
    "                inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n",
    "                targets = [i+[-1]*(max_len-len(i)) for i in targets]\n",
    "                yield list2torch(inputs),list2torch(targets),None,False\n",
    "                inputs,targets = [],[]\n",
    "                max_len = 0\n",
    "        inputs = [i+[0]*(max_len-len(i)) for i in inputs]\n",
    "        targets = [i+[0]*(max_len-len(i)) for i in targets]\n",
    "        yield list2torch(inputs),list2torch(targets),epoch,False\n",
    "        inputs,targets = [],[]\n",
    "        max_len = 0\n",
    "    yield None,None,None,True\n",
    "            \n",
    "\n",
    "def list2torch(ins):\n",
    "    return torch.from_numpy(np.array(ins))\n",
    "\n",
    "# 因此这边提前配置好用于训练的相关参数\n",
    "# 不要每次重新生成\n",
    "if not os.path.exists('parameter.pkl'):\n",
    "    parameter = parameter_copy\n",
    "    # 构建相关字典和对应的数据集\n",
    "    parameter = build_dataSet(parameter)\n",
    "    pk.dump(parameter,open('parameter.pkl','wb'))\n",
    "else:\n",
    "    # 读取已经处理好的parameter，但是考虑到模型训练的参数会发生变化，\n",
    "    # 因此此处对于parameter中模型训练参数进行替换\n",
    "    parameter = pk.load(open('parameter.pkl','rb'))\n",
    "    for i in parameter_copy.keys():\n",
    "        if i not in parameter:\n",
    "            parameter[i] = parameter_copy[i]\n",
    "            continue\n",
    "        if parameter_copy[i] != parameter[i]:\n",
    "            parameter[i] = parameter_copy[i]\n",
    "    for i in parameter_copy.keys():\n",
    "        print(i,':',parameter[i])\n",
    "    pk.dump(parameter,open('parameter.pkl','wb'))\n",
    "    del parameter_copy,i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, BertConfig,get_linear_schedule_with_warmup,AdamW, BertTokenizer\n",
    "from transformers import BertModel,BertPreTrainedModel\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch.nn.functional as F # pytorch 激活函数的类\n",
    "from torch import nn,optim # 构建模型和优化器\n",
    "from torchcrf import CRF\n",
    "\n",
    "class bert(BertPreTrainedModel):\n",
    "    def __init__(self, config,parameter):\n",
    "        super(bert, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        embedding_dim = parameter['d_model']\n",
    "        output_size = parameter['output_size']\n",
    "        self.fc = nn.Linear(embedding_dim, output_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "        self.crf = CRF(output_size,batch_first=True)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,labels=None):\n",
    "        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.fc(sequence_output)\n",
    "        return logits\n",
    "    \n",
    "config_class, model_class, tokenizer_class = BertConfig, bert, BertTokenizer\n",
    "config = config_class.from_pretrained(\"prev_trained_model\")\n",
    "tokenizer = tokenizer_class.from_pretrained(\"prev_trained_model\")\n",
    "model = model_class.from_pretrained(\"prev_trained_model\",config=config,parameter = parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle as pk\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "model = bert(config,parameter).to(parameter['device'])\n",
    "\n",
    "# 确定训练模式\n",
    "model.train()\n",
    "\n",
    "# 确定优化器和损失\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=3*10**-5, momentum=0.95, nesterov=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5, betas=(0.9, 0.999), eps=1e-6, \\\n",
    "                             weight_decay = 0)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "# 准备学习率策略\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# 准备迭代器\n",
    "train_yield = batch_yield_bert(parameter,tokenizer)\n",
    "\n",
    "# 开始训练\n",
    "loss_cal = []\n",
    "min_loss = float('inf')\n",
    "logging_steps = 0\n",
    "while 1:\n",
    "        inputs,targets,epoch,keys = next(train_yield)\n",
    "        if keys:\n",
    "            break\n",
    "        out = model(inputs.long().to(parameter['device']))\n",
    "        loss = criterion(out, targets.view(-1).long().to(parameter['device']))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_cal.append(loss.item())\n",
    "        logging_steps += 1\n",
    "        if logging_steps%100 == 0:\n",
    "            print(sum(loss_cal)/len(loss_cal))\n",
    "        if epoch is not None:\n",
    "            if (epoch+1)%1 == 0:\n",
    "                loss_cal = sum(loss_cal)/len(loss_cal)\n",
    "                if loss_cal < min_loss:\n",
    "                    min_loss = loss_cal\n",
    "                    torch.save(model.state_dict(), 'bert.h5')\n",
    "                print('epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, \\\n",
    "                                                       parameter['epoch'],loss_cal))\n",
    "            loss_cal = [loss.item()]\n",
    "            scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
