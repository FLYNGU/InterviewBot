实体1,实体2,关系,,,,,,,,,,,
AutoML问题构成?,特征选择,question2answer,,,,,,,,,,,
AutoML问题构成?,模型选择,question2answer,,,,,,,,,,,
AutoML问题构成?,算法选择,question2answer,,,,,,,,,,,
特征工程选择思路？,有监督的特征选择,question2answer,,,,,,,,,,,
特征工程选择思路？,基于模型，lr的系数，树模型的importance等等,question2answer,,,,,,,,,,,
特征工程选择思路？,基于选择，前项后项选择,question2answer,,,,,,,,,,,
特征工程选择思路？,无监督的特征选择,question2answer,,,,,,,,,,,
特征工程选择思路？,基于统计信息的，熵、相关性、KL系数,question2answer,,,,,,,,,,,
特征工程选择思路？,基于方差，因子分解，PCA主成分分享，方差系数,question2answer,,,,,,,,,,,
模型相关的选择思路?,模型选择,question2answer,,,,,,,,,,,
模型相关的选择思路?,各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,question2answer,,,,,,,,,,,
模型相关的选择思路?,参数选择,question2answer,,,,,,,,,,,
模型相关的选择思路?,grid_search,question2answer,,,,,,,,,,,
模型相关的选择思路?,random_search,question2answer,,,,,,,,,,,
模型相关的选择思路?,...,question2answer,,,,,,,,,,,
常见优化算法思路？,SGD,question2answer,,,,,,,,,,,
常见优化算法思路？,GD,question2answer,,,,,,,,,,,
常见优化算法思路？,LBFGS,question2answer,,,,,,,,,,,
常见优化算法思路？,FTRL,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,暴力搜索,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,grid_search,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,random_search,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,拟合搜索,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,贝叶斯优化,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,其他方法,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,Meta学习,question2answer,,,,,,,,,,,
AutoML参数选择所使用的方法？,转移学习,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,步骤：,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,随机选取几个超参数进行f拟合，得到先验数据集合D,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,根据先验数据D得到模型M,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,循环23两步直至达到条件,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,问题：,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,稳定性：同一组超参数的预测结果在不同轮次不一致,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,f函数需要多次计算，资源耗费时间损失,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,难以确定比较通用的拟合模型f,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,手记：,question2answer,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9n45r6f3nj30q40tkjz8.jpg),question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,随机生成若干超参点，更新gp模型,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,根据gp模型选取最优推荐值,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,推荐值附近随机生成点，根据cquisitionfunction选取附近点极值点，acquisitionfunction通常：,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,基于GPUCB的最大置信上界,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,基于均值和方差的平衡结果,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,ThompsonSampling,question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,EI(期望提升),question2answer,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,重复以上步骤,question2answer,,,,,,,,,,,
写出全概率公式&贝叶斯公式,全概率公式：设事件![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wed60nzaj305i01cmx2.jpg)构成一个完备事件组，即它们两两不相容，和为全集且![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wedhjqtej304w01cjra.jpg)，则对任一事件A有：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8weetxaqxj30dk01e74b.jpg),question2answer,,,,,,,,,,,
写出全概率公式&贝叶斯公式,贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg),question2answer,,,,,,,,,,,
说说你怎么理解为什么有全概率公式&贝叶斯公式,全概率公式为全概率就是表示达到某个目的，有多种方式，算到达目的的概率。**key：算概率**,question2answer,,,,,,,,,,,
说说你怎么理解为什么有全概率公式&贝叶斯公式,贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**,question2answer,,,,,,,,,,,
什么是先验概率,先验概率（priorprobability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计,question2answer,,,,,,,,,,,
什么是后验概率,后验概率（posteriorprobability）：指某件事已经发生，想要计算这件事发生的原因是由某个因素引起的概率。key：条件概率,question2answer,,,,,,,,,,,
经典概率题,有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？,question2answer,,,,,,,,,,,
经典概率题,P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。,question2answer,,,,,,,,,,,
经典概率题,P[i]=(i/M)*P[i]+(1i/M)*P[i+1]+1,question2answer,,,,,,,,,,,
经典概率题,解释一下，每一次抽取，(i/M)概率不变，(1i/M)进入下一轮，额外加一次本次操作,question2answer,,,,,,,,,,,
解释方差：,期望值与真实值之间的波动程度，衡量的是**稳定性**,question2answer,,,,,,,,,,,
解释偏差：,期望值与真实值之间的一致差距，衡量的是**准确性**,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,**Err=bias+var+irreducibleerror**,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,"以回归任务为例,其实更准确的公式为：**Err=bias^2+var+irreducibleerror^2**",question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,符号的定义：一个真实的任务可以理解为Y=f(x)+e，其中f(x)为规律部分，e为噪声部分,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg),question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg),question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,"Err(x)=Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))+Err(f,Y)",question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,"Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差",question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,"Err(f,Y)为噪声e部分，即为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxc64l5yj300g00b0pn.jpg)",question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxyj2g67j301k00mmwx.jpg)可推导如下：,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxkvx39gj30gh01b3yj.jpg),question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)中![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxm8gzmrj301z00mq2p.jpg)为常数。所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxlzjp9hj305s00mt8j.jpg)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxo2v9ozj30bw00m0sn.jpg)=0,question2answer,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,Err(x)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lxvudtfgj30cm00mjra.jpg),question2answer,,,,,,,,,,,
什么情况下引发高方差？,过高复杂度的模型，对训练集进行过拟合,question2answer,,,,,,,,,,,
什么情况下引发高方差？,带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差,question2answer,,,,,,,,,,,
什么情况下引发高方差？,更加形象的理解就是用一条高次方程去拟合线性数据,question2answer,,,,,,,,,,,
如何解决高方差问题？,在模型复杂程度不变的情况下，增加更多数据,question2answer,,,,,,,,,,,
如何解决高方差问题？,在数据量不变的情况下，减少特征维度,question2answer,,,,,,,,,,,
如何解决高方差问题？,在数据和模型都不变的情况下，加入正则化,question2answer,,,,,,,,,,,
以上方法是否一定有效？,增加数据如果和原数据分布一致，无论增加多少必定解决不了高方差,question2answer,,,,,,,,,,,
以上方法是否一定有效？,smote对样本进行扩充是否必定可以避免高方差？,question2answer,,,,,,,,,,,
以上方法是否一定有效？,过采样是否解决高方差问题？,question2answer,,,,,,,,,,,
以上方法是否一定有效？,减少的特征维度如果是共线性的维度，对原模型没有任何影响,question2answer,,,,,,,,,,,
以上方法是否一定有效？,罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,question2answer,,,,,,,,,,,
以上方法是否一定有效？,正则化通常都是有效的,question2answer,,,,,,,,,,,
如何解决高偏差问题？,尝试获得更多的特征,question2answer,,,,,,,,,,,
如何解决高偏差问题？,从数据入手，进行特征交叉，或者特征的embedding化,question2answer,,,,,,,,,,,
如何解决高偏差问题？,尝试增加多项式特征,question2answer,,,,,,,,,,,
如何解决高偏差问题？,从模型入手，增加更多线性及非线性变化，提高模型的复杂度,question2answer,,,,,,,,,,,
如何解决高偏差问题？,尝试减少正则化程度λ,question2answer,,,,,,,,,,,
以上方法是否一定有效？,特征越稀疏，高方差的风险越高,question2answer,,,,,,,,,,,
以上方法是否一定有效？,多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换,question2answer,,,,,,,,,,,
以上方法是否一定有效？,正则化通常都是有效的,question2answer,,,,,,,,,,,
遇到过的机器学习中的偏差与方差问题？,从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,question2answer,,,,,,,,,,,
遇到过的机器学习中的偏差与方差问题？,从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,基础：,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Bagging,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyp35ynkj308l00qglh.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,其中,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lypqw1tjj300d00c0q9.jpg)可以直接提取出来,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyuaep07j308v00ya9x.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,所以，化简以上的式子可得：Var(F)=m*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg)*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,E(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz97enuuj3059011743.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,结论：,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Boosting同理,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,boosting的前提是弱模型之间高度相关，我们不妨设相关度为1,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzj06hv1j304300qwea.jpg),question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,结论：,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。,question2answer,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,GradientBoostingDecisionTree为典型例子,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,dropout,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,dense中的normalization,question2answer,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,数据的shuffle,question2answer,,,,,,,,,,,
方差、偏差与模型的复杂度之间的关系？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8ly6pdyouj30dn07eq38.jpg),question2answer,,,,,,,,,,,
什么叫生成模型？,"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",question2answer,,,,,,,,,,,
什么叫生成模型？,说白了就是玩的一手x在不同类别下出现的概率+不同类别的概率+x出现的概率进行复合计算，复合的时候考虑独立性的问题。,question2answer,,,,,,,,,,,
什么叫判别模型？,求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,所以这个问题更想问的是什么时候要去用生成模型：,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,一般会追问，如何构造？,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,FM/FFM,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,NeuralNetwork,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,线性Dense,question2answer,,,,,,,,,,,
什么时候会选择生成/判别模型？,非线性激活,question2answer,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,question2answer,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,question2answer,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率,question2answer,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,判别式模型：最大熵模型，CRF,question2answer,,,,,,,,,,,
我的理解：,无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,question2answer,,,,,,,,,,,
我的理解：,算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型,question2answer,,,,,,,,,,,
我的理解：,生成模型得分布，判别模型得最优划分,question2answer,,,,,,,,,,,
我的理解：,生成模型可以得到判别模型，反之不成立,question2answer,,,,,,,,,,,
我的理解：,生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,question2answer,,,,,,,,,,,
极大似然估计 - MLE,原理：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值。,question2answer,,,,,,,,,,,
极大似然估计 - MLE,似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,question2answer,,,,,,,,,,,
最大后验估计 - MAP,"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",question2answer,,,,,,,,,,,
极大似然估计与最大后验概率的区别？,最大似然估计中的采样满足所有采样都是独立同分布的假设,question2answer,,,,,,,,,,,
极大似然估计与最大后验概率的区别？,最大后验概率在考虑了p(X/θ)的同时，还考虑了p(θ),question2answer,,,,,,,,,,,
到底什么是似然什么是概率估计？,似然：给定了x求θ真实的可能性,question2answer,,,,,,,,,,,
到底什么是似然什么是概率估计？,概率估计：给定了θ，X=x的可能性,question2answer,,,,,,,,,,,
DNN与DeepFM之间的区别?,DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,question2answer,,,,,,,,,,,
Wide&Deep与DeepFM之间的区别?,DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,question2answer,,,,,,,,,,,
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,question2answer,,,,,,,,,,,
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,question2answer,,,,,,,,,,,
DeepFM怎么优化的？,embedding向量可以通过FM初始化,question2answer,,,,,,,,,,,
DeepFM怎么优化的？,Deep层可以做优化,question2answer,,,,,,,,,,,
DeepFM怎么优化的？,NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致,question2answer,,,,,,,,,,,
DeepFM怎么优化的？,FM层可以变得交叉更多阶,question2answer,,,,,,,,,,,
DeepFM怎么优化的？,XDeepFM,question2answer,,,,,,,,,,,
不定长文本数据如何输入deepFM？,截断补齐,question2answer,,,,,,,,,,,
不定长文本数据如何输入deepFM？,结合文本id+文本长度，在做文本处理之前，先做不等长的sum_pooled的操作,question2answer,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",question2answer,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",question2answer,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,文本项目上也可以用预训练好的特征,question2answer,,,,,,,,,,,
主要使用了什么机制?,Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。,question2answer,,,,,,,,,,,
activation unit的作用,基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,question2answer,,,,,,,,,,,
activation unit的作用,![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15ugjlkmj308901imx1.jpg),question2answer,,,,,,,,,,,
activation unit的作用,通常用户兴趣可以由历史行为(点击/浏览/收藏)等合并得到，及![](https://tva1.sinaimg.cn/large/006tNbRwgy1ga15vxtcu8j302w01imwy.jpg),question2answer,,,,,,,,,,,
activation unit的作用,activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,question2answer,,,,,,,,,,,
DICE怎么设计的,先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,question2answer,,,,,,,,,,,
DICE怎么设计的,"x_p=tf.sigmoid(tf.layers.batch_normalization(x,center=False,scale=False,training=True))",question2answer,,,,,,,,,,,
DICE怎么设计的,aplha*(1x_p)*x+x_p*x,question2answer,,,,,,,,,,,
DICE使用的过程中，有什么需要注意的地方,在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,question2answer,,,,,,,,,,,
DICE使用的过程中，有什么需要注意的地方,test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),question2answer,,,,,,,,,,,
选用的原因？,类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,question2answer,,,,,,,,,,,
选用的原因？,向量级别的特征交互而不是元素级交互,question2answer,,,,,,,,,,,
选用的原因？,经验上，vectorwise的方式构建的特征交叉关系比bitwise的方式更容易学习,question2answer,,,,,,,,,,,
选用的原因？,我也不知道具体好在哪，如果有大佬会可以指导一下，感恩,question2answer,,,,,,,,,,,
选用的原因？,"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",question2answer,,,,,,,,,,,
选用的原因？,"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",question2answer,,,,,,,,,,,
选用的原因？,思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,question2answer,,,,,,,,,question2answer,,
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,显示是可以写出feature交互的公式，隐式相反,question2answer,,,,,,,,,batch,"output_layer]""",question2answer
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,元素级是以feature值交互，向量级是feature向量级点乘处理,question2answer,,,,,,,,,,,
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,高阶特征交互：DNN,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,这样的网络结构保证来来自X0的1，2，3...N阶的特征组合,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,流程概述：,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,featureembedding,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,CIN中：,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"获取上一次的layerout：X0，并进行切分：embedding`*`\[batch,field,1]",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,加偏置项，并进行激活函数处理，完成一轮处理,question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",question2answer,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,question2answer,,,,,,,,,,,
和DCN比，有哪些核心的变化？,DCN是bitwise的，而CIN是vectorwise的,question2answer,,,,,,,,,,,
和DCN比，有哪些核心的变化？,DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,question2answer,,,,,,,,,,,
和DCN比，有哪些核心的变化？,![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)和![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg)差异的![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9in06u4m6j300j00f0sh.jpg)导致的,question2answer,,,,,,,,,,,
时间复杂度多少？,假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,question2answer,,,,,,,,,,,
时间复杂度多少？,CIN:O(m`*`L`*`H`*`H),question2answer,,,,,,,,,,,
时间复杂度多少？,DNN:O(m`*`D`*`H+L`*`H`*`H),question2answer,,,,,,,,,,,
变长数据如何处理的？,input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的historylength,question2answer,,,,,,,,,,,
变长数据如何处理的？,20次点击过去一周内的行为，曾经尝试扩大历史点击次数到40，60没有很明显的效果提升,question2answer,,,,,,,,,,,
变长数据如何处理的？,点击行为是处理过的，停留时间过短的click不要,question2answer,,,,,,,,,,,
变长数据如何处理的？,点击行为是处理过的，连续多次的重复点击会去重,question2answer,,,,,,,,,,,
变长数据如何处理的？,点击行为是处理过的，session内的点击次数需要在约定范围内,question2answer,,,,,,,,,,,
input是怎么构造的,最近历史20次点击商品id/文章id，如果不足不需要补充,question2answer,,,,,,,,,,,
input是怎么构造的,最近历史20次点击商品id对应的品牌/文章id对应的类目，如果不足不需要补充,question2answer,,,,,,,,,,,
input是怎么构造的,最近历史20次点击商品id对应的类别/文章id对应的栏目，如果不足不需要补充,question2answer,,,,,,,,,,,
input是怎么构造的,最后一次点击商品id/文章id,question2answer,,,,,,,,,,,
input是怎么构造的,历史上最高频的商品id/文章id,question2answer,,,,,,,,,,,
input是怎么构造的,exampleage,question2answer,,,,,,,,,,,
input是怎么构造的,user_info:age/gender/地理位置/注册时长,question2answer,,,,,,,,,,,
input是怎么构造的,cross_info:最后一次点击距click时间，最后一次点击商品浏览次数,question2answer,,,,,,,,,,,
input是怎么构造的,phone_info:设备信息，登录状态,question2answer,,,,,,,,,,,
最后一次点击实际如何处理的？,我们会以日进行切分，每日首次点击的lastclick会以\[unknow]进行替代，隔日的点击不会进行计算,question2answer,,,,,,,,,,,
output的是时候train和predict如何处理的,train的时候是进行负采样的,question2answer,,,,,,,,,,,
output的是时候train和predict如何处理的,predict的时候是进行的all_embeddingdot,question2answer,,,,,,,,,,,
如何进行负采样的？,该次点击时间之前所以的item或者article作为候选集,question2answer,,,,,,,,,,,
如何进行负采样的？,负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样,question2answer,,,,,,,,,,,
如何进行负采样的？,均衡采样，不会根据其他样本showtime进行加权,question2answer,,,,,,,,,,,
如何进行负采样的？,为了尽可能多的修正全量样本，尽快达到收敛,question2answer,,,,,,,,,,,
如何进行负采样的？,为了避免其他推荐产生的交叉影响,question2answer,,,,,,,,,,,
item向量在softmax的时候你们怎么选择的？,是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,question2answer,,,,,,,,,,,
Example Age的理解？,官方：upload_timeclick_time,question2answer,,,,,,,,,,,
Example Age的理解？,希望更倾向于新上视频,question2answer,,,,,,,,,,,
Example Age的理解？,民间：click_timenow,question2answer,,,,,,,,,,,
Example Age的理解？,希望平衡样本构造时间对当前的影响,question2answer,,,,,,,,,,,
什么叫做不对称的共同浏览（asymmetric co-watch）问题？,item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,question2answer,,,,,,,,,,,
为什么不采取类似RNN的Sequence model？,在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,question2answer,,,,,,,,,,,
YouTube如何避免百万量级的softmax问题的？,负采样,question2answer,,,,,,,,,,,
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度,question2answer,,,,,,,,,,,
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,exampleage,question2answer,,,,,,,,,,,
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,不对称的共同浏览问题，避免引入futureinformation，产生与事实不符的数据穿越,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,embedding,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,引入了doc2vec做init,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,权重共享，没有在softmax处重新构造,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,负采样,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,限定采样集合在click时间发生之前已经有的item,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,剔除该次点击click时同时展现的其他item,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,均衡采样,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,加快收敛,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,避免热门商品item的过度影响,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,click数据的预处理,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,historyclick,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,停留时间过短的click不要,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,连续多次的重复点击会去重,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,session内的点击次数需要在约定范围内,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,lastclick,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,每日初次点击的lastclick以\[unknown]替代，不做隔日的数据连接,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,在history引入multiheadattention,question2answer,,,,,,,,,,,
整个过程中有什么亮点？有哪些决定性的提升？,ctr提高了，但是有效点击没有变,question2answer,,,,,,,,,,,
辗转相除法,```python,question2answer,,,,,,,,,,,
辗转相除法,"defsolve(a,b):",question2answer,,,,,,,,,,,
辗转相除法,"returnaifb==0elsesolve(b,a%b)",question2answer,,,,,,,,,,,
辗转相除法,```,question2answer,,,,,,,,,,,
其他方法,穷举法,question2answer,,,,,,,,,,,
其他方法,辗转相减法,question2answer,,,,,,,,,,,
四则运算,(u+v)'=u'+v',question2answer,,,,,,,,,,,
四则运算,(uv)'=u'v',question2answer,,,,,,,,,,,
四则运算,(uv)'=u'v+uv',question2answer,,,,,,,,,,,
四则运算,(u/v)'=(u'vuv')/v^2,question2answer,,,,,,,,,,,
常见导数,"y=c(常数),y'=0",question2answer,,,,,,,,,,,
常见导数,"y=pow(x,a),y'=a・pow(x,a1)",question2answer,,,,,,,,,,,
常见导数,"y=pow(a,x),y'=pow(a,x)・ln(a)",question2answer,,,,,,,,,,,
常见导数,"y=log(a,x),y'=1/(xlna);特别的ln(x)=1/x",question2answer,,,,,,,,,,,
常见导数,"y=sin(x),y'=cos(x)",question2answer,,,,,,,,,,,
常见导数,"y=cos(x),y'=sin(x)",question2answer,,,,,,,,,,,
常见导数,"y=tan(x),y'=1/(cos(x)^2)",question2answer,,,,,,,,,,,
复合函数的运算法则,"若y=f(g(x)),y'=f'(g(x))・g'(x),前提是g在x处可导，f在g(x)处可导",question2answer,,,,,,,,,,,
莱布尼兹公式,"若u(x),v(x)均n阶可导，则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xl2xwg4ij307000udfv.jpg)",question2answer,,,,,,,,,,,
切线方程,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xko8hysoj306c0123yi.jpg),question2answer,,,,,,,,,,,
法线方程,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8xkomfj6lj309c01amx9.jpg),question2answer,,,,,,,,,,,
费马定理,f在x0的邻域内有定义，且恒满足：f(x)<=f(x0)｜f(x)>=f(x0),question2answer,,,,,,,,,,,
费马定理,f在x0处可导，则满足f'(x0)=0,question2answer,,,,,,,,,,,
拉格朗日中值定理,设函数f(x)满足条件：,question2answer,,,,,,,,,,,
拉格朗日中值定理,"\[a,b]上连续",question2answer,,,,,,,,,,,
拉格朗日中值定理,"\(a,b)内可导，则\(a,b)存在ζ，使得f(b)f(a)=f'(ζ)(ba)",question2answer,,,,,,,,,,,
柯西中值定理,"设函数f(x),g(x)满足条件：",question2answer,,,,,,,,,,,
柯西中值定理,"\[a,b]上连续",question2answer,,,,,,,,,,,
柯西中值定理,"\(a,b)内可导，且f'(x)和g'(x)存在，且g'(x)!=0",question2answer,,,,,,,,,,,
柯西中值定理,"则\(a,b)存在ζ，使得(f(b)f(a))g'(ζ)=f'(ζ)(g(b)g(a))",question2answer,,,,,,,,,,,
期望,离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,question2answer,,,,,,,,,,,
期望,若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,question2answer,,,,,,,,,,,
期望,E(ax+by+c)=aE(x)+bE(y)+c,question2answer,,,,,,,,,,,
期望,如果x和y独立，E(xy)=E(x)E(y),question2answer,,,,,,,,,,,
方差,方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,question2answer,,,,,,,,,,,
方差,方差刻画了随机变量的取值对于其数学期望的离散程度。,question2answer,,,,,,,,,,,
方差,方差深入：,question2answer,,,,,,,,,,,
方差,很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,question2answer,,,,,,,,,,,
方差,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zkdv6ukj308700kweb.jpg),question2answer,,,,,,,,,,,
方差,如果x和y独立，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zl5np3yj308600kt8j.jpg),question2answer,,,,,,,,,,,
标准差,标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,question2answer,,,,,,,,,,,
协方差,协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,question2answer,,,,,,,,,,,
协方差,在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。,question2answer,,,,,,,,,,,
协方差,"Cov(x,y)=E((xE(x))(yE(y)))",question2answer,,,,,,,,,,,
协方差,"Cov(c+ax,d+by)=abCov(x,y)",question2answer,,,,,,,,,,,
相关系数,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zq1gbe7j306601ct8k.jpg),question2answer,,,,,,,,,,,
相关系数,"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",question2answer,,,,,,,,,,,
均匀分布,"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",question2answer,,,,,,,,,,,
均匀分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91ysz3sxsj30aw023glh.jpg),question2answer,,,,,,,,,,,
伯努利分布,"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",question2answer,,,,,,,,,,,
伯努利分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91yyoulv0j306m00lmwz.jpg),question2answer,,,,,,,,,,,
伯努利分布,期望为p，方差为p(1p),question2answer,,,,,,,,,,,
二项分布,独立重复地进行n次试验中，成功x次的概率:,question2answer,,,,,,,,,,,
二项分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z2sy9m5j308800ja9w.jpg),question2answer,,,,,,,,,,,
二项分布,期望为np，方差为np(1p),question2answer,,,,,,,,,,,
高斯分布,我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：,question2answer,,,,,,,,,,,
高斯分布,随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布,question2answer,,,,,,,,,,,
高斯分布,在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,question2answer,,,,,,,,,,,
高斯分布,典型的一维正态分布的概率密度函数为:,question2answer,,,,,,,,,,,
高斯分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z5lcnmcj30dk02fa9z.jpg),question2answer,,,,,,,,,,,
拉普拉斯分布,概率密度函数：,question2answer,,,,,,,,,,,
拉普拉斯分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z66211aj309c01hjr9.jpg),question2answer,,,,,,,,,,,
拉普拉斯分布,期望为u，方差为2γ^2,question2answer,,,,,,,,,,,
拉普拉斯分布,拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质,question2answer,,,,,,,,,,,
泊松分布,假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为k的概率。,question2answer,,,,,,,,,,,
泊松分布,概率密度函数：,question2answer,,,,,,,,,,,
泊松分布,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91z8oplp1j306f01g0sk.jpg),question2answer,,,,,,,,,,,
泊松分布,期望：λ，方差为：λ,question2answer,,,,,,,,,,,
条件概率,P(A/B)=P(AB)/P(B),question2answer,,,,,,,,,,,
独立,P(AB)=P(A)P(B),question2answer,,,,,,,,,,,
概率基础公式,加法：P(A+B)=P(A)+P(B)P(AB),question2answer,,,,,,,,,,,
概率基础公式,减法：P(AB)=P(A)P(AB),question2answer,,,,,,,,,,,
概率基础公式,乘法：P(AB)=P(A)P(B/A),question2answer,,,,,,,,,,,
全概率：,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920h3l62yj305u00qweb.jpg),question2answer,,,,,,,,,,,
贝叶斯,P(B/A)=P(B)*P(A/B)/P(A),question2answer,,,,,,,,,,,
切比雪夫不等式,"p(|xu|>k?)<=1/(k^2),满足k>0,u为期望,?为标准差",question2answer,,,,,,,,,,,
切比雪夫不等式,绝大多数数据都应该在均值附近,question2answer,,,,,,,,,,,
抽球,有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),question2answer,,,,,,,,,,,
抽球,无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),question2answer,,,,,,,,,,,
纸牌问题,问题：54张牌，分成6份，每份9张牌，大小王在一起的概率？,question2answer,,,,,,,,,,,
纸牌问题,54张牌分成6等份，共有M=(C54取9)*(C45取9)*...种分法。,question2answer,,,,,,,,,,,
纸牌问题,其中大小王在同一份的分法有N=(C6取1)*(C52取7)*(C45取9)*...种。,question2answer,,,,,,,,,,,
纸牌问题,因此所求概率为P=N/M,question2answer,,,,,,,,,,,
棍子/绳子问题,问题：一根棍子折三段能组成三角形的概率？,question2answer,,,,,,,,,,,
棍子/绳子问题,假设：棍子长度为1，第一段长度为x，第二段长度为y，第三段长度1xy,question2answer,,,,,,,,,,,
棍子/绳子问题,分母：总样本空间为：1*1=1,question2answer,,,,,,,,,,,
棍子/绳子问题,分子：两边之和大于第三边，得1/8,question2answer,,,,,,,,,,,
贝叶斯,问题：某城市发生一起汽车撞人逃跑事件，该城市只有两种颜色的车，蓝20%绿80%，事发时现场只有一个目击者，他指正是蓝车，但根据专家分析，当时那种条件下能看正确的可能性是80%，那么肇事的车是蓝车的概率是多少？,question2answer,,,,,,,,,,,
贝叶斯,假设事件A为目击者指正蓝车，事件B为肇事车为蓝车，事件C为肇事车为绿车，那么有：,question2answer,,,,,,,,,,,
贝叶斯,0.2`*`0.8/(0.2`*`0.8+0.8`*`0.2)=0.5,question2answer,,,,,,,,,,,
选择时间问题,"问题：一个活动,n个女生手里拿着长短不一的玫瑰花,无序的排成一排,一个男生从头走到尾,试图拿更长的玫瑰花,一旦拿了一朵就不能再拿其他的,错过了就不能回头,问最好的策略及其概率?",question2answer,,,,,,,,,,,
选择时间问题,1/e,question2answer,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均匀分布：,question2answer,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,E(x)=(a+b)/2,question2answer,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,标准差：D(x)=(ba)^2/12,question2answer,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,所以只需要对x做变换：sqrt(12(x1/2))即可,question2answer,,,,,,,,,,,
抽红蓝球球,问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,question2answer,,,,,,,,,,,
抽红蓝球球,假设设抽到蓝球的概率为p，设抽到红球的概率为q，那么抽取到的次数为：1・p+2p・q+...+np・q^(n1),question2answer,,,,,,,,,,,
抽红蓝球球,"可得E=p\[1+2q+...+nq^(n1)],令1+2q+...+nq^(n1)=s，再由s为等比公式和ssq得，E=1/p",question2answer,,,,,,,,,,,
泰勒公式,定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得,question2answer,,,,,,,,,,,
泰勒公式,f(x)=f(x0)+f'(x0)(xx0)+1/2!f''(x0)(xx0)^2+...+Rn(x),question2answer,,,,,,,,,,,
泰勒公式,其中，Rn(x)=f<n+1>(ζ)/(n+1)!(xx0)^(n+1)，为泰勒余项,question2answer,,,,,,,,,,,
常见泰勒公式,![](https://i.bmp.ovh/imgs/2019/11/0a10d9591cc0c4ac.png),question2answer,,,,,,,,,,,
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfple4x4j303x00i742.jpg),question2answer,,,,,,,,,,,
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfr2t8flj306l00mmwz.jpg),question2answer,,,,,,,,,,,
迭代公式推导,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9bfruh1klj3043015wea.jpg),question2answer,,,,,,,,,,,
迭代公式推导,这边是2次，所以直接以y=x^2化简了,question2answer,,,,,,,,,,,
实现它,```,question2answer,,,,,,,,,,,
实现它,"defget_ans(nums,count=10000):",question2answer,,,,,,,,,,,
实现它,ans=nums,question2answer,,,,,,,,,,,
实现它,ifnotans:,question2answer,,,,,,,,,,,
实现它,returnans,question2answer,,,,,,,,,,,
实现它,Times=0,question2answer,,,,,,,,,,,
实现它,whileTimes<count:,question2answer,,,,,,,,,,,
实现它,ans=0.5*(ans+nums/ans),question2answer,,,,,,,,,,,
实现它,Times+=1,question2answer,,,,,,,,,,,
实现它,returnans,question2answer,,,,,,,,,,,
实现它,```,question2answer,,,,,,,,,,,
范数,1范数：各列绝对值和的最大值,question2answer,,,,,,,,,,,
范数,"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",question2answer,,,,,,,,,,,
特征值分解，特征向量,特征值分解可以得到特征值与特征向量,question2answer,,,,,,,,,,,
特征值分解，特征向量,特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,question2answer,,,,,,,,,,,
特征值分解，特征向量,"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",question2answer,,,,,,,,,,,
特征值分解，特征向量,也可写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9201wcuefj303600qa9u.jpg),question2answer,,,,,,,,,,,
特征值分解，特征向量,其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵,question2answer,,,,,,,,,,,
正定性,如何判断矩阵的正定性？,question2answer,,,,,,,,,,,
正定性,矩阵的特征值大于等于0，半正定,question2answer,,,,,,,,,,,
正定性,矩阵的特征值大于0，正定,question2answer,,,,,,,,,,,
正定性,正定性的用途？,question2answer,,,,,,,,,,,
正定性,Hessian矩阵正定性在梯度下降的应用,question2answer,,,,,,,,,,,
正定性,"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",question2answer,,,,,,,,,,,
正定性,在svm中核函数构造的基本假设,question2answer,,,,,,,,,,,
统计方法,3?原则,question2answer,,,,,,,,,,,
统计方法,数据需要服从正态分布,question2answer,,,,,,,,,,,
统计方法,只能解决一维问题：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m954lh5tj301h00gdfl.jpg),question2answer,,,,,,,,,,,
统计方法,基于正态分布的离群点检测方法,question2answer,,,,,,,,,,,
统计方法,一元高斯分布校验：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m91basuwj306201h0sl.jpg)，如果概率值大小离群则代表为异常点,question2answer,,,,,,,,,,,
统计方法,多元高斯分布检测：,question2answer,,,,,,,,,,,
统计方法,假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),question2answer,,,,,,,,,,,
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fdr3xfj30140080sl.jpg)的协方差矩阵：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9fly7rpj306g00ja9z.jpg),question2answer,,,,,,,,,,,
统计方法,得到![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9gzmd1cj30bz01874b.jpg),question2answer,,,,,,,,,,,
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m92kshc2j30d6073dft.jpg),question2answer,,,,,,,,,,,
统计方法,马氏距离,question2answer,,,,,,,,,,,
统计方法,假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,question2answer,,,,,,,,,,,
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kguzdgj307e00lmx3.jpg),question2answer,,,,,,,,,,,
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验,question2answer,,,,,,,,,,,
统计方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ryphxxj304u00kmx2.jpg),question2answer,,,,,,,,,,,
统计方法,"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",question2answer,,,,,,,,,,,
统计方法,箱型图,question2answer,,,,,,,,,,,
统计方法,"IQR，\[Q13/2(Q3Q1),Q3+3/2(Q3Q1)]",question2answer,,,,,,,,,,,
矩阵分解方法,PCA,question2answer,,,,,,,,,,,
矩阵分解方法,去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,question2answer,,,,,,,,,,,
矩阵分解方法,核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,question2answer,,,,,,,,,,,
矩阵分解方法,问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,question2answer,,,,,,,,,,,
矩阵分解方法,SVD,question2answer,,,,,,,,,,,
矩阵分解方法,假设dataMat是一个p维的数据集合，有N个样本，它的协方差矩阵是X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg),question2answer,,,,,,,,,,,
矩阵分解方法,"其中P是一个(p,p)维的正交矩阵，它的每一列都是X的特征向量。D是一个(p,p)维的对角矩阵，包含了特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。",question2answer,,,,,,,,,,,
矩阵分解方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mar6eb43j304300h746.jpg)可以认为是dataMat在主成分topj上的映射,question2answer,,,,,,,,,,,
矩阵分解方法,最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg),question2answer,,,,,,,,,,,
矩阵分解方法,异常值分数（outlierscore）：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8maynk5wkj30a300mjrc.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mayu64x7j305500mt8m.jpg),question2answer,,,,,,,,,,,
特征值和特征向量的本质是什么？,一个特征向量可以看成2维平面上面的一条线，或者高维空间里面的一个超平面,question2answer,,,,,,,,,,,
特征值和特征向量的本质是什么？,特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,question2answer,,,,,,,,,,,
矩阵乘法的实际意义？,两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,question2answer,,,,,,,,,,,
矩阵乘法的实际意义？,矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,question2answer,,,,,,,,,,,
密度的离群点检测,定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,question2answer,,,,,,,,,,,
密度的离群点检测,我们可以通过随机选择联通点，人为设置联通点附近最小半径a，半径内最小容忍点个数b，再考虑密度可达，形成蓝色方框内的正常数据区域，剩下的黄色区域内的点即为异常点。,question2answer,,,,,,,,,,,
密度的离群点检测,LocalOutlierFactor算法,question2answer,,,,,,,,,,,
密度的离群点检测,孤立森林:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mcoswixrj30iy0em0tb.jpg),question2answer,,,,,,,,,,,
密度的离群点检测,经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),question2answer,,,,,,,,,,,
密度的离群点检测,经验2：树的个数在256棵以下,question2answer,,,,,,,,,,,
密度的离群点检测,缺点：,question2answer,,,,,,,,,,,
密度的离群点检测,计算量大：o(n^2),question2answer,,,,,,,,,,,
密度的离群点检测,需要人为选择阈值,question2answer,,,,,,,,,,,
聚类的离群点检测,一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。,question2answer,,,,,,,,,,,
聚类的离群点检测,缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择,question2answer,,,,,,,,,,,
如何处理异常点？,删除含有异常值的记录：直接将含有异常值的记录删除；,question2answer,,,,,,,,,,,
如何处理异常点？,视为缺失值：将异常值视为缺失值，利用缺失值处理的方法进行处理；,question2answer,,,,,,,,,,,
如何处理异常点？,平均值修正：可用前后两个观测值的平均值修正该异常值；,question2answer,,,,,,,,,,,
如何处理异常点？,生成列新特征：category异常,question2answer,,,,,,,,,,,
如何处理异常点？,不处理：直接在具有异常值的数据集上进行数据挖掘；,question2answer,,,,,,,,,,,
为什么要对数据进行采样平衡,"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",question2answer,,,,,,,,,,,
为什么要对数据进行采样平衡,上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,question2answer,,,,,,,,,,,
为什么要对数据进行采样平衡,比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况,question2answer,,,,,,,,,,,
是否一定需要对原始数据进行采样平衡,否。,question2answer,,,,,,,,,,,
是否一定需要对原始数据进行采样平衡,采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,question2answer,,,,,,,,,,,
是否一定需要对原始数据进行采样平衡,采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,随机采样,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,无放回的简单抽样：每条样本被采到的概率相等且都为1/N,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,有放回的简单抽样：每条样本可能多次被选中,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,上采样：即合理地增加少数类的样本,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,下采样：欠抽样技术是将数据从原始数据集中移除,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,平衡采样：考虑正负样本比,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,"整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集",question2answer,,,,,,,,,,,
有哪些常见的采样方法？,合成采样,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",question2answer,,,,,,,,,,,
有哪些常见的采样方法？,SMOTE,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,"x_new=x+rand(0,1)*(x′?x)",question2answer,,,,,,,,,,,
有哪些常见的采样方法？,**带来新样本的同时有可能造成不同类别样本之间的重合**,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,BorderlineSMOTE为了解决上面的问题，在x_new生成之前，会先判断x这个点是否周围都是同类别的点,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,ADASYN,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,同上，也是在构造样本点的过程中考虑了正负样本比,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,平衡欠采样,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,EasyEnsemble，利用模型融合的方法（Ensemble）,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,BalanceCascade，利用模型融合的方法（Boost）,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,每次剔除预测正确的多数样本，加入新的未预测的多数样本,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,NearMiss,question2answer,,,,,,,,,,,
有哪些常见的采样方法？,选择离各种情况下的少数样本位置最远的多数样本进行训练,question2answer,,,,,,,,,,,
能否避免采样？,可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,question2answer,,,,,,,,,,,
你平时怎么用采样方法？,尽量避免使用合成采样的方式去做数据填充，总结如下：,question2answer,,,,,,,,,,,
你平时怎么用采样方法？,由于项目中时间的充裕问题，填充的结果往往是正负样本交叠且无感知的，会干扰分类器,question2answer,,,,,,,,,,,
你平时怎么用采样方法？,通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,question2answer,,,,,,,,,,,
你平时怎么用采样方法？,合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,question2answer,,,,,,,,,,,
为什么需要对数据进行变换？,避免异常点：比如对连续变量进行份桶离散化,question2answer,,,,,,,,,,,
为什么需要对数据进行变换？,可解释性或者需要连续输出：比如评分卡模型中的iv+woe,question2answer,,,,,,,,,,,
为什么需要对数据进行变换？,使得原始数据的信息量更大：比如log/sqrt变换,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,归一化(maxmin),question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,标准化(zscore),question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,作用,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,解决部分模型由于数据值域不同对模型产生的影响，尤其是距离模型,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,更快的收敛,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,去量纲化,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,避免数值计算溢出,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,总结,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,常用模型,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,knn：计算距离，不去量冈则结果受值域范围影响大,question2answer,,,,,,,,,,,
归一化和标准化之间的关系？,neuralnetwork：梯度异常问题+激活函数问题,question2answer,,,,,,,,,,,
连续特征常用方法,截断,question2answer,,,,,,,,,,,
连续特征常用方法,"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",question2answer,,,,,,,,,,,
连续特征常用方法,参考异常点里面的outlier识别，以最大值填充或者以None,question2answer,,,,,,,,,,,
连续特征常用方法,二值化,question2answer,,,,,,,,,,,
连续特征常用方法,数据分布过于不平衡,question2answer,,,,,,,,,,,
连续特征常用方法,空值/异常值过多,question2answer,,,,,,,,,,,
连续特征常用方法,分桶,question2answer,,,,,,,,,,,
连续特征常用方法,小范围连续数据内不存在逻辑关系，比如31岁和32岁之间不存在明显的差异，可以归为一类,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2hcomdwj30k60djdh4.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,离散化,question2answer,,,,,,,,,,,
连续特征常用方法,数值无意义，比如学历、祖籍等等,question2answer,,,,,,,,,,,
连续特征常用方法,缩放,question2answer,,,,,,,,,,,
连续特征常用方法,zscore标准化,question2answer,,,,,,,,,,,
连续特征常用方法,minmax归一化,question2answer,,,,,,,,,,,
连续特征常用方法,范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,L1范数,question2answer,,,,,,,,,,,
连续特征常用方法,L2范数,question2answer,,,,,,,,,,,
连续特征常用方法,平方根缩放,question2answer,,,,,,,,,,,
连续特征常用方法,对数缩放,question2answer,,,,,,,,,,,
连续特征常用方法,对数缩放适用于处理长尾分且取值为正数的数值变量,question2answer,,,,,,,,,,,
连续特征常用方法,它将大端长尾压缩为短尾，并将小端进行延伸,question2answer,,,,,,,,,,,
连续特征常用方法,可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),question2answer,,,,,,,,,,,
连续特征常用方法,可以把有偏分布修正为近似正太分布,question2answer,,,,,,,,,,,
连续特征常用方法,BoxCox转换,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g8mfjjwir3j309m038gln.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,question2answer,,,,,,,,,,,
连续特征常用方法,特征交叉,question2answer,,,,,,,,,,,
连续特征常用方法,人为分段交叉,question2answer,,,,,,,,,,,
连续特征常用方法,提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,question2answer,,,,,,,,,,,
连续特征常用方法,离散变量的交并补,question2answer,,,,,,,,,,,
连续特征常用方法,连续变量的点积，attention类似,question2answer,,,,,,,,,,,
连续特征常用方法,交叉中需要并行特征筛选的步骤,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2rf5aawj30ka0j6q48.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,自动组合,question2answer,,,,,,,,,,,
连续特征常用方法,FM/FFM中的矩阵点积,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2un2ckzj30eb07jaan.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,NeuralNetwork里面的dense,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2vb1nnij30co07lt94.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,条件选择,question2answer,,,,,,,,,,,
连续特征常用方法,通过树或者类似的特征组合模型去做最低熵的特征选择,question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2t44a2mj30im0bt0tz.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n2v20wjbj30hs0dx0tq.jpg),question2answer,,,,,,,,,,,
连续特征常用方法,非线性编码,question2answer,,,,,,,,,,,
连续特征常用方法,核向量进行升维,question2answer,,,,,,,,,,,
连续特征常用方法,树模型的叶子结点的stack,question2answer,,,,,,,,,,,
连续特征常用方法,谱聚类/pca/svd等信息抽取编码,question2answer,,,,,,,,,,,
连续特征常用方法,lda/EM等分布拟合表示,question2answer,,,,,,,,,,,
离散特征常用方法,onehotencoder,question2answer,,,,,,,,,,,
离散特征常用方法,分层编码,question2answer,,,,,,,,,,,
离散特征常用方法,有一定规律的类别数据，邮政编码，手机号等等,question2answer,,,,,,,,,,,
离散特征常用方法,计数编码,question2answer,,,,,,,,,,,
离散特征常用方法,"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",question2answer,,,,,,,,,,,
离散特征常用方法,"对异常值比较敏感,特征取值有可能冲突",question2answer,,,,,,,,,,,
离散特征常用方法,计数排名编码,question2answer,,,,,,,,,,,
离散特征常用方法,解决上述问题，以排名代替值,question2answer,,,,,,,,,,,
离散特征常用方法,Embedding,question2answer,,,,,,,,,,,
离散特征常用方法,"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",question2answer,,,,,,,,,,,
离散特征常用方法,类别特征之间交叉组合,question2answer,,,,,,,,,,,
离散特征常用方法,笛卡尔交叉,question2answer,,,,,,,,,,,
离散特征常用方法,类别特征和数值特征之间交叉组合,question2answer,,,,,,,,,,,
离散特征常用方法,均值、中位数、标准差、最大值和最小值,question2answer,,,,,,,,,,,
离散特征常用方法,分位数、方差、vif值、分段冲量,question2answer,,,,,,,,,,,
文本特征,预处理手段有哪些？,question2answer,,,,,,,,,,,
文本特征,将字符转化为小写,question2answer,,,,,,,,,,,
文本特征,分词,question2answer,,,,,,,,,,,
文本特征,去除无用字符,question2answer,,,,,,,,,,,
文本特征,繁体转中文,question2answer,,,,,,,,,,,
文本特征,去除停用词,question2answer,,,,,,,,,,,
文本特征,去除稀有词,question2answer,,,,,,,,,,,
文本特征,半角全角切换,question2answer,,,,,,,,,,,
文本特征,错词纠正,question2answer,,,,,,,,,,,
文本特征,关键词标记,question2answer,,,,,,,,,,,
文本特征,TFIDF,question2answer,,,,,,,,,,,
文本特征,LDA,question2answer,,,,,,,,,,,
文本特征,LSA,question2answer,,,,,,,,,,,
文本特征,提取词根,question2answer,,,,,,,,,,,
文本特征,词干提取,question2answer,,,,,,,,,,,
文本特征,标点符号编码,question2answer,,,,,,,,,,,
文本特征,文档特征,question2answer,,,,,,,,,,,
文本特征,实体插入和提取,question2answer,,,,,,,,,,,
文本特征,文本向量化,question2answer,,,,,,,,,,,
文本特征,word2vec,question2answer,,,,,,,,,,,
文本特征,glove,question2answer,,,,,,,,,,,
文本特征,bert,question2answer,,,,,,,,,,,
文本特征,文本相似性,question2answer,,,,,,,,,,,
文本特征,如何做样本构造？,question2answer,,,,,,,,,,,
文本特征,按标点切分,question2answer,,,,,,,,,,,
文本特征,按句切分,question2answer,,,,,,,,,,,
文本特征,对话session切分,question2answer,,,,,,,,,,,
文本特征,按文章切分,question2answer,,,,,,,,,,,
文本特征,按场景切分,question2answer,,,,,,,,,,,
文本特征,分词过程中会考虑哪些方面？,question2answer,,,,,,,,,,,
文本特征,词性标注,question2answer,,,,,,,,,,,
文本特征,词形还原和词干提取,question2answer,,,,,,,,,,,
文本特征,词形还原为了通用性特征的提取,question2answer,,,,,,,,,,,
文本特征,词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,question2answer,,,,,,,,,,,
文本特征,文本中的统计信息一般有哪些？,question2answer,,,,,,,,,,,
文本特征,直接统计值：,question2answer,,,,,,,,,,,
文本特征,文本的长度,question2answer,,,,,,,,,,,
文本特征,单词个数,question2answer,,,,,,,,,,,
文本特征,数字个数,question2answer,,,,,,,,,,,
文本特征,字母个数,question2answer,,,,,,,,,,,
文本特征,大小写单词个数,question2answer,,,,,,,,,,,
文本特征,大小写字母个数,question2answer,,,,,,,,,,,
文本特征,标点符号个数,question2answer,,,,,,,,,,,
文本特征,特殊字符个数,question2answer,,,,,,,,,,,
文本特征,数字占比,question2answer,,,,,,,,,,,
文本特征,字母占比,question2answer,,,,,,,,,,,
文本特征,特殊字符占比,question2answer,,,,,,,,,,,
文本特征,不同词性个数,question2answer,,,,,,,,,,,
文本特征,直接统计值的统计信息：,question2answer,,,,,,,,,,,
文本特征,最小最大均值方差标准差,question2answer,,,,,,,,,,,
文本特征,分位数，最早/最晚出现位置,question2answer,,,,,,,,,,,
文本特征,直接对文本特征进行整理手段有哪些？,question2answer,,,,,,,,,,,
文本特征,NGram模型,question2answer,,,,,,,,,,,
文本特征,将文本转换为连续序列，扩充样本特征,question2answer,,,,,,,,,,,
文本特征,连续语意的提取,question2answer,,,,,,,,,,,
文本特征,TFIDF,question2answer,,,,,,,,,,,
文本特征,"权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""",question2answer,,,,,,,,,,,
文本特征,LDA,question2answer,,,,,,,,,,,
文本特征,主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,question2answer,,,,,,,,,,,
文本特征,相似度,question2answer,,,,,,,,,,,
文本特征,余弦相似度,question2answer,,,,,,,,,,,
文本特征,Jaccard相似度,question2answer,,,,,,,,,,,
文本特征,共现性,question2answer,,,,,,,,,,,
文本特征,Levenshtein(编辑距离),question2answer,,,,,,,,,,,
文本特征,文本近似程度,question2answer,,,,,,,,,,,
文本特征,海林格距离,question2answer,,,,,,,,,,,
文本特征,用来衡量概率分布之间的相似性,question2answer,,,,,,,,,,,
文本特征,JSD,question2answer,,,,,,,,,,,
文本特征,衡量prob1和prob2两个分布的相似程度,question2answer,,,,,,,,,,,
文本特征,向量化,question2answer,,,,,,,,,,,
文本特征,word2vec,question2answer,,,,,,,,,,,
文本特征,glove,question2answer,,,,,,,,,,,
文本特征,bert,question2answer,,,,,,,,,,,
文本特征,文本处理有大量可以讲，可以谈的，以上只是做了一个最简单的汇总，详细的会在自然语言处理的专题一条一条分析，面试官也一定会就每个知识点进行展开,question2answer,,,,,,,,,,,
画一个最简单的最快速能实现的框架,建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,question2answer,,,,,,,,,,,
画一个最简单的最快速能实现的框架,建议从简单的开始，然后面试官说还有其他方法么？再做延展：,question2answer,,,,,,,,,,,
画一个最简单的最快速能实现的框架,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mzet073zj31jw0u046i.jpg),question2answer,,,,,,,,,,,
为什么要做特征选择？,耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,question2answer,,,,,,,,,,,
为什么要做特征选择？,过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,question2answer,,,,,,,,,,,
为什么要做特征选择？,共线性：单因子对目标的作用被稀释，解释力下降,question2answer,,,,,,,,,,,
从哪些方面可以做特征选择？,方差，是的feature内的方向更大，对目标区分度提高更高贡献,question2answer,,,,,,,,,,,
从哪些方面可以做特征选择？,相关性，与区分目标有高相关的特征才有意义,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,方差,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,移除低方差特征,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,考虑有值数据中的占比，异常数据的占比，正常范围数据过少的数据也可以移除,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,相关性,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,皮尔森相关系数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n46pedyoj303e019t8i.jpg),question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,Fisher得分:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8n4jajze8j31hc0u0dt2.jpg),question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,假设检验,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,卡方检验,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,ANOVA,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,熵检验,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,互信息熵,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,KL散度,question2answer,,,,,,,,,,,
既然说了两个方向，分别介绍一些吧,相对熵,question2answer,,,,,,,,,,,
是不是一定需要对缺失值处理？,当缺失值占比在可接受的范围以内的时候才需要进行填充，如果缺失值大于50%以上的时候，可以选择进行二分化，如果缺失值大于80%可以完整删除该列而不是强行去填充,question2answer,,,,,,,,,,,
直接填充方法有哪些？,均值,question2answer,,,,,,,,,,,
直接填充方法有哪些？,中位数,question2answer,,,,,,,,,,,
直接填充方法有哪些？,众数,question2answer,,,,,,,,,,,
直接填充方法有哪些？,分位数,question2answer,,,,,,,,,,,
模型插值方法有哪些？及方法的问题,有效性存疑，取决于特征列数,question2answer,,,,,,,,,,,
模型插值方法有哪些？及方法的问题,生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关,question2answer,,,,,,,,,,,
如何直接离散化？,离散特征新增缺失的category,question2answer,,,,,,,,,,,
hold位填充方法有哪些？,把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,question2answer,,,,,,,,,,,
hold位填充方法有哪些？,可以参考YouTube中的新商品向量生成逻辑,question2answer,,,,,,,,,,,
hold位填充方法有哪些？,bert中的\[UNK]向量，\[unused]向量,question2answer,,,,,,,,,,,
怎么理解分布补全？,如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,question2answer,,,,,,,,,,,
random方法,在缺失量特别少(通常认为小于1%)的时候，可以随机生成,question2answer,,,,,,,,,,,
总结,实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,question2answer,,,,,,,,,,,
总结,快速,question2answer,,,,,,,,,,,
总结,对原始数据的前提假设最少，也不会影响到非缺失列,question2answer,,,,,,,,,,,
总结,在深度学习中，hold位填充方法用的最多,question2answer,,,,,,,,,,,
总结,在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,question2answer,,,,,,,,,,,
总结,而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,question2answer,,,,,,,,,,,
常见决策树,|模型|ID3|C4.5|CART|,question2answer,,,,,,,,,,,
常见决策树,|:|:|:|::|,question2answer,,,,,,,,,,,
常见决策树,|结构|多叉树|多叉树|二叉树|,question2answer,,,,,,,,,,,
常见决策树,|特征选择|信息增益|信息增益率|Gini系数/均方差|,question2answer,,,,,,,,,,,
常见决策树,|连续值处理|不支持|支持|支持|,question2answer,,,,,,,,,,,
常见决策树,|缺失值处理|不支持|支持|支持|,question2answer,,,,,,,,,,,
常见决策树,|枝剪|不支持|支持|支持|,question2answer,,,,,,,,,,,
简述决策树构建过程,1.构建根节点，将所有训练数据都放在根节点,question2answer,,,,,,,,,,,
简述决策树构建过程,2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,question2answer,,,,,,,,,,,
简述决策树构建过程,3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,question2answer,,,,,,,,,,,
详述信息熵计算方法及存在问题,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925k1ns0yj305y018glg.jpg),question2answer,,,,,,,,,,,
详述信息熵计算方法及存在问题,其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类),question2answer,,,,,,,,,,,
详述信息增益计算方法,条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,question2answer,,,,,,,,,,,
详述信息增益计算方法,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925puuv1cj308p01kdfr.jpg),question2answer,,,,,,,,,,,
详述信息增益计算方法,信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,question2answer,,,,,,,,,,,
详述信息增益计算方法,"I(D,A)=H(D)H(D/A)",question2answer,,,,,,,,,,,
详述信息增益计算方法,简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,question2answer,,,,,,,,,,,
详述信息增益率计算方法,"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",question2answer,,,,,,,,,,,
详述信息增益率计算方法,信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg),question2answer,,,,,,,,,,,
解释Gini系数,Gini系数二分情况下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92705elt9j308000q744.jpg),question2answer,,,,,,,,,,,
解释Gini系数,对于决策树样本D来说，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9273px3t2j30a4018wee.jpg),question2answer,,,,,,,,,,,
解释Gini系数,对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),question2answer,,,,,,,,,,,
ID3存在的问题,缺点：,question2answer,,,,,,,,,,,
ID3存在的问题,存在偏向于选择取值较多的特征问题,question2answer,,,,,,,,,,,
ID3存在的问题,连续值不支持,question2answer,,,,,,,,,,,
ID3存在的问题,缺失值不支持,question2answer,,,,,,,,,,,
ID3存在的问题,无法枝剪,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,主动进行的连续的特征离散化,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,**连续特征可以再后序特征划分中仍可继续参与计算**,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,缺失问题优化,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布,question2answer,,,,,,,,,,,
C4.5相对于ID3的改进点,采用预枝剪,question2answer,,,,,,,,,,,
CART的连续特征改进点,分类情况下的变量特征选择,question2answer,,,,,,,,,,,
CART的连续特征改进点,离散变量：二分划分,question2answer,,,,,,,,,,,
CART的连续特征改进点,连续变量：和C4.5一致，如果当前节点为连续属性，则该属性后面依旧可以参与子节点的产生选择过程,question2answer,,,,,,,,,,,
CART的连续特征改进点,回归情况下，连续变量不再采取中间值划分，采用最小方差法,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,我们的算法从根节点开始，用训练集递归的建立CART树。,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,question2answer,,,,,,,,,,,
CART分类树建立算法的具体流程,递归1～4,question2answer,,,,,,,,,,,
CART回归树建立算法的具体流程,其他部分都一样，在构建过程中遇到连续值的话，并不是利用C4.5中的中间值基尼系数的方式，而是采取了最小方差方法：,question2answer,,,,,,,,,,,
CART回归树建立算法的具体流程,对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),question2answer,,,,,,,,,,,
CART输出结果的逻辑？,回归树：利用最终叶子的均值或者中位数来作为输出结果,question2answer,,,,,,,,,,,
CART输出结果的逻辑？,分类树：利用最终叶子的大概率的分类类别来作为输出结果,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,当然，这是两种极端情况。一般来说，??越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的??，一定存在使损失函数????(??)最小的唯一子树。,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,"由枝剪到根结点及不枝剪两种情况可得：??=(??(??)???(????))/(|????|?1),C(T)为根结点误差",question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,计算出每个子树是否剪枝的阈值??,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,选择阈值??集合中的最小值,question2answer,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,分别针对不同的最小值??所对应的剪枝后的最优子树做交叉验证,question2answer,,,,,,,,,,,
树形结构为何不需要归一化？,无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,question2answer,,,,,,,,,,,
决策树的优缺点,优点：,question2answer,,,,,,,,,,,
决策树的优缺点,缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,question2answer,,,,,,,,,,,
决策树的优缺点,可解释性强,question2answer,,,,,,,,,,,
决策树的优缺点,算法对数据没有强假设,question2answer,,,,,,,,,,,
决策树的优缺点,可以解决线性及非线性问题,question2answer,,,,,,,,,,,
决策树的优缺点,有特征选择等辅助功能,question2answer,,,,,,,,,,,
决策树的优缺点,缺点：,question2answer,,,,,,,,,,,
决策树的优缺点,处理关联性数据比较薄弱,question2answer,,,,,,,,,,,
决策树的优缺点,正负量级有偏样本的样本效果较差,question2answer,,,,,,,,,,,
决策树的优缺点,单棵树的拟合效果欠佳，容易过拟合,question2answer,,,,,,,,,,,
简单介绍SVM?,从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2・||W||・||W||)subjectto：y(wx+b)>=1，其中||・||为2范数,question2answer,,,,,,,,,,,
简单介绍SVM?,然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,question2answer,,,,,,,,,,,
简单介绍SVM?,最后再利用SMO（序列最小优化）来解决这个对偶问题,question2answer,,,,,,,,,,,
什么叫最优超平面？,两类样本分别分割在该超平面的两侧,question2answer,,,,,,,,,,,
什么叫最优超平面？,超平面两侧的点离超平面尽可能的远,question2answer,,,,,,,,,,,
什么是支持向量？,在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点,question2answer,,,,,,,,,,,
SVM 和全部数据有关还是和局部数据有关?,局部,question2answer,,,,,,,,,,,
加大训练数据量一定能提高SVM准确率吗？,支持向量的添加才会提高，否则无效,question2answer,,,,,,,,,,,
如何解决多分类问题？,对训练器进行组合。其中比较典型的有一对一，和一对多,question2answer,,,,,,,,,,,
可以做回归吗，怎么做？,可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg),question2answer,,,,,,,,,,,
SVM 能解决哪些问题？,线性问题,question2answer,,,,,,,,,,,
SVM 能解决哪些问题？,对于n为数据，找到n1维的超平面将数据分成2份。通过增加一个约束条件：要求这个超平面到每边最近数据点的距离是最大的,question2answer,,,,,,,,,,,
SVM 能解决哪些问题？,非线性问题,question2answer,,,,,,,,,,,
SVM 能解决哪些问题？,SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,question2answer,,,,,,,,,,,
介绍一下你知道的不同的SVM分类器？,硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,question2answer,,,,,,,,,,,
介绍一下你知道的不同的SVM分类器？,软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,question2answer,,,,,,,,,,,
介绍一下你知道的不同的SVM分类器？,kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,question2answer,,,,,,,,,,,
什么叫软间隔？,软间隔允许部分样本点不满足约束条件：1<y(wx+b),question2answer,,,,,,,,,,,
SVM 软间隔与硬间隔表达式,硬间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93ha4y4glj3094011a9w.jpg),question2answer,,,,,,,,,,,
SVM 软间隔与硬间隔表达式,软间隔：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93hae5o8lj30d701f0sn.jpg),question2answer,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),question2answer,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),question2answer,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,"拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss",question2answer,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,question2answer,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,question2answer,,,,,,,,,,,
为什么要把原问题转换为对偶问题？,因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,question2answer,,,,,,,,,,,
为什么要把原问题转换为对偶问题？,引入了核函数,question2answer,,,,,,,,,,,
为什么求解对偶问题更加高效？,原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,question2answer,,,,,,,,,,,
为什么求解对偶问题更加高效？,因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0,question2answer,,,,,,,,,,,
alpha系数有多少个？,样本点的个数,question2answer,,,,,,,,,,,
KKT限制条件，KKT条件有哪些，完整描述,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,question2answer,,,,,,,,,,,
KKT限制条件，KKT条件有哪些，完整描述,KKT乘子λ>=0,question2answer,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),question2answer,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),question2answer,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,要求KKT乘子λ>=0,question2answer,,,,,,,,,,,
核函数的作用是啥,核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,question2answer,,,,,,,,,,,
核函数的种类和应用场景,线性核函数：主要用于线性可分的情形。参数少，速度快。,question2answer,,,,,,,,,,,
核函数的种类和应用场景,多项式核函数：,question2answer,,,,,,,,,,,
核函数的种类和应用场景,高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,question2answer,,,,,,,,,,,
核函数的种类和应用场景,sigmoid核函数：,question2answer,,,,,,,,,,,
核函数的种类和应用场景,拉普拉斯核函数：,question2answer,,,,,,,,,,,
如何选择核函数,我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,question2answer,,,,,,,,,,,
常用核函数的定义？,在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,question2answer,,,,,,,,,,,
常用核函数的定义？,"1)线性：K(v1,v2)=<v1,v2>",question2answer,,,,,,,,,,,
常用核函数的定义？,"2)多项式：K(v1,v2)=(r<v1,v2>+c)^n",question2answer,,,,,,,,,,,
常用核函数的定义？,"3)Radialbasisfunction：K(v1,v2)=exp(r||v1v2||^2)",question2answer,,,,,,,,,,,
常用核函数的定义？,"4)Sigmoid：tanh(r<v1,v2>+c)",question2answer,,,,,,,,,,,
核函数需要满足什么条件？,Mercer定理：核函数矩阵是对称半正定的,question2answer,,,,,,,,,,,
为什么在数据量大的情况下常常用lr代替核SVM？,计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),question2answer,,,,,,,,,,,
为什么在数据量大的情况下常常用lr代替核SVM？,在使用核函数的时候参数假设全靠试，时间成本过高,question2answer,,,,,,,,,,,
高斯核可以升到多少维？为什么,无穷维,question2answer,,,,,,,,,,,
高斯核可以升到多少维？为什么,e的n次方的泰勒展开得到了一个无穷维度的映射,question2answer,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",线性问题：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",线性：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑回归，线性svm,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",非线性：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",贝叶斯，决策树，核svm，DNN,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据问题：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据量大特征多：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑回归,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",决策树算法,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",数据量少特征少：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",核svm,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",缺失值多：,question2answer,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",树模型,question2answer,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,LR是参数模型，SVM为非参数模型。,question2answer,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,question2answer,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,question2answer,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,LR的模型相对简单，在进行大规模线性分类时比较方便。,question2answer,,,,,,,,,,,
损失函数是啥,"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,最小二乘,question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,求导可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93489pnxxj3052014jr7.jpg),question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg),question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,如果X点乘X的转置可逆则有唯一解，否则无法如此求解,question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,梯度下降,question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),question2answer,,,,,,,,,,,
最小二乘/梯度下降手推,求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg),question2answer,,,,,,,,,,,
介绍一下岭回归,加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),question2answer,,,,,,,,,,,
介绍一下岭回归,在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg),question2answer,,,,,,,,,,,
什么时候使用岭回归？,样本数少，或者样本重复程度高,question2answer,,,,,,,,,,,
什么时候用Lasso回归？,特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,question2answer,,,,,,,,,,,
请问从EM角度理解kmeans?,kmeans是两个步骤交替进行，可以分别看成E步和M步,question2answer,,,,,,,,,,,
请问从EM角度理解kmeans?,M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；,question2answer,,,,,,,,,,,
请问从EM角度理解kmeans?,E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似,question2answer,,,,,,,,,,,
为什么kmeans一定会收敛?,M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,question2answer,,,,,,,,,,,
kmeans初始点除了随机选取之外的方法？,先层次聚类，再在不同层次上选取初始点进行kmeans聚类,question2answer,,,,,,,,,,,
解释一下朴素贝叶斯中考虑到的条件独立假设,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92b15s6daj308x00rq2s.jpg),question2answer,,,,,,,,,,,
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),question2answer,,,,,,,,,,,
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,"朴素贝叶斯=贝叶斯公式+条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可",question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作,question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,先验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92bmactdlj303o0133yb.jpg),question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,后验平滑：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g92borlh0nj3043018q2r.jpg),question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,question2answer,,,,,,,,,,,
朴素贝叶斯中出现的常见模型有哪些,伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,question2answer,,,,,,,,,,,
出现估计概率值为 0 怎么处理,拉普拉斯平滑,question2answer,,,,,,,,,,,
朴素贝叶斯的优缺点？,优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,question2answer,,,,,,,,,,,
朴素贝叶斯的优缺点？,缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）,question2answer,,,,,,,,,,,
朴素贝叶斯与 LR 区别？,生成模型和判别模型,question2answer,,,,,,,,,,,
朴素贝叶斯与 LR 区别？,条件独立要求,question2answer,,,,,,,,,,,
朴素贝叶斯与 LR 区别？,小数据集和大数据集,question2answer,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg),question2answer,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg),question2answer,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,question2answer,,,,,,,,,,,
LR推导，基础5连问,基础公式,question2answer,,,,,,,,,,,
LR推导，基础5连问,f(x)=wx+b,question2answer,,,,,,,,,,,
LR推导，基础5连问,y=sigmoid(f(x)),question2answer,,,,,,,,,,,
LR推导，基础5连问,可以看作是一次线性拟合+一次sigmoid的非线性变化,question2answer,,,,,,,,,,,
LR推导，基础5连问,伯努利过程,question2answer,,,,,,,,,,,
LR推导，基础5连问,对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：,question2answer,,,,,,,,,,,
LR推导，基础5连问,"p(y=1/x,θ)=h(θ,x)",question2answer,,,,,,,,,,,
LR推导，基础5连问,"p(y=0/x,θ)=1h(θ,x)",question2answer,,,,,,,,,,,
LR推导，基础5连问,"p(y/x,θ)=h(θ,x)^y・(1h(θ,x))^(1y)",question2answer,,,,,,,,,,,
LR推导，基础5连问,第i个样本正确预测的概率如上可得,question2answer,,,,,,,,,,,
LR推导，基础5连问,几率odds,question2answer,,,,,,,,,,,
LR推导，基础5连问,数据特征下属于正例及反例的比值,question2answer,,,,,,,,,,,
LR推导，基础5连问,ln(y/(1y)),question2answer,,,,,,,,,,,
LR推导，基础5连问,极大似然,question2answer,,,,,,,,,,,
LR推导，基础5连问,第i个样本正确预测的概率如上可得每条样本的情况下,question2answer,,,,,,,,,,,
LR推导，基础5连问,综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：,question2answer,,,,,,,,,,,
LR推导，基础5连问,"∏(h(θ,x)^y・(1h(θ,x))^(1y))",question2answer,,,,,,,,,,,
LR推导，基础5连问,损失函数,question2answer,,,,,,,,,,,
LR推导，基础5连问,通常会对极大似然取对数，得到损失函数，方便计算,question2answer,,,,,,,,,,,
LR推导，基础5连问,"∑ylogh(θ,x)+(1y)log(1h(θ,x))最大",question2answer,,,,,,,,,,,
LR推导，基础5连问,"及1/m・∑ylogh(θ,x)+(1y)log(1h(θ,x))最小",question2answer,,,,,,,,,,,
LR推导，基础5连问,梯度下降,question2answer,,,,,,,,,,,
LR推导，基础5连问,损失函数求偏导，更新θ,question2answer,,,,,,,,,,,
LR推导，基础5连问,θj+1=θj?・?Loss/?θ=θj?・1/m・∑x・(hy),question2answer,,,,,,,,,,,
LR推导，基础5连问,?为学习率,question2answer,,,,,,,,,,,
梯度下降如何并行化？,首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg),question2answer,,,,,,,,,,,
梯度下降如何并行化？,∑处的并行，不同样本在不同机器上进行计算，计算完再进行合并,question2answer,,,,,,,,,,,
梯度下降如何并行化？,同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,question2answer,,,,,,,,,,,
LR明明是分类模型为什么叫回归？,观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,question2answer,,,,,,,,,,,
为什么LR可以用来做CTR预估？,1.点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match,question2answer,,,,,,,,,,,
为什么LR可以用来做CTR预估？,2.实现简单，方便并行，计算迭代速度很快,question2answer,,,,,,,,,,,
为什么LR可以用来做CTR预估？,3.可解释性强，可结合正则化等优化方法,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,特征之间尽可能独立,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,不独立所以我们把不独立的特征交叉了,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,还记得FM的思路？,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,离散特征,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,连续特征通常没有特别含义，31岁和32岁差在哪？,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,离散特征方便交叉考虑,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,在异常值处理上也更加方便,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,使的lr满足分布假设,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,什么分布假设？,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,在某种确定分类上的特征分布满足高斯分布,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wh7dd6bkj310w034gmb.jpg),question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,实际中不满足的很多，不满足我们通常就离散化，oneHotEncode,question2answer,,,,,,,,,,,
满足什么样条件的数据用LR最好？,此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路二：Exponentialmodel的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。,question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,二分类上：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmsc1vfkj30jm036wet.jpg),question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,化简即为sigmoid,question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,以上思路源自：PRML（PatternRecognitionandMachineLearning）,question2answer,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,question2answer,,,,,,,,,,,
利用几率odds的意义在哪？,直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,question2answer,,,,,,,,,,,
利用几率odds的意义在哪？,由预测0/1的类别扩展到了预测01的概率值,question2answer,,,,,,,,,,,
利用几率odds的意义在哪？,任意阶可导的优秀性质,question2answer,,,,,,,,,,,
Sigmoid函数到底起了什么作用？,"数据规约：\[0,1]",question2answer,,,,,,,,,,,
Sigmoid函数到底起了什么作用？,线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,question2answer,,,,,,,,,,,
Sigmoid函数到底起了什么作用？,sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,question2answer,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,更新速度只与真实的x和y相关，与激活函数无关，更新平稳,question2answer,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,question2answer,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,mse下的lr损失函数非凸，难以得到解析解,question2answer,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,question2answer,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,"way2:把激活函数换成tanh，因为tanh的值域范围为\[1,1],满足结果，推导不变",question2answer,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,question2answer,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,question2answer,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,question2answer,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,question2answer,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,结论：可以，加l2正则项后可用,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,原因：,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,w拆解的z的线性组合中的系数α来源,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,representertheorem的证明,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,question2answer,,,,,,,,,,,
LR可以用核么？可以怎么用？,如何将将W*表示成β的形式带到我们最佳化的问题,question2answer,,,,,,,,,,,
LR中的L1/L2正则项是啥？,"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",question2answer,,,,,,,,,,,
LR中的L1/L2正则项是啥？,"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",question2answer,,,,,,,,,,,
lr加l1还是l2好？,这个问题还可以换一个说法，l1和l2的各自作用。,question2answer,,,,,,,,,,,
lr加l1还是l2好？,刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,question2answer,,,,,,,,,,,
正则化是依据什么理论实现模型优化？,结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,question2answer,,,,,,,,,,,
LR可以用来处理非线性问题么？,特征交叉，类似fm,question2answer,,,,,,,,,,,
LR可以用来处理非线性问题么？,核逻辑回归，类似svm,question2answer,,,,,,,,,,,
LR可以用来处理非线性问题么？,线性变换+非线性激活，类似neuralnetwork,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,工程角度：,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,加速收敛,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,提高计算效率,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,理论角度:,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,梯度下降过程稳定,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,使得数据在某类上更服从高斯分布，满足前提假设，这个是必须要答出来的,question2answer,,,,,,,,,,,
为什么LR需要归一化或者取对数?,[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),question2answer,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,question2answer,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散后结合正则化可以进行特征筛选，更好防止过拟合,question2answer,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,数据的鲁棒性更好，不会因为无意义的连续值变动导致异常因素的影响，（31岁和32岁的差异在哪呢？）,question2answer,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散变量的计算相对于连续变量更快,question2answer,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,question2answer,,,,,,,,,,,
LR对比万物？,lr和线性回归,question2answer,,,,,,,,,,,
LR对比万物？,lr解用的极大似然，线性回归用的最小二乘,question2answer,,,,,,,,,,,
LR对比万物？,lr用于分类，线性回归用于回归,question2answer,,,,,,,,,,,
LR对比万物？,但两者都是广义线性回归GLM问题,question2answer,,,,,,,,,,,
LR对比万物？,两者对非线性问题的处理能力都是欠佳的,question2answer,,,,,,,,,,,
LR对比万物？,lr和最大熵,question2answer,,,,,,,,,,,
LR对比万物？,在解决二分类问题是等同的,question2answer,,,,,,,,,,,
LR对比万物？,lr和svm,question2answer,,,,,,,,,,,
LR对比万物？,都可分类，都是判别式模型思路,question2answer,,,,,,,,,,,
LR对比万物？,通常都是用正则化进行规约,question2answer,,,,,,,,,,,
LR对比万物？,模型上,question2answer,,,,,,,,,,,
LR对比万物？,lr是交叉熵，svm是HingeLoss,question2answer,,,,,,,,,,,
LR对比万物？,lr是全量数据拟合，svm是支持向量拟合,question2answer,,,,,,,,,,,
LR对比万物？,lr是参数估计有参数的前提假设，svm没有,question2answer,,,,,,,,,,,
LR对比万物？,lr依赖的是极大似然，svm依赖的是距离,question2answer,,,,,,,,,,,
LR对比万物？,lr和朴素贝叶斯,question2answer,,,,,,,,,,,
LR对比万物？,如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致,question2answer,,,,,,,,,,,
LR对比万物？,lr是判别模型，朴素贝叶斯是生成模型,question2answer,,,,,,,,,,,
LR对比万物？,lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,question2answer,,,,,,,,,,,
LR对比万物？,lr和最大熵模型,question2answer,,,,,,,,,,,
LR对比万物？,本质没有区别,question2answer,,,,,,,,,,,
LR对比万物？,最大熵模型在解决二分类问题就是逻辑回归,question2answer,,,,,,,,,,,
LR对比万物？,最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,question2answer,,,,,,,,,,,
LR梯度下降方法？,随机梯度下降,question2answer,,,,,,,,,,,
LR梯度下降方法？,局部最优解，可跳出鞍点,question2answer,,,,,,,,,,,
LR梯度下降方法？,计算快,question2answer,,,,,,,,,,,
LR梯度下降方法？,批梯度下降,question2answer,,,,,,,,,,,
LR梯度下降方法？,全局最优解,question2answer,,,,,,,,,,,
LR梯度下降方法？,计算量大,question2answer,,,,,,,,,,,
LR梯度下降方法？,mini批梯度下降,question2answer,,,,,,,,,,,
LR梯度下降方法？,综合以上两种方法,question2answer,,,,,,,,,,,
LR梯度下降方法？,除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,question2answer,,,,,,,,,,,
LR的优缺点？,优点,question2answer,,,,,,,,,,,
LR的优缺点？,简单，易部署，训练速度快,question2answer,,,,,,,,,,,
LR的优缺点？,模型下限较高,question2answer,,,,,,,,,,,
LR的优缺点？,可解释性强,question2answer,,,,,,,,,,,
LR的优缺点？,缺点,question2answer,,,,,,,,,,,
LR的优缺点？,只能线性可分,question2answer,,,,,,,,,,,
LR的优缺点？,数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),question2answer,,,,,,,,,,,
LR的优缺点？,模型上限较低,question2answer,,,,,,,,,,,
除了做分类，你还会用LR做什么？,特征筛选，特征的系数决定该特征的重要性,question2answer,,,,,,,,,,,
你有用过sklearn中的lr么？你用的是哪个包？,sklearn.linear_model.LogisticRegression,question2answer,,,,,,,,,,,
看过源码么？为什么去看？,看部分参数的解释,question2answer,,,,,,,,,,,
看过源码么？为什么去看？,比如dual、weight_class中的1:0还是0:1比,question2answer,,,,,,,,,,,
看过源码么？为什么去看？,比如输出值的形式，输出的格式,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,penalty是正则化，solver是函数优化方法,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,l1和l2选择参考上面讲的正则化部分,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,首先，决定是否为多分类的参数是multi_class,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,在二分类的时候，multi和ovr和auto都是一样的,question2answer,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,question2answer,,,,,,,,,,,
我的总结,逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,question2answer,,,,,,,,,,,
我的总结,逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,question2answer,,,,,,,,,,,
我的总结,逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,question2answer,,,,,,,,,,,
我的总结,逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,question2answer,,,,,,,,,,,
解释下随机森林?,随机森林=bagging+决策树,question2answer,,,,,,,,,,,
解释下随机森林?,随机：特征选择随机+数据采样随机,question2answer,,,,,,,,,,,
解释下随机森林?,特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机,question2answer,,,,,,,,,,,
解释下随机森林?,每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,question2answer,,,,,,,,,,,
解释下随机森林?,数据采样，是有放回的采样,question2answer,,,,,,,,,,,
解释下随机森林?,1个样本**未被选到**的概率为p=(11/N)^N=1/e，即为OOB,question2answer,,,,,,,,,,,
解释下随机森林?,森林：多决策树组合,question2answer,,,,,,,,,,,
解释下随机森林?,可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,question2answer,,,,,,,,,,,
随机森林用的是什么树？,CART树,question2answer,,,,,,,,,,,
随机森林的生成过程？,生成单棵决策树,question2answer,,,,,,,,,,,
随机森林的生成过程？,随机选取样本,question2answer,,,,,,,,,,,
随机森林的生成过程？,从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,question2answer,,,,,,,,,,,
随机森林的生成过程？,不需要剪枝，直到该节点的所有训练样例都属于同一类,question2answer,,,,,,,,,,,
随机森林的生成过程？,生成若干个决策树,question2answer,,,,,,,,,,,
解释下随机森林节点的分裂策略？,Gini系数,question2answer,,,,,,,,,,,
解释下随机森林节点的分裂策略？,在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),question2answer,,,,,,,,,,,
随机森林的损失函数是什么？,"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",question2answer,,,,,,,,,,,
随机森林的损失函数是什么？,回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,question2answer,,,,,,,,,,,
随机森林的损失函数是什么？,参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),question2answer,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,增加树的数量,question2answer,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,增加叶子结点的数据数量,question2answer,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),question2answer,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,question2answer,,,,,,,,,,,
随机森林特征选择的过程？,特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,question2answer,,,,,,,,,,,
随机森林特征选择的过程？,通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）,question2answer,,,,,,,,,,,
随机森林特征选择的过程？,是使用uniform或者gaussian抽取随机值替换原特征,question2answer,,,,,,,,,,,
是否用过随机森林，有什么技巧?,除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,要调整的参数主要是n_estimators和max_features,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,回归：max_features=n_features,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,分类：max_features=sqrt(n_features),question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,其他参数中,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,class_weight也可以调整正负样本的权重,question2answer,,,,,,,,,,,
RF的参数有哪些，如何调参？,max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,question2answer,,,,,,,,,,,
RF的优缺点 ？,优点:,question2answer,,,,,,,,,,,
RF的优缺点 ？,不同决策树可以由不同主机并行训练生成，效率很高,question2answer,,,,,,,,,,,
RF的优缺点 ？,随机森林算法继承了CART的优点,question2answer,,,,,,,,,,,
RF的优缺点 ？,将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,question2answer,,,,,,,,,,,
RF的优缺点 ？,缺点：,question2answer,,,,,,,,,,,
RF的优缺点 ？,没有严格数学理论支持,question2answer,,,,,,,,,,,
介绍一下Boosting的思想？,初始化训练一个弱学习器，初始化下的各条样本的权重一致,question2answer,,,,,,,,,,,
介绍一下Boosting的思想？,根据上一个弱学习器的结果，调整权重，使得错分的样本的权重变得更高,question2answer,,,,,,,,,,,
介绍一下Boosting的思想？,基于调整后的样本及样本权重训练下一个弱学习器,question2answer,,,,,,,,,,,
介绍一下Boosting的思想？,预测时直接串联综合各学习器的加权结果,question2answer,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,question2answer,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,question2answer,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值,question2answer,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,递归重复以上步骤，直到满足叶子结点上值的要求,question2answer,,,,,,,,,,,
有哪些直接利用了Boosting思想的树模型？,adaboost，gbdt等等,question2answer,,,,,,,,,,,
gbdt和boostingtree的boosting分别体现在哪里？,boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,question2answer,,,,,,,,,,,
gbdt和boostingtree的boosting分别体现在哪里？,gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,question2answer,,,,,,,,,,,
gbdt的中的tree是什么tree？有什么特征？,Carttree，但是都是回归树,question2answer,,,,,,,,,,,
常用回归问题的损失函数？,mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg),question2answer,,,,,,,,,,,
常用回归问题的损失函数？,负梯度：yh(x),question2answer,,,,,,,,,,,
常用回归问题的损失函数？,初始模型F0由目标变量的平均值给出,question2answer,,,,,,,,,,,
常用回归问题的损失函数？,绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg),question2answer,,,,,,,,,,,
常用回归问题的损失函数？,负梯度：sign(yh(x)),question2answer,,,,,,,,,,,
常用回归问题的损失函数？,初始模型F0由目标变量的中值给出,question2answer,,,,,,,,,,,
常用回归问题的损失函数？,Huber损失：mse和绝对损失的结合,question2answer,,,,,,,,,,,
常用回归问题的损失函数？,负梯度：yh(x)和sign(yh(x))分段函数,question2answer,,,,,,,,,,,
常用回归问题的损失函数？,它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,question2answer,,,,,,,,,,,
常用分类问题的损失函数？,对数似然损失函数,question2answer,,,,,,,,,,,
常用分类问题的损失函数？,"二元且标签y属于{1,+1}：??(??,??(??))=??????(1+??????(?????(??)))",question2answer,,,,,,,,,,,
常用分类问题的损失函数？,负梯度：y/(1+??????(?????(??))),question2answer,,,,,,,,,,,
常用分类问题的损失函数？,多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg),question2answer,,,,,,,,,,,
常用分类问题的损失函数？,"指数损失函数:??(??,??(??))=??????(?????(??))",question2answer,,,,,,,,,,,
常用分类问题的损失函数？,负梯度：y・??????(?????(??)),question2answer,,,,,,,,,,,
常用分类问题的损失函数？,除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,question2answer,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg),question2answer,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值??????,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,首先，根据feature切分后的损失均方差大小，选取最优的特征切分,question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,这样就完整的构造出一棵树：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bfr5cn5j303d01kjr6.jpg),question2answer,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg),question2answer,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),question2answer,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,m轮树模型可以写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h8zovulj305v00iglf.jpg),question2answer,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",question2answer,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",question2answer,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（??）,question2answer,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,question2answer,,,,,,,,,,,
Shrinkage收缩的作用？,每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,question2answer,,,,,,,,,,,
Shrinkage收缩的作用？,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94i9bokjzj306g00iglf.jpg),question2answer,,,,,,,,,,,
Shrinkage收缩的作用？,Shrinkage：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94iawlfq3j307600it8j.jpg),question2answer,,,,,,,,,,,
feature属性会被重复多次使用么？,会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,子采样,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,每一棵树基于原始原本的一个子集进行训练,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,rf是有放回采样，gbdt是无放回采样,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,特征子采样可以来控制模型整体的方差,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,利用Shrinkage收缩，控制每一棵子树的贡献度,question2answer,,,,,,,,,,,
gbdt如何进行正则化的？,每棵Cart树的枝剪,question2answer,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,question2answer,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,bagging，关注于提升分类器的泛化能力,question2answer,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,boosting，关注于提升分类器的精度,question2answer,,,,,,,,,,,
gbdt的优缺点？,优点：,question2answer,,,,,,,,,,,
gbdt的优缺点？,数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值,question2answer,,,,,,,,,,,
gbdt的优缺点？,使用一些健壮的损失函数，对异常值的鲁棒性非常强,question2answer,,,,,,,,,,,
gbdt的优缺点？,调参相对较简单,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,相同：,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,都是多棵树的组合,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,不同：,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,gbdt对异常值比rf更加敏感,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,gbdt是串行，rf是并行,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,gbdt是cart回归树，rf是cart分类回归树都可以,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能,question2answer,,,,,,,,,,,
gbdt和randomforest区别？,gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,question2answer,,,,,,,,,,,
GBDT和LR的差异？,从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,question2answer,,,,,,,,,,,
GBDT和LR的差异？,当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,question2answer,,,,,,,,,,,
XGboost缺点,每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,question2answer,,,,,,,,,,,
XGboost缺点,预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,question2answer,,,,,,,,,,,
XGboost缺点,levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,优点：时间开销由O(features)降低到O(bins),question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,缺点：很多数据精度被丢失，相当于用了正则,question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,利用leafwise代替levelwise,question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环,question2answer,,,,,,,,,,,
LightGBM对Xgboost的优化,直方图做差加速,question2answer,,,,,,,,,,,
LightGBM亮点,单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,question2answer,,,,,,,,,,,
LightGBM亮点,"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,显示的把树模型复杂度作为正则项加到优化目标中,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,优化目标计算中用到二阶泰勒展开代替一阶，更加准确,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,实现了分裂点寻找近似算法,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,暴力枚举,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,近似算法（分桶）,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,更加高效和快速,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,数据事先排序并且以block形式存储，有利于并行计算,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,基于分布式通信框架rabit，可以运行在MPI和yarn上,question2answer,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,实现做了面向体系结构的优化，针对cache和内存做了性能优化,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,模型优化上：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,基模型的优化：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,损失函数上的优化：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,gbdt没有在loss中带入结点个数和预测值的正则项,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,特征选择上的优化：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,正则化的优化：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,特征采样,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,样本采样,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,工程优化上：,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,"cacheaware,outofcorecomputation",question2answer,,,,,,,,,,,
xgboost和gbdt的区别？,支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit,question2answer,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,原始：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mjezeisj307401fmx0.jpg),question2answer,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg),question2answer,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,改变：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mldvhz5j30ay01k3yf.jpg),question2answer,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,J为叶子结点的个数，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mo56g1vj300o00e0s1.jpg)为第j个叶子结点中的最优值,question2answer,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒二阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mtjds7qj309r0193yg.jpg),question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg),question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg),question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,利用可导的函数逼近MAE或MAPE,question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,mse,question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,Huberloss,question2answer,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,PseudoHuberloss,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,暴力枚举,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,近似算法（approx）,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,离散值直接分桶,question2answer,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,连续值分位数分桶,question2answer,,,,,,,,,,,
xgboost如何处理缺失值？,训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,question2answer,,,,,,,,,,,
xgboost如何处理缺失值？,预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,question2answer,,,,,,,,,,,
xgboost在计算速度上有了哪些点上提升？,特征预排序,question2answer,,,,,,,,,,,
xgboost在计算速度上有了哪些点上提升？,按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,question2answer,,,,,,,,,,,
xgboost在计算速度上有了哪些点上提升？,block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间,question2answer,,,,,,,,,,,
xgboost特征重要性是如何得到的？,’weight‘：代表着某个特征被选作分裂结点的次数；,question2answer,,,,,,,,,,,
xgboost特征重要性是如何得到的？,’gain‘：使用该特征作为分类结点的信息增益；,question2answer,,,,,,,,,,,
xgboost特征重要性是如何得到的？,’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,question2answer,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,question2answer,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,question2answer,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂,question2answer,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,直接修改模型：,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,降低树的深度,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,增大叶子结点的权重,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,增大惩罚系数,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,subsample的力度变大，降低异常点的影响,question2answer,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,减小learningrate，提高estimator,question2answer,,,,,,,,,,,
xgboost如何调参数？,先确定learningrate和estimator,question2answer,,,,,,,,,,,
xgboost如何调参数？,再确定每棵树的基本信息，max_depth和min_child_weight,question2answer,,,,,,,,,,,
xgboost如何调参数？,再确定全局信息：比如最小分裂增益，子采样参数，正则参数,question2answer,,,,,,,,,,,
xgboost如何调参数？,重新降低learningrate，得到最优解,question2answer,,,,,,,,,,,
Attention对比RNN和CNN，分别有哪点你觉得的优势？,对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,question2answer,,,,,,,,,,,
Attention对比RNN和CNN，分别有哪点你觉得的优势？,对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,question2answer,,,,,,,,,,,
写出Attention的公式？,![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986pbyaj0j308t019t8l.jpg),question2answer,,,,,,,,,,,
解释你怎么理解Attention的公式的？,"Q:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rjgs7qj301400g741.jpg),K:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986rz9d1ej301a00g741.jpg),V:![](https://tva1.sinaimg.cn/large/006y8mN6ly1g986shjz4tj301900g741.jpg)",question2answer,,,,,,,,,,,
解释你怎么理解Attention的公式的？,首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列,question2answer,,,,,,,,,,,
解释你怎么理解Attention的公式的？,除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,question2answer,,,,,,,,,,,
解释你怎么理解Attention的公式的？,qk除了点击还可以直接拼接再内接一个参数变量等等,question2answer,,,,,,,,,,,
解释你怎么理解Attention的公式的？,MultiAttention只是重复了h次的Attention，最后把结果进行拼接,question2answer,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,增加了positionEmbedding,question2answer,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,可以直接随机初始化,question2answer,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,也可以参考Google的sin/cos位置初始化方法,question2answer,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,如此选取的原因之一是sin(a+b)=sin(a)cos(b)+cos(a)sin(b)。这很好的保证了位置p+k可以表示成p的线性变换，相对位置可解释,question2answer,,,,,,,,,,,
"Attention机制，里面的q,k,v分别代表什么？",Q：指的是query，相当于decoder的内容,question2answer,,,,,,,,,,,
"Attention机制，里面的q,k,v分别代表什么？",K：指的是key，相当于encoder的内容,question2answer,,,,,,,,,,,
"Attention机制，里面的q,k,v分别代表什么？",V：指的是value，相当于encoder的内容,question2answer,,,,,,,,,,,
"Attention机制，里面的q,k,v分别代表什么？",q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,question2answer,,,,,,,,,,,
为什么self-attention可以替代seq2seq？,seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,question2answer,,,,,,,,,,,
为什么self-attention可以替代seq2seq？,selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,question2answer,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,question2answer,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,"针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)",question2answer,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,E(XY)=E(X)E(Y)=0,question2answer,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9enzh4vzvj30gh017t8t.jpg),question2answer,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,question2answer,,,,,,,,,,,
你觉得bn过程是什么样的？,按batch进行期望和标准差计算,question2answer,,,,,,,,,,,
你觉得bn过程是什么样的？,对整体数据进行标准化,question2answer,,,,,,,,,,,
你觉得bn过程是什么样的？,对标准化的数据进行线性变换,question2answer,,,,,,,,,,,
你觉得bn过程是什么样的？,变换系数需要学习,question2answer,,,,,,,,,,,
手写一下bn过程？,"mu=1.0*np.sum(X,axis=0)/X.shape\[0]",question2answer,,,,,,,,,,,
手写一下bn过程？,Xmu=Xmu,question2answer,,,,,,,,,,,
手写一下bn过程？,sq=Xmu**2,question2answer,,,,,,,,,,,
手写一下bn过程？,"var=1.0*np.sum(sq,axis=0)/X.shape\[0]",question2answer,,,,,,,,,,,
手写一下bn过程？,out=alhpa*(XXmu)/np.sqrt(var+eps)+beta,question2answer,,,,,,,,,,,
知道LN么？讲讲原理,和bn过程近似，只是作用的方向是在维度上，而不是batch上,question2answer,,,,,,,,,,,
知道LN么？讲讲原理,这样做的好处就是不会受到batch大小不一致的影响,question2answer,,,,,,,,,,,
介绍残差网络,常见结构，CV里面用的比较多,question2answer,,,,,,,,,,,
介绍残差网络,y=F(x)+x,question2answer,,,,,,,,,,,
介绍残差网络,y=F(x)+indentity`*`x,question2answer,,,,,,,,,,,
残差网络为什么能解决梯度消失的问题,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is4x344xj304j01jglf.jpg),question2answer,,,,,,,,,,,
残差网络为什么能解决梯度消失的问题,![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8cigikj305s0180sl.jpg),question2answer,,,,,,,,,,,
残差网络为什么能解决梯度消失的问题,虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,question2answer,,,,,,,,,,,
残差网络残差作用,防止梯度消失,question2answer,,,,,,,,,,,
残差网络残差作用,恒等映射使得网络突破层数限制，避免网络退化,question2answer,,,,,,,,,,,
残差网络残差作用,对输出的变化更敏感,question2answer,,,,,,,,,,,
残差网络残差作用,X=5;F(X)=5.1;F(X)=H(X)+X=>H(X)=0.1,question2answer,,,,,,,,,,,
残差网络残差作用,X=5;F(X)=5.2;F(X)=H(X)+X=>H(X)=0.2,question2answer,,,,,,,,,,,
残差网络残差作用,H(X)变换了100%，去掉相同的主体部分，从而突出微小的变化,question2answer,,,,,,,,,,,
你平时有用过么？或者你在哪些地方遇到了,我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,question2answer,,,,,,,,,,,
你平时有用过么？或者你在哪些地方遇到了,Bert和Transform中attention部分残差网络用的比较频繁,question2answer,,,,,,,,,,,
Bert的双向体现在什么地方？,mask+attention，mask的word结合全部其他encoderword的信息,question2answer,,,,,,,,,,,
Bert的是怎样实现mask构造的？,MLM：将完整句子中的部分字mask，预测该mask词,question2answer,,,,,,,,,,,
Bert的是怎样实现mask构造的？,NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,question2answer,,,,,,,,,,,
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,question2answer,,,,,,,,,,,
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,随机替换也帮助训练修正了\[unused]和\[UNK],question2answer,,,,,,,,,,,
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,强迫文本记忆上下文信息,question2answer,,,,,,,,,,,
为什么BERT有3个嵌入层，它们都是如何实现的？,input_id是语义表达，和传统的w2v一样，方法也一样的lookup,question2answer,,,,,,,,,,,
为什么BERT有3个嵌入层，它们都是如何实现的？,"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",question2answer,,,,,,,,,,,
为什么BERT有3个嵌入层，它们都是如何实现的？,"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",question2answer,,,,,,,,,,,
bert的损失函数？,"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",question2answer,,,,,,,,,,,
bert的损失函数？,"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",question2answer,,,,,,,,,,,
bert的损失函数？,MLM+NSP即为最后的损失,question2answer,,,,,,,,,,,
手写一个multi-head attention？,"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)",question2answer,,,,,,,,,,,
长文本预测如何构造Tokens？,headonly：保存前510个token（留两个位置给\[CLS]和\[SEP]）,question2answer,,,,,,,,,,,
长文本预测如何构造Tokens？,tailonly：保存最后510个token,question2answer,,,,,,,,,,,
长文本预测如何构造Tokens？,head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,question2answer,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,modeling.py,question2answer,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",question2answer,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,在通过embedding_lookup把input_id向量化，如果存在句子之间的位置差异则需要对segment_id进行处理，否则无操作；再进行position_embedding操作,question2answer,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,question2answer,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],question2answer,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,BasicTokenizer：根据空格等进行普通的分词,question2answer,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,question2answer,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,question2answer,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,中文不处理，因为有词缀一说：解决OOV,question2answer,,,,,,,,,,,
Bert中如何获得词意和句意？,get_pooled_out代表了涵盖了整条语句的信息,question2answer,,,,,,,,,,,
Bert中如何获得词意和句意？,get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,question2answer,,,,,,,,,,,
源码中Attention后实际的流程是如何的？,Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,question2answer,,,,,,,,,,,
源码中Attention后实际的流程是如何的？,所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,question2answer,,,,,,,,,,,
为什么要在Attention后使用残差结构？,残差结构能够很好的消除层数加深所带来的信息损失问题,question2answer,,,,,,,,,,,
平时用官方Bert包么？耗时怎么样？,第三方：bert_serving,question2answer,,,,,,,,,,,
平时用官方Bert包么？耗时怎么样？,官方：bert_base,question2answer,,,,,,,,,,,
平时用官方Bert包么？耗时怎么样？,耗时：64GTesla，64max_seq_length，8090doc/s,question2answer,,,,,,,,,,,
平时用官方Bert包么？耗时怎么样？,在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,question2answer,,,,,,,,,,,
你觉得BERT比普通LM的新颖点？,mask机制,question2answer,,,,,,,,,,,
你觉得BERT比普通LM的新颖点？,next_sentence_predict机制,question2answer,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,question2answer,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,question2answer,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,question2answer,,,,,,,,,,,
阐述CRF原理？,"首先X,Y是随机变量，P(Y/X)是给定X条件下Y的条件概率分布",question2answer,,,,,,,,,,,
阐述CRF原理？,如果Y满足马尔可夫满足马尔科夫性，及不相邻则条件独立,question2answer,,,,,,,,,,,
阐述CRF原理？,则条件概率分布P(Y|X)为条件随机场CRF,question2answer,,,,,,,,,,,
线性链条件随机场的公式是？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ah66y1nxj30cr015747.jpg),question2answer,,,,,,,,,,,
CRF与HMM区别?,"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",question2answer,,,,,,,,,,,
CRF与HMM区别?,CRF是无向图，HMM是有向图,question2answer,,,,,,,,,,,
CRF与HMM区别?,CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,Bert把中文文本进行了embedding，得到每个字的表征向量,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,dense操作得到了每个文本文本对应的未归一化的tag概率,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,CRF层能从训练数据中获得约束性的规则,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,比如开始都是以xxxB，中间都是以xxxI，结尾都是以xxxE,question2answer,,,,,,,,,,,
Bert+crf中的各部分作用详解？,比如在只有label1I，label2I..的情况下，不会出现label1B,question2answer,,,,,,,,,,,
GolVe的损失函数？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9achsr4agj3094016a9x.jpg),question2answer,,,,,,,,,,,
解释GolVe的损失函数？,"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",question2answer,,,,,,,,,,,
为什么GolVe会用的相对比W2V少？,GloVe算法本身使用了全局信息，自然内存费的也就多一些,question2answer,,,,,,,,,,,
为什么GolVe会用的相对比W2V少？,公现矩阵，NXN的，N为词袋量,question2answer,,,,,,,,,,,
为什么GolVe会用的相对比W2V少？,W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能,question2answer,,,,,,,,,,,
如何处理未出现词？,"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",question2answer,,,,,,,,,,,
详述LDA原理？,从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),question2answer,,,,,,,,,,,
详述LDA原理？,多项式分布的共轭分布是狄利克雷分布,question2answer,,,,,,,,,,,
详述LDA原理？,二项式分布的共轭分布是Beta分布,question2answer,,,,,,,,,,,
详述LDA原理？,从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),question2answer,,,,,,,,,,,
详述LDA原理？,从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),question2answer,,,,,,,,,,,
详述LDA原理？,从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg),question2answer,,,,,,,,,,,
详述LDA原理？,文档里某个单词出现的概率可以用公式表示：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b9y6avdtj306e01jdfo.jpg),question2answer,,,,,,,,,,,
详述LDA原理？,采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,吉布斯采样,question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	先随机给每个词附上主题",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,通常会引申出如下几个问题：,question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	MCMC中什么叫做蒙特卡洛方法？",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		通常用于求概率密度的积分",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		用已知分布去评估未知分布",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		rejectacpect过程",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	马尔科夫链收敛性质？",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		非周期性，不能出现死循环",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		连通性，不能有断点",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	MCMC中什么叫做马尔科夫链采样过程？",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		再根据平稳矩阵后的条件概率p(x/xt)得到平稳分布的样本集(xn+1,xn+2...)",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	给定平稳矩阵如何得到概率分布样本集？",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		MC采样",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			给定任意的转移矩阵Q，已知π(i)p(i,j)=π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j)=π(j)Q(j,i)a(j,i)",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			根据Q的条件概率Q(x/xt)得到xt+1",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			u~uniform",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			u<π(xt+1)Q(xt+1,xt)则accept，就和蒙特模拟一样否则xt+1=xt",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			(xt,xt+1...)代表着我们的分布样本集",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		MH采样",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		Gibbs采样",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"			同上，差别在固定n?1个特征在某一个特征采样及坐标轮换采样",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	什么叫做坐标转换采样？",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		平面上任意两点满足细致平稳条件π(A)P(A>B)=π(B)P(B>A)",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		从条件概率分布P(x2|x(t)1)中采样得到样本x(t+1)2",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		从条件概率分布P(x1|x(t+1)2)中采样得到样本x(t+1)1",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,变分推断EM算法,question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"	EM过程",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		E：最小化相对熵，偏导为0得到变分参数",question2answer,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,"		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值",question2answer,,,,,,,,,,,
LDA的共轭分布解释下?,以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,question2answer,,,,,,,,,,,
PLSA和LDA的区别?,LDA是加了狄利克雷先验的PLSA,question2answer,,,,,,,,,,,
PLSA和LDA的区别?,PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,question2answer,,,,,,,,,,,
PLSA和LDA的区别?,LDA是贝叶斯思想，PLSA是MLE,question2answer,,,,,,,,,,,
怎么确定LDA的topic个数,对文档d属于哪个topic有多不确定，这个不确定程度就是Perplexity,question2answer,,,,,,,,,,,
怎么确定LDA的topic个数,多次尝试，调优perplexitytopicnumber曲线,question2answer,,,,,,,,,,,
怎么确定LDA的topic个数,困惑度越小，越容易过拟合,question2answer,,,,,,,,,,,
怎么确定LDA的topic个数,某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg),question2answer,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA比较是doc，word2vec是词,question2answer,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,question2answer,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,question2answer,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,question2answer,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,通常alpha为1/k，k为类别数，beta一般为0.01,question2answer,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,question2answer,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,question2answer,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,chucksize大一些更新的过程比较平稳，收敛更加平稳,question2answer,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,迭代次数一般不超过2000次，200万doc大约在2300次收敛,question2answer,,,,,,,,,,,
从隐藏层到输出的Softmax层的计算有哪些方法？,层次softmax,question2answer,,,,,,,,,,,
从隐藏层到输出的Softmax层的计算有哪些方法？,负采样,question2answer,,,,,,,,,,,
层次softmax流程？,构造HuffmanTree,question2answer,,,,,,,,,,,
层次softmax流程？,最大化对数似然函数,question2answer,,,,,,,,,,,
层次softmax流程？,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9adl5ex45j305g00qmwz.jpg),question2answer,,,,,,,,,,,
层次softmax流程？,输入层：是上下文的词语的词向量,question2answer,,,,,,,,,,,
层次softmax流程？,投影层：对其求和，所谓求和，就是简单的向量加法,question2answer,,,,,,,,,,,
层次softmax流程？,输出层：输出最可能的word,question2answer,,,,,,,,,,,
层次softmax流程？,沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,question2answer,,,,,,,,,,,
层次softmax流程？,对每层每个变量求偏导，参考sgd,question2answer,,,,,,,,,,,
负采样流程？,统计每个词出现对概率，丢弃词频过低对词,question2answer,,,,,,,,,,,
负采样流程？,每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）,question2answer,,,,,,,,,,,
负采样流程？,负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,question2answer,,,,,,,,,,,
word2vec两种方法各自的优势?,**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,question2answer,,,,,,,,,,,
word2vec两种方法各自的优势?,"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",question2answer,,,,,,,,,,,
怎么衡量学到的embedding的好坏?,从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,question2answer,,,,,,,,,,,
怎么衡量学到的embedding的好坏?,对item2vec得到的词向量进行聚类或者可视化,question2answer,,,,,,,,,,,
word2vec和glove区别？,word2vec是基于邻近词共现，glove是基于全文共现,question2answer,,,,,,,,,,,
word2vec和glove区别？,word2vec利用了负采样或者层次softmax加速，相对更快,question2answer,,,,,,,,,,,
word2vec和glove区别？,glove用了全局共现矩阵，更占内存资源,question2answer,,,,,,,,,,,
word2vec和glove区别？,word2vec是“predictive”的模型，而GloVe是“countbased”的模型,question2answer,,,,,,,,,,,
你觉得word2vec有哪些问题？,没考虑词序,question2answer,,,,,,,,,,,
你觉得word2vec有哪些问题？,对于中文依赖分词结果的好坏,question2answer,,,,,,,,,,,
你觉得word2vec有哪些问题？,新生词无法友好处理,question2answer,,,,,,,,,,,
你觉得word2vec有哪些问题？,无正则化处理,question2answer,,,,,,,,,,,
AutoML问题构成?,AutoML,question2feature,,,,,,,,,,,
特征工程选择思路？,特征,question2feature,,,,,,,,,,,
常见优化算法思路？,优化,question2feature,,,,,,,,,,,
AutoML参数选择所使用的方法？,AutoML,question2feature,,,,,,,,,,,
AutoML参数选择所使用的方法？,参数,question2feature,,,,,,,,,,,
讲讲贝叶斯优化如何在automl上应用？,优化,question2feature,,,,,,,,,,,
以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,搜索,question2feature,,,,,,,,,,,
写出全概率公式&贝叶斯公式,贝叶斯公式,question2feature,,,,,,,,,,,
说说你怎么理解为什么有全概率公式&贝叶斯公式,贝叶斯公式,question2feature,,,,,,,,,,,
模型训练为什么要引入偏差和方差？请理论论证。,训练,question2feature,,,,,,,,,,,
遇到过的机器学习中的偏差与方差问题？,机器学习,question2feature,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Bagging,question2feature,,,,,,,,,,,
就理论角度论证Bagging、Boosting的方差偏差问题,Boosting,question2feature,,,,,,,,,,,
遇到过的深度学习中的偏差与方差问题？,深度,question2feature,,,,,,,,,,,
方差、偏差与模型的复杂度之间的关系？,复杂度,question2feature,,,,,,,,,,,
什么叫判别模型？,判别,question2feature,,,,,,,,,,,
什么时候会选择生成/判别模型？,判别,question2feature,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,CRF,question2feature,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,EM,question2feature,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,熵,question2feature,,,,,,,,,,,
CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,混合,question2feature,,,,,,,,,,,
极大似然估计 - MLE,极大似然,question2feature,,,,,,,,,,,
极大似然估计 - MLE,估计,question2feature,,,,,,,,,,,
极大似然估计 - MLE,MLE,question2feature,,,,,,,,,,,
最大后验估计 - MAP,估计,question2feature,,,,,,,,,,,
最大后验估计 - MAP,MAP,question2feature,,,,,,,,,,,
极大似然估计与最大后验概率的区别？,极大似然,question2feature,,,,,,,,,,,
极大似然估计与最大后验概率的区别？,估计,question2feature,,,,,,,,,,,
到底什么是似然什么是概率估计？,估计,question2feature,,,,,,,,,,,
DNN与DeepFM之间的区别?,DNN,question2feature,,,,,,,,,,,
DNN与DeepFM之间的区别?,DeepFM,question2feature,,,,,,,,,,,
Wide&Deep与DeepFM之间的区别?,DeepFM,question2feature,,,,,,,,,,,
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,deepFM,question2feature,,,,,,,,,,,
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,欠拟合,question2feature,,,,,,,,,,,
你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,过拟合,question2feature,,,,,,,,,,,
DeepFM怎么优化的？,DeepFM,question2feature,,,,,,,,,,,
DeepFM怎么优化的？,优化,question2feature,,,,,,,,,,,
不定长文本数据如何输入deepFM？,长文本,question2feature,,,,,,,,,,,
不定长文本数据如何输入deepFM？,deepFM,question2feature,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,deepfm,question2feature,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,embedding,question2feature,,,,,,,,,,,
deepfm的embedding初始化有什么值得注意的地方吗？,初始化,question2feature,,,,,,,,,,,
activation unit的作用,activation,question2feature,,,,,,,,,,,
activation unit的作用,unit,question2feature,,,,,,,,,,,
DICE怎么设计的,DICE,question2feature,,,,,,,,,,,
DICE使用的过程中，有什么需要注意的地方,DICE,question2feature,,,,,,,,,,,
什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,特征,question2feature,,,,,,,,,,,
简单介绍一下XDeepFm的思想？,XDeepFm,question2feature,,,,,,,,,,,
和DCN比，有哪些核心的变化？,DCN,question2feature,,,,,,,,,,,
时间复杂度多少？,复杂度,question2feature,,,,,,,,,,,
如何进行负采样的？,负采样,question2feature,,,,,,,,,,,
item向量在softmax的时候你们怎么选择的？,向量,question2feature,,,,,,,,,,,
item向量在softmax的时候你们怎么选择的？,softmax,question2feature,,,,,,,,,,,
为什么不采取类似RNN的Sequence model？,RNN,question2feature,,,,,,,,,,,
YouTube如何避免百万量级的softmax问题的？,YouTube,question2feature,,,,,,,,,,,
YouTube如何避免百万量级的softmax问题的？,softmax,question2feature,,,,,,,,,,,
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,YouTube,question2feature,,,,,,,,,,,
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,训练,question2feature,,,,,,,,,,,
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,预测,question2feature,,,,,,,,,,,
serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,搜索,question2feature,,,,,,,,,,,
Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,feature,question2feature,,,,,,,,,,,
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,YouTube,question2feature,,,,,,,,,,,
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,留一法,question2feature,,,,,,,,,,,
在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,random,question2feature,,,,,,,,,,,
常见导数,导数,question2feature,,,,,,,,,,,
复合函数的运算法则,函数,question2feature,,,,,,,,,,,
切线方程,切线,question2feature,,,,,,,,,,,
法线方程,法线,question2feature,,,,,,,,,,,
拉格朗日中值定理,拉格朗日中值,question2feature,,,,,,,,,,,
期望,期望,question2feature,,,,,,,,,,,
均匀分布,均匀分布,question2feature,,,,,,,,,,,
拉普拉斯分布,拉普拉斯,question2feature,,,,,,,,,,,
泊松分布,泊松,question2feature,,,,,,,,,,,
切比雪夫不等式,切比雪夫不等式,question2feature,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均匀分布,question2feature,,,,,,,,,,,
0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,均值,question2feature,,,,,,,,,,,
泰勒公式,泰勒公式,question2feature,,,,,,,,,,,
常见泰勒公式,泰勒公式,question2feature,,,,,,,,,,,
迭代公式推导,迭代,question2feature,,,,,,,,,,,
范数,范数,question2feature,,,,,,,,,,,
特征值分解，特征向量,特征值,question2feature,,,,,,,,,,,
特征值分解，特征向量,分解,question2feature,,,,,,,,,,,
统计方法,统计,question2feature,,,,,,,,,,,
矩阵分解方法,矩阵分解,question2feature,,,,,,,,,,,
特征值和特征向量的本质是什么？,特征值,question2feature,,,,,,,,,,,
聚类的离群点检测,聚类,question2feature,,,,,,,,,,,
为什么要对数据进行采样平衡,平衡,question2feature,,,,,,,,,,,
是否一定需要对原始数据进行采样平衡,原始数据,question2feature,,,,,,,,,,,
是否一定需要对原始数据进行采样平衡,平衡,question2feature,,,,,,,,,,,
归一化和标准化之间的关系？,标准化,question2feature,,,,,,,,,,,
连续特征常用方法,特征,question2feature,,,,,,,,,,,
离散特征常用方法,离散,question2feature,,,,,,,,,,,
离散特征常用方法,特征,question2feature,,,,,,,,,,,
文本特征,特征,question2feature,,,,,,,,,,,
画一个最简单的最快速能实现的框架,快速,question2feature,,,,,,,,,,,
为什么要做特征选择？,特征选择,question2feature,,,,,,,,,,,
从哪些方面可以做特征选择？,特征选择,question2feature,,,,,,,,,,,
如何直接离散化？,离散,question2feature,,,,,,,,,,,
random方法,random,question2feature,,,,,,,,,,,
详述信息增益计算方法,增益,question2feature,,,,,,,,,,,
详述信息增益率计算方法,增益,question2feature,,,,,,,,,,,
ID3存在的问题,ID3,question2feature,,,,,,,,,,,
C4.5相对于ID3的改进点,ID3,question2feature,,,,,,,,,,,
CART的连续特征改进点,CART,question2feature,,,,,,,,,,,
CART的连续特征改进点,特征,question2feature,,,,,,,,,,,
CART分类树建立算法的具体流程,CART,question2feature,,,,,,,,,,,
CART分类树建立算法的具体流程,分类,question2feature,,,,,,,,,,,
CART分类树建立算法的具体流程,树,question2feature,,,,,,,,,,,
CART回归树建立算法的具体流程,CART,question2feature,,,,,,,,,,,
CART回归树建立算法的具体流程,回归,question2feature,,,,,,,,,,,
CART回归树建立算法的具体流程,树,question2feature,,,,,,,,,,,
CART输出结果的逻辑？,CART,question2feature,,,,,,,,,,,
CART输出结果的逻辑？,输出,question2feature,,,,,,,,,,,
CART输出结果的逻辑？,逻辑,question2feature,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,CART,question2feature,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,树,question2feature,,,,,,,,,,,
CART树算法的剪枝过程是怎么样的？,剪枝,question2feature,,,,,,,,,,,
简单介绍SVM?,SVM,question2feature,,,,,,,,,,,
什么是支持向量？,向量,question2feature,,,,,,,,,,,
SVM 和全部数据有关还是和局部数据有关?,SVM,question2feature,,,,,,,,,,,
加大训练数据量一定能提高SVM准确率吗？,训练,question2feature,,,,,,,,,,,
加大训练数据量一定能提高SVM准确率吗？,SVM,question2feature,,,,,,,,,,,
如何解决多分类问题？,分类,question2feature,,,,,,,,,,,
可以做回归吗，怎么做？,回归,question2feature,,,,,,,,,,,
SVM 能解决哪些问题？,SVM,question2feature,,,,,,,,,,,
介绍一下你知道的不同的SVM分类器？,SVM,question2feature,,,,,,,,,,,
SVM 软间隔与硬间隔表达式,SVM,question2feature,,,,,,,,,,,
SVM 软间隔与硬间隔表达式,表达式,question2feature,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,SVM,question2feature,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,对偶,question2feature,,,,,,,,,,,
SVM原问题和对偶问题的关系/解释原问题和对偶问题？,对偶,question2feature,,,,,,,,,,,
为什么要把原问题转换为对偶问题？,对偶,question2feature,,,,,,,,,,,
为什么求解对偶问题更加高效？,对偶,question2feature,,,,,,,,,,,
KKT限制条件，KKT条件有哪些，完整描述,KKT,question2feature,,,,,,,,,,,
KKT限制条件，KKT条件有哪些，完整描述,KKT,question2feature,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,拉格朗日,question2feature,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,优化方法,question2feature,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,损失,question2feature,,,,,,,,,,,
引入拉格朗日的优化方法后的损失函数解释,函数,question2feature,,,,,,,,,,,
核函数的作用是啥,核,question2feature,,,,,,,,,,,
核函数的作用是啥,函数,question2feature,,,,,,,,,,,
核函数的种类和应用场景,核,question2feature,,,,,,,,,,,
核函数的种类和应用场景,函数,question2feature,,,,,,,,,,,
如何选择核函数,核,question2feature,,,,,,,,,,,
如何选择核函数,函数,question2feature,,,,,,,,,,,
常用核函数的定义？,核,question2feature,,,,,,,,,,,
常用核函数的定义？,函数,question2feature,,,,,,,,,,,
核函数需要满足什么条件？,核,question2feature,,,,,,,,,,,
核函数需要满足什么条件？,函数,question2feature,,,,,,,,,,,
为什么在数据量大的情况下常常用lr代替核SVM？,lr,question2feature,,,,,,,,,,,
为什么在数据量大的情况下常常用lr代替核SVM？,核,question2feature,,,,,,,,,,,
为什么在数据量大的情况下常常用lr代替核SVM？,SVM,question2feature,,,,,,,,,,,
高斯核可以升到多少维？为什么,核,question2feature,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,SVM,question2feature,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,逻辑,question2feature,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,回归,question2feature,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,训练,question2feature,,,,,,,,,,,
SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,决策,question2feature,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",机器学习,question2feature,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",svm,question2feature,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",逻辑,question2feature,,,,,,,,,,,
"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",回归,question2feature,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,Linear,question2feature,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,SVM,question2feature,,,,,,,,,,,
Linear SVM 和 LR 有什么异同？,LR,question2feature,,,,,,,,,,,
损失函数是啥,损失,question2feature,,,,,,,,,,,
损失函数是啥,函数,question2feature,,,,,,,,,,,
最小二乘/梯度下降手推,二乘,question2feature,,,,,,,,,,,
最小二乘/梯度下降手推,梯度,question2feature,,,,,,,,,,,
介绍一下岭回归,回归,question2feature,,,,,,,,,,,
什么时候使用岭回归？,回归,question2feature,,,,,,,,,,,
什么时候用Lasso回归？,回归,question2feature,,,,,,,,,,,
请问从EM角度理解kmeans?,EM,question2feature,,,,,,,,,,,
请问从EM角度理解kmeans?,kmeans,question2feature,,,,,,,,,,,
为什么kmeans一定会收敛?,kmeans,question2feature,,,,,,,,,,,
为什么kmeans一定会收敛?,收敛,question2feature,,,,,,,,,,,
kmeans初始点除了随机选取之外的方法？,kmeans,question2feature,,,,,,,,,,,
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,贝叶斯公式,question2feature,,,,,,,,,,,
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,分类,question2feature,,,,,,,,,,,
讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,差别,question2feature,,,,,,,,,,,
出现估计概率值为 0 怎么处理,估计,question2feature,,,,,,,,,,,
朴素贝叶斯与 LR 区别？,LR,question2feature,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,logistic,question2feature,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,函数,question2feature,,,,,,,,,,,
logistic分布函数和密度函数，手绘大概的图像,函数,question2feature,,,,,,,,,,,
LR推导，基础5连问,LR,question2feature,,,,,,,,,,,
梯度下降如何并行化？,梯度,question2feature,,,,,,,,,,,
LR明明是分类模型为什么叫回归？,LR,question2feature,,,,,,,,,,,
LR明明是分类模型为什么叫回归？,分类,question2feature,,,,,,,,,,,
LR明明是分类模型为什么叫回归？,回归,question2feature,,,,,,,,,,,
为什么LR可以用来做CTR预估？,LR,question2feature,,,,,,,,,,,
为什么LR可以用来做CTR预估？,CTR,question2feature,,,,,,,,,,,
满足什么样条件的数据用LR最好？,LR,question2feature,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,LR,question2feature,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,sigmoid,question2feature,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,函数,question2feature,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,激活函数,question2feature,,,,,,,,,,,
LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,函数,question2feature,,,,,,,,,,,
利用几率odds的意义在哪？,odds,question2feature,,,,,,,,,,,
Sigmoid函数到底起了什么作用？,Sigmoid,question2feature,,,,,,,,,,,
Sigmoid函数到底起了什么作用？,函数,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,LR,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,极大似然,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,熵,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,损失,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,损失,question2feature,,,,,,,,,,,
LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,函数,question2feature,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,LR,question2feature,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,损失,question2feature,,,,,,,,,,,
LR中若标签为+1和-1，损失函数如何推导？,函数,question2feature,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,特征,question2feature,,,,,,,,,,,
如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,特征,question2feature,,,,,,,,,,,
LR可以用核么？可以怎么用？,LR,question2feature,,,,,,,,,,,
LR可以用核么？可以怎么用？,核,question2feature,,,,,,,,,,,
LR中的L1/L2正则项是啥？,LR,question2feature,,,,,,,,,,,
LR中的L1/L2正则项是啥？,L1,question2feature,,,,,,,,,,,
LR中的L1/L2正则项是啥？,L2,question2feature,,,,,,,,,,,
lr加l1还是l2好？,lr,question2feature,,,,,,,,,,,
lr加l1还是l2好？,l1,question2feature,,,,,,,,,,,
lr加l1还是l2好？,l2,question2feature,,,,,,,,,,,
正则化是依据什么理论实现模型优化？,优化,question2feature,,,,,,,,,,,
LR可以用来处理非线性问题么？,LR,question2feature,,,,,,,,,,,
LR可以用来处理非线性问题么？,非线性,question2feature,,,,,,,,,,,
为什么LR需要归一化或者取对数?,LR,question2feature,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,LR,question2feature,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,特征,question2feature,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散,question2feature,,,,,,,,,,,
为什么LR把特征离散化后效果更好？离散化的好处有哪些？,离散,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,逻辑,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,参数,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,目标,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,函数,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,逻辑,question2feature,,,,,,,,,,,
逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,回归,question2feature,,,,,,,,,,,
LR对比万物？,LR,question2feature,,,,,,,,,,,
LR梯度下降方法？,LR,question2feature,,,,,,,,,,,
LR梯度下降方法？,梯度,question2feature,,,,,,,,,,,
LR的优缺点？,LR,question2feature,,,,,,,,,,,
除了做分类，你还会用LR做什么？,分类,question2feature,,,,,,,,,,,
除了做分类，你还会用LR做什么？,LR,question2feature,,,,,,,,,,,
你有用过sklearn中的lr么？你用的是哪个包？,sklearn,question2feature,,,,,,,,,,,
你有用过sklearn中的lr么？你用的是哪个包？,lr,question2feature,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,sklearn,question2feature,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,LogisticRegression,question2feature,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,sklearn,question2feature,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,LogisticRegression,question2feature,,,,,,,,,,,
谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,分类,question2feature,,,,,,,,,,,
解释下随机森林?,随机森林,question2feature,,,,,,,,,,,
随机森林用的是什么树？,随机森林,question2feature,,,,,,,,,,,
随机森林用的是什么树？,树,question2feature,,,,,,,,,,,
随机森林的生成过程？,随机森林,question2feature,,,,,,,,,,,
解释下随机森林节点的分裂策略？,随机森林,question2feature,,,,,,,,,,,
随机森林的损失函数是什么？,随机森林,question2feature,,,,,,,,,,,
随机森林的损失函数是什么？,损失,question2feature,,,,,,,,,,,
随机森林的损失函数是什么？,函数,question2feature,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,随机森林,question2feature,,,,,,,,,,,
为了防止随机森林过拟合可以怎么做?,过拟合,question2feature,,,,,,,,,,,
随机森林特征选择的过程？,随机森林,question2feature,,,,,,,,,,,
随机森林特征选择的过程？,特征选择,question2feature,,,,,,,,,,,
是否用过随机森林，有什么技巧?,随机森林,question2feature,,,,,,,,,,,
RF的参数有哪些，如何调参？,RF,question2feature,,,,,,,,,,,
RF的参数有哪些，如何调参？,参数,question2feature,,,,,,,,,,,
RF的优缺点 ？,RF,question2feature,,,,,,,,,,,
介绍一下Boosting的思想？,Boosting,question2feature,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,二乘,question2feature,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,回归,question2feature,,,,,,,,,,,
最小二乘回归树的切分过程是怎么样的？,树,question2feature,,,,,,,,,,,
有哪些直接利用了Boosting思想的树模型？,Boosting,question2feature,,,,,,,,,,,
有哪些直接利用了Boosting思想的树模型？,树,question2feature,,,,,,,,,,,
gbdt和boostingtree的boosting分别体现在哪里？,gbdt,question2feature,,,,,,,,,,,
gbdt和boostingtree的boosting分别体现在哪里？,boostingtree,question2feature,,,,,,,,,,,
gbdt和boostingtree的boosting分别体现在哪里？,boosting,question2feature,,,,,,,,,,,
gbdt的中的tree是什么tree？有什么特征？,gbdt,question2feature,,,,,,,,,,,
gbdt的中的tree是什么tree？有什么特征？,特征,question2feature,,,,,,,,,,,
常用回归问题的损失函数？,回归,question2feature,,,,,,,,,,,
常用回归问题的损失函数？,损失,question2feature,,,,,,,,,,,
常用回归问题的损失函数？,函数,question2feature,,,,,,,,,,,
常用分类问题的损失函数？,分类,question2feature,,,,,,,,,,,
常用分类问题的损失函数？,损失,question2feature,,,,,,,,,,,
常用分类问题的损失函数？,函数,question2feature,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,gbdt,question2feature,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,损失,question2feature,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,函数,question2feature,,,,,,,,,,,
什么是gbdt中的损失函数的负梯度？,梯度,question2feature,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,损失,question2feature,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,函数,question2feature,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,梯度,question2feature,,,,,,,,,,,
如何用损失函数的负梯度实现gbdt？,gbdt,question2feature,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,拟合,question2feature,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,损失,question2feature,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,函数,question2feature,,,,,,,,,,,
拟合损失函数的负梯度为什么是可行的？,梯度,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,损失,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,函数,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,梯度,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,拟合,question2feature,,,,,,,,,,,
即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,梯度,question2feature,,,,,,,,,,,
feature属性会被重复多次使用么？,feature,question2feature,,,,,,,,,,,
gbdt如何进行正则化的？,gbdt,question2feature,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,树,question2feature,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,集成学习,question2feature,,,,,,,,,,,
为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,树,question2feature,,,,,,,,,,,
gbdt的优缺点？,gbdt,question2feature,,,,,,,,,,,
gbdt和randomforest区别？,gbdt,question2feature,,,,,,,,,,,
gbdt和randomforest区别？,randomforest,question2feature,,,,,,,,,,,
GBDT和LR的差异？,GBDT,question2feature,,,,,,,,,,,
GBDT和LR的差异？,LR,question2feature,,,,,,,,,,,
XGboost缺点,XGboost,question2feature,,,,,,,,,,,
LightGBM对Xgboost的优化,LightGBM,question2feature,,,,,,,,,,,
LightGBM对Xgboost的优化,Xgboost,question2feature,,,,,,,,,,,
LightGBM对Xgboost的优化,优化,question2feature,,,,,,,,,,,
LightGBM亮点,LightGBM,question2feature,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,xgboost,question2feature,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,gbdt,question2feature,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,boosting,question2feature,,,,,,,,,,,
xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,优化,question2feature,,,,,,,,,,,
xgboost和gbdt的区别？,xgboost,question2feature,,,,,,,,,,,
xgboost和gbdt的区别？,gbdt,question2feature,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,xgboost,question2feature,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,优化,question2feature,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,目标,question2feature,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,损失,question2feature,,,,,,,,,,,
xgboost优化目标/损失函数改变成什么样？,函数,question2feature,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,xgboost,question2feature,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,MAE,question2feature,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,MAPE,question2feature,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,目标,question2feature,,,,,,,,,,,
xgboost如何使用MAE或MAPE作为目标函数？,函数,question2feature,,,,,,,,,,,
xgboost如何寻找分裂节点的候选集？,xgboost,question2feature,,,,,,,,,,,
xgboost如何处理缺失值？,xgboost,question2feature,,,,,,,,,,,
xgboost在计算速度上有了哪些点上提升？,xgboost,question2feature,,,,,,,,,,,
xgboost特征重要性是如何得到的？,xgboost,question2feature,,,,,,,,,,,
xgboost特征重要性是如何得到的？,特征,question2feature,,,,,,,,,,,
xgboost特征重要性是如何得到的？,重要性,question2feature,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,XGBoost,question2feature,,,,,,,,,,,
XGBoost中如何对树进行剪枝？,剪枝,question2feature,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,XGBoost,question2feature,,,,,,,,,,,
XGBoost模型如果过拟合了怎么解决？,过拟合,question2feature,,,,,,,,,,,
xgboost如何调参数？,xgboost,question2feature,,,,,,,,,,,
xgboost如何调参数？,参数,question2feature,,,,,,,,,,,
Attention对比RNN和CNN，分别有哪点你觉得的优势？,Attention,question2feature,,,,,,,,,,,
Attention对比RNN和CNN，分别有哪点你觉得的优势？,RNN,question2feature,,,,,,,,,,,
Attention对比RNN和CNN，分别有哪点你觉得的优势？,CNN,question2feature,,,,,,,,,,,
写出Attention的公式？,Attention,question2feature,,,,,,,,,,,
解释你怎么理解Attention的公式的？,Attention,question2feature,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,Attention,question2feature,,,,,,,,,,,
Attention模型怎么避免词袋模型的顺序问题的困境的？,词袋模型,question2feature,,,,,,,,,,,
"Attention机制，里面的q,k,v分别代表什么？",Attention,question2feature,,,,,,,,,,,
为什么self-attention可以替代seq2seq？,seq2seq,question2feature,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,维度,question2feature,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,点积,question2feature,,,,,,,,,,,
维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,维度,question2feature,,,,,,,,,,,
你觉得bn过程是什么样的？,bn,question2feature,,,,,,,,,,,
手写一下bn过程？,bn,question2feature,,,,,,,,,,,
知道LN么？讲讲原理,LN,question2feature,,,,,,,,,,,
残差网络为什么能解决梯度消失的问题,梯度,question2feature,,,,,,,,,,,
Bert的双向体现在什么地方？,Bert,question2feature,,,,,,,,,,,
Bert的是怎样实现mask构造的？,Bert,question2feature,,,,,,,,,,,
Bert的是怎样实现mask构造的？,mask,question2feature,,,,,,,,,,,
在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,mask,question2feature,,,,,,,,,,,
为什么BERT有3个嵌入层，它们都是如何实现的？,BERT,question2feature,,,,,,,,,,,
bert的损失函数？,bert,question2feature,,,,,,,,,,,
bert的损失函数？,损失,question2feature,,,,,,,,,,,
bert的损失函数？,函数,question2feature,,,,,,,,,,,
手写一个multi-head attention？,attention,question2feature,,,,,,,,,,,
长文本预测如何构造Tokens？,长文本,question2feature,,,,,,,,,,,
长文本预测如何构造Tokens？,预测,question2feature,,,,,,,,,,,
你用过什么模块？bert流程是怎么样的？,bert,question2feature,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,分词,question2feature,,,,,,,,,,,
知道分词模块：FullTokenizer做了哪些事情么？,FullTokenizer,question2feature,,,,,,,,,,,
Bert中如何获得词意和句意？,Bert,question2feature,,,,,,,,,,,
源码中Attention后实际的流程是如何的？,Attention,question2feature,,,,,,,,,,,
为什么要在Attention后使用残差结构？,Attention,question2feature,,,,,,,,,,,
平时用官方Bert包么？耗时怎么样？,Bert,question2feature,,,,,,,,,,,
你觉得BERT比普通LM的新颖点？,BERT,question2feature,,,,,,,,,,,
你觉得BERT比普通LM的新颖点？,LM,question2feature,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,elmo,question2feature,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,GPT,question2feature,,,,,,,,,,,
elmo、GPT、bert三者之间有什么区别？,bert,question2feature,,,,,,,,,,,
阐述CRF原理？,CRF,question2feature,,,,,,,,,,,
CRF与HMM区别?,CRF,question2feature,,,,,,,,,,,
CRF与HMM区别?,HMM,question2feature,,,,,,,,,,,
Bert+crf中的各部分作用详解？,Bert,question2feature,,,,,,,,,,,
Bert+crf中的各部分作用详解？,crf,question2feature,,,,,,,,,,,
GolVe的损失函数？,GolVe,question2feature,,,,,,,,,,,
GolVe的损失函数？,损失,question2feature,,,,,,,,,,,
GolVe的损失函数？,函数,question2feature,,,,,,,,,,,
解释GolVe的损失函数？,GolVe,question2feature,,,,,,,,,,,
解释GolVe的损失函数？,损失,question2feature,,,,,,,,,,,
解释GolVe的损失函数？,函数,question2feature,,,,,,,,,,,
为什么GolVe会用的相对比W2V少？,GolVe,question2feature,,,,,,,,,,,
为什么GolVe会用的相对比W2V少？,W2V,question2feature,,,,,,,,,,,
详述LDA原理？,LDA,question2feature,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,LDA,question2feature,,,,,,,,,,,
LDA中的主题矩阵如何计算?词分布矩阵如何计算？,主题,question2feature,,,,,,,,,,,
LDA的共轭分布解释下?,LDA,question2feature,,,,,,,,,,,
PLSA和LDA的区别?,LDA,question2feature,,,,,,,,,,,
怎么确定LDA的topic个数,LDA,question2feature,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA,question2feature,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,Word2Vec,question2feature,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,LDA,question2feature,,,,,,,,,,,
LDA和Word2Vec区别？LDA和Doc2Vec区别？,Doc2Vec,question2feature,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,LDA,question2feature,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,Dirichlet,question2feature,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,参数,question2feature,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,beta,question2feature,,,,,,,,,,,
LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,trick,question2feature,,,,,,,,,,,
从隐藏层到输出的Softmax层的计算有哪些方法？,隐藏层,question2feature,,,,,,,,,,,
从隐藏层到输出的Softmax层的计算有哪些方法？,输出,question2feature,,,,,,,,,,,
从隐藏层到输出的Softmax层的计算有哪些方法？,Softmax,question2feature,,,,,,,,,,,
层次softmax流程？,softmax,question2feature,,,,,,,,,,,
负采样流程？,负采样,question2feature,,,,,,,,,,,
word2vec两种方法各自的优势?,word2vec,question2feature,,,,,,,,,,,
怎么衡量学到的embedding的好坏?,embedding,question2feature,,,,,,,,,,,
word2vec和glove区别？,word2vec,question2feature,,,,,,,,,,,
word2vec和glove区别？,glove,question2feature,,,,,,,,,,,
你觉得word2vec有哪些问题？,word2vec,question2feature,,,,,,,,,,,
特征选择,特征选择,answer2feature,,,,,,,,,,,
有监督的特征选择,特征选择,answer2feature,,,,,,,,,,,
基于模型，lr的系数，树模型的importance等等,lr,answer2feature,,,,,,,,,,,
基于模型，lr的系数，树模型的importance等等,树,answer2feature,,,,,,,,,,,
无监督的特征选择,特征选择,answer2feature,,,,,,,,,,,
基于统计信息的，熵、相关性、KL系数,统计,answer2feature,,,,,,,,,,,
基于统计信息的，熵、相关性、KL系数,熵,answer2feature,,,,,,,,,,,
基于统计信息的，熵、相关性、KL系数,相关性,answer2feature,,,,,,,,,,,
基于统计信息的，熵、相关性、KL系数,KL,answer2feature,,,,,,,,,,,
基于方差，因子分解，PCA主成分分享，方差系数,分解,answer2feature,,,,,,,,,,,
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,非线性,answer2feature,,,,,,,,,,,
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,特征,answer2feature,,,,,,,,,,,
各自模型的优劣势，线性非线性，低阶特征/高阶特征交互，场景选择,特征,answer2feature,,,,,,,,,,,
参数选择,参数,answer2feature,,,,,,,,,,,
SGD,SGD,answer2feature,,,,,,,,,,,
GD,GD,answer2feature,,,,,,,,,,,
FTRL,FTRL,answer2feature,,,,,,,,,,,
暴力搜索,搜索,answer2feature,,,,,,,,,,,
拟合搜索,拟合,answer2feature,,,,,,,,,,,
拟合搜索,搜索,answer2feature,,,,,,,,,,,
贝叶斯优化,优化,answer2feature,,,,,,,,,,,
Meta学习,Meta,answer2feature,,,,,,,,,,,
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,拟合,answer2feature,,,,,,,,,,,
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature,,,,,,,,,,,
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature,,,,,,,,,,,
目的：通过拟合参数和模型能力之间的关系：模型能力=f(超参数)，找到最合适的超参数,参数,answer2feature,,,,,,,,,,,
随机选取几个超参数进行f拟合，得到先验数据集合D,参数,answer2feature,,,,,,,,,,,
随机选取几个超参数进行f拟合，得到先验数据集合D,拟合,answer2feature,,,,,,,,,,,
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,参数,answer2feature,,,,,,,,,,,
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,参数,answer2feature,,,,,,,,,,,
根据模型M得到预测出一些较优超参数，并把该超参数对于的f结果加入原始数据集合D,原始数据,answer2feature,,,,,,,,,,,
稳定性：同一组超参数的预测结果在不同轮次不一致,稳定性,answer2feature,,,,,,,,,,,
稳定性：同一组超参数的预测结果在不同轮次不一致,参数,answer2feature,,,,,,,,,,,
稳定性：同一组超参数的预测结果在不同轮次不一致,预测,answer2feature,,,,,,,,,,,
f函数需要多次计算，资源耗费时间损失,函数,answer2feature,,,,,,,,,,,
f函数需要多次计算，资源耗费时间损失,损失,answer2feature,,,,,,,,,,,
难以确定比较通用的拟合模型f,拟合,answer2feature,,,,,,,,,,,
基于均值和方差的平衡结果,均值,answer2feature,,,,,,,,,,,
基于均值和方差的平衡结果,平衡,answer2feature,,,,,,,,,,,
EI(期望提升),期望,answer2feature,,,,,,,,,,,
贝叶斯公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wefkh3r5j30l203iq3c.jpg),贝叶斯公式,answer2feature,,,,,,,,,,,
贝叶斯公式为当给定条件发生变化后，会导致事件发生的可能性发生何种变化。**key：概率变化**,贝叶斯公式,answer2feature,,,,,,,,,,,
先验概率（priorprobability）：指根据以往经验和分析。在实验或采样前就可以得到的概率。key:简单的暴力统计,统计,answer2feature,,,,,,,,,,,
有一个木桶，里面有M个白球，小明每分钟从桶中随机取出一个球涂成红色（无论白或红都涂红）再放回，问小明将桶中球全部涂红的期望时间是多少？,期望,answer2feature,,,,,,,,,,,
P[i]代表M个球中已经有i个球是红色后，还需要的时间期望，去将所有球都变成红色。,期望,answer2feature,,,,,,,,,,,
解释一下，每一次抽取，(i/M)概率不变，(1i/M)进入下一轮，额外加一次本次操作,抽取,answer2feature,,,,,,,,,,,
期望值与真实值之间的波动程度，衡量的是**稳定性**,稳定性,answer2feature,,,,,,,,,,,
期望值与真实值之间的一致差距，衡量的是**准确性**,准确性,answer2feature,,,,,,,,,,,
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,优化,answer2feature,,,,,,,,,,,
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,监督学习,answer2feature,,,,,,,,,,,
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,优化,answer2feature,,,,,,,,,,,
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,误差,answer2feature,,,,,,,,,,,
优化监督学习=优化模型的泛化误差，模型的泛化误差可分解为偏差、方差与噪声之和,误差,answer2feature,,,,,,,,,,,
"以回归任务为例,其实更准确的公式为：**Err=bias^2+var+irreducibleerror^2**",回归,answer2feature,,,,,,,,,,,
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature,,,,,,,,,,,
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature,,,,,,,,,,,
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature,,,,,,,,,,,
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,期望,answer2feature,,,,,,,,,,,
训练数据D训练的模型称之为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)，当我们使用相同的算法，但使用不同的训练数据D时就会得到多个![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwz21bi9j300y00pdfl.jpg)。则![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型的期望，即使用某一算法训练模型所能得到的稳定的平均水平。,训练,answer2feature,,,,,,,,,,,
方差：模型的稳定性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx3a2qc3j306400ot8j.jpg),稳定性,answer2feature,,,,,,,,,,,
偏差：模型的准确性：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx6rqg34j305g00odfn.jpg),准确性,answer2feature,,,,,,,,,,,
"Err(f,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lx8dhkplj300e00m0s6.jpg))为可解释规则误差",误差,answer2feature,,,,,,,,,,,
f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,预测,answer2feature,,,,,,,,,,,
f为真实值，固定；![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)代表了这个模型不同数据预测结果的期望，固定；所以f![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lwzqiav7j301q00pq2p.jpg)固定,期望,answer2feature,,,,,,,,,,,
过高复杂度的模型，对训练集进行过拟合,复杂度,answer2feature,,,,,,,,,,,
过高复杂度的模型，对训练集进行过拟合,训练,answer2feature,,,,,,,,,,,
过高复杂度的模型，对训练集进行过拟合,过拟合,answer2feature,,,,,,,,,,,
带来的后果就是在训练集合上效果非常好，但是在校验集合上效果极差,训练,answer2feature,,,,,,,,,,,
更加形象的理解就是用一条高次方程去拟合线性数据,高次方程,answer2feature,,,,,,,,,,,
更加形象的理解就是用一条高次方程去拟合线性数据,拟合,answer2feature,,,,,,,,,,,
在数据量不变的情况下，减少特征维度,特征,answer2feature,,,,,,,,,,,
在数据量不变的情况下，减少特征维度,维度,answer2feature,,,,,,,,,,,
减少的特征维度如果是共线性的维度，对原模型没有任何影响,特征,answer2feature,,,,,,,,,,,
减少的特征维度如果是共线性的维度，对原模型没有任何影响,维度,answer2feature,,,,,,,,,,,
减少的特征维度如果是共线性的维度，对原模型没有任何影响,维度,answer2feature,,,,,,,,,,,
罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,回归,answer2feature,,,,,,,,,,,
罗辑回归中，如果把一列特征重复2遍，会对最后的结果产生影响么？,特征,answer2feature,,,,,,,,,,,
尝试获得更多的特征,特征,answer2feature,,,,,,,,,,,
从数据入手，进行特征交叉，或者特征的embedding化,特征,answer2feature,,,,,,,,,,,
从数据入手，进行特征交叉，或者特征的embedding化,特征,answer2feature,,,,,,,,,,,
从数据入手，进行特征交叉，或者特征的embedding化,embedding,answer2feature,,,,,,,,,,,
尝试增加多项式特征,多项式,answer2feature,,,,,,,,,,,
尝试增加多项式特征,特征,answer2feature,,,,,,,,,,,
从模型入手，增加更多线性及非线性变化，提高模型的复杂度,非线性,answer2feature,,,,,,,,,,,
从模型入手，增加更多线性及非线性变化，提高模型的复杂度,复杂度,answer2feature,,,,,,,,,,,
特征越稀疏，高方差的风险越高,特征,answer2feature,,,,,,,,,,,
特征越稀疏，高方差的风险越高,稀疏,answer2feature,,,,,,,,,,,
多个线性变换=一个线性变换，多个非线性变换不一定=一个多线性变换,非线性,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,分解,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,Bagging,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Bagging主要关注降低方差，因此它在不剪枝决策树，神经网络等易受样本扰动的学习器上效果更为明显。,剪枝,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,分解,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,Boosting,answer2feature,,,,,,,,,,,
从偏差方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成。,Boosting,answer2feature,,,,,,,,,,,
bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,bagging,answer2feature,,,,,,,,,,,
bagging和boosting都要n个模型，假设基模型权重![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyh07lb3j300a00c0ok.jpg)，相关系数![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyi3r97uj300900c0oe.jpg)，方差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzqhhkfwj300h00g0rq.jpg)均相等,boosting,answer2feature,,,,,,,,,,,
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature,,,,,,,,,,,
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature,,,,,,,,,,,
"Var(x,y)=Var(x)+Var(y)+2Cov(x,y)",Var,answer2feature,,,,,,,,,,,
Bagging,Bagging,answer2feature,,,,,,,,,,,
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyk4wjxkj302v00qa9u.jpg),Var,answer2feature,,,,,,,,,,,
所以，化简以上的式子可得：Var(F)=m*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyw59y9oj300h00g0rq.jpg)*![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lywg4870j300g00k0r2.jpg)+![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lyzygc4lj304300lgle.jpg),Var,answer2feature,,,,,,,,,,,
以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,bagging,answer2feature,,,,,,,,,,,
以上为通式，对于bagging来说，每个基模型的权重等于1/m且期望近似相等，所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz1clt7wj301g011gld.jpg)，带入即可,期望,answer2feature,,,,,,,,,,,
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lz7qrpwsj304f015t8i.jpg),Var,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,期望,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望，这也就意味着整体模型的偏差和基模型的偏差近似,期望,answer2feature,,,,,,,,,,,
整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,相关性,answer2feature,,,,,,,,,,,
整体模型的方差小于等于基模型的方差（当相关性为1时取等号），随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,过拟合,answer2feature,,,,,,,,,,,
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,bagging,answer2feature,,,,,,,,,,,
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,过拟合,answer2feature,,,,,,,,,,,
bagging的防止过拟合的极限在1/m项趋近于0，所以并不是可以无穷的降低方差达到提高模型准确性的效果的,准确性,answer2feature,,,,,,,,,,,
Boosting同理,Boosting,answer2feature,,,,,,,,,,,
boosting的前提是弱模型之间高度相关，我们不妨设相关度为1,boosting,answer2feature,,,,,,,,,,,
Var(F)=![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8lzwvy93dj302k00kmwx.jpg),Var,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,期望,answer2feature,,,,,,,,,,,
整体模型的期望近似于基模型的期望之和，模型越多期望越容易拟合真实值,拟合,answer2feature,,,,,,,,,,,
整体模型的方差等于基模型的数量平方成正比，越多模型不稳定性越高，越容易过拟合。,过拟合,answer2feature,,,,,,,,,,,
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,拟合,answer2feature,,,,,,,,,,,
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,训练,answer2feature,,,,,,,,,,,
神经网络的拟合能力非常强，因此它的训练误差（偏差）通常较小；,误差,answer2feature,,,,,,,,,,,
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,拟合,answer2feature,,,,,,,,,,,
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,误差,answer2feature,,,,,,,,,,,
但是过强的拟合能力会导致较大的方差，使模型的测试误差（泛化误差）增大；,误差,answer2feature,,,,,,,,,,,
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,深度,answer2feature,,,,,,,,,,,
因此深度学习的核心工作之一就是研究如何降低模型的泛化误差，这类方法统称为正则化方法。,误差,answer2feature,,,,,,,,,,,
dropout,dropout,answer2feature,,,,,,,,,,,
dense中的normalization,dense,answer2feature,,,,,,,,,,,
dense中的normalization,normalization,answer2feature,,,,,,,,,,,
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",自变量,answer2feature,,,,,,,,,,,
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",因变量,answer2feature,,,,,,,,,,,
"求自变量和因变量的联合概率分布，P(x,y);再通过贝叶斯公式：P(y/x)=p(x,y)/p(x)。",贝叶斯公式,answer2feature,,,,,,,,,,,
求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,自变量,answer2feature,,,,,,,,,,,
求一个通过自变量能够表示出因变量的公式y=F(x)或者p(y/x)，核心是对于x找到一个最合适的公式得到y,因变量,answer2feature,,,,,,,,,,,
明确一点：绝大多数情况下，判别模型都要比生成模型效果好，而且需要的数据量和前提假设都要小于生成模型，上面的概念中可得原因。,判别,answer2feature,,,,,,,,,,,
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,判别,answer2feature,,,,,,,,,,,
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,拟合,answer2feature,,,,,,,,,,,
但是如果存在异常点检测的需求，或者样本中有部分异常点的情况下，判别模型会结合所有数据进行拟合；而生成模型则是通过分布拟合的方式减少该部分的影响,拟合,answer2feature,,,,,,,,,,,
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,分类,answer2feature,,,,,,,,,,,
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,判别,answer2feature,,,,,,,,,,,
如果明明知道隐变量在此次分类的过程中起到非常巨大作用的情况下，判别模型对隐变量的学习往往通过人为构造，更加不确定性,不确定性,answer2feature,,,,,,,,,,,
FM/FFM,FM,answer2feature,,,,,,,,,,,
FM/FFM,FFM,answer2feature,,,,,,,,,,,
线性Dense,Dense,answer2feature,,,,,,,,,,,
非线性激活,非线性,answer2feature,,,,,,,,,,,
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,knn,answer2feature,,,,,,,,,,,
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,svm,answer2feature,,,,,,,,,,,
因为这几个模型中都有概率计算的过程，不像knn，svm等都是距离计算一看就知道是判别模型。,判别,answer2feature,,,,,,,,,,,
生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,混合,answer2feature,,,,,,,,,,,
生成式模型：朴素贝叶斯，混合高斯模型，马尔科夫随机场，EM,EM,answer2feature,,,,,,,,,,,
仔细看过这些模型细节的朋友都应该知道，他们最后都是判断x属于拟合一个正负样本分布，然后对比属于正负样本的概率,拟合,answer2feature,,,,,,,,,,,
判别式模型：最大熵模型，CRF,判别式,answer2feature,,,,,,,,,,,
判别式模型：最大熵模型，CRF,熵,answer2feature,,,,,,,,,,,
判别式模型：最大熵模型，CRF,CRF,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,判别,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,函数,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,函数,answer2feature,,,,,,,,,,,
无论是生成还是判别模型都是来求有监督模型的，目的就是求分类函数y=F(x)或者条件概率分布P(y/x)，通过分类函数或者条件概率函数进行数据分类,分类,answer2feature,,,,,,,,,,,
算出属于正负样本的概率在相互对比的就是生成模型，直接得到结果概率的就是判别模型,判别,answer2feature,,,,,,,,,,,
生成模型得分布，判别模型得最优划分,判别,answer2feature,,,,,,,,,,,
生成模型可以得到判别模型，反之不成立,判别,answer2feature,,,,,,,,,,,
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,判别,answer2feature,,,,,,,,,,,
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,判别,answer2feature,,,,,,,,,,,
生成模型是求联合概率分布，判别模型是求条件概率分布，这句话不错，但是如果只回答到这，我认为是背答案式回答，其实生成模型的也是求的条件概率是通过的是联合概率得到的，而判别模型是之间得到，用来做分类的话，大概率都是条件概率作为最终结果；补充一下，二分情况下，如果单纯只用联合概率也可以判断,分类,answer2feature,,,,,,,,,,,
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,函数,answer2feature,,,,,,,,,,,
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,函数,answer2feature,,,,,,,,,,,
似然函数可以表示为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g924j4sibwj306g00r0sk.jpg)。目的使求的使似然函数能够达到最大情况下的θ'即为未知参数θ最大似然估计值,参数,answer2feature,,,,,,,,,,,
"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",MAP,answer2feature,,,,,,,,,,,
"MAP的基础使贝叶斯公式：P(θ/X)=P(θ,X)/P(X)。目的是通过观测值使得后验概率P(θ,X)最大即可",贝叶斯公式,answer2feature,,,,,,,,,,,
最大似然估计中的采样满足所有采样都是独立同分布的假设,估计,answer2feature,,,,,,,,,,,
概率估计：给定了θ，X=x的可能性,估计,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DNN,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DeepFM,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,DeepFM,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,特征,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,FM,answer2feature,,,,,,,,,,,
DNN是DeepFM中的一个部分，DeepFM多一次特征，多一个FM层的二次交叉特征,特征,answer2feature,,,,,,,,,,,
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,DeepFM,answer2feature,,,,,,,,,,,
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,Wide,answer2feature,,,,,,,,,,,
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,优化,answer2feature,,,,,,,,,,,
DeepFM对Wide&Deep中的Wide层进行了优化，增加了交叉特征,特征,answer2feature,,,,,,,,,,,
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,欠拟合,answer2feature,,,,,,,,,,,
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,epoch,answer2feature,,,,,,,,,,,
欠拟合：增加deep部分的层数，增加epoch的轮数，增加learningrate，减少正则化力度,learningrate,answer2feature,,,,,,,,,,,
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,过拟合,answer2feature,,,,,,,,,,,
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,dropout,answer2feature,,,,,,,,,,,
过拟合：在deep层直接增加dropout的率，减少epoch轮数，增加更多的数据，增加正则化力度，shuffle数据,epoch,answer2feature,,,,,,,,,,,
embedding向量可以通过FM初始化,embedding,answer2feature,,,,,,,,,,,
embedding向量可以通过FM初始化,向量,answer2feature,,,,,,,,,,,
embedding向量可以通过FM初始化,FM,answer2feature,,,,,,,,,,,
embedding向量可以通过FM初始化,初始化,answer2feature,,,,,,,,,,,
Deep层可以做优化,Deep,answer2feature,,,,,,,,,,,
Deep层可以做优化,优化,answer2feature,,,,,,,,,,,
NFM:把deep层的做NFM类型的处理，其实就是deep层在输入之前也做一个二阶特征的交叉处理和fm层一致,特征,answer2feature,,,,,,,,,,,
FM层可以变得交叉更多阶,FM,answer2feature,,,,,,,,,,,
XDeepFM,XDeepFM,answer2feature,,,,,,,,,,,
截断补齐,截断,answer2feature,,,,,,,,,,,
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",Xavier,answer2feature,,,,,,,,,,,
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",输出,answer2feature,,,,,,,,,,,
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",输出,answer2feature,,,,,,,,,,,
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",正态分布,answer2feature,,,,,,,,,,,
"常规的是Xavier，输出和输出可以保持正态分布且方差相近：np.random.rand(layer\[n1],layer\[n])*np.sqrt(1/layer\[n1])",random,answer2feature,,,,,,,,,,,
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",relu,answer2feature,,,,,,,,,,,
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",神经元,answer2feature,,,,,,,,,,,
"relu的情况下通常是HE，保证半数神经元失活的情况下对输出方差影响最小:：np.random.rand(layer\[n1],layer\[n])*np.sqrt(2/layer\[n1])",random,answer2feature,,,,,,,,,,,
文本项目上也可以用预训练好的特征,特征,answer2feature,,,,,,,,,,,
Attention机制，针对不同的广告，用户历史行为与该广告的权重是不同的。,Attention,answer2feature,,,,,,,,,,,
基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,Attention,answer2feature,,,,,,,,,,,
基于Attention机制，结合当前需对比的不同目标，将用户历史行为进行不同的权重分配。,目标,answer2feature,,,,,,,,,,,
activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,目标,answer2feature,,,,,,,,,,,
activationunit在这种思路上，认为面对不同的对象Va兴趣的权重Wi应该也是变换而不是固定的，所以用了g(ViVa)来动态刻画不同目标下的历史行为的不同重要性,重要性,answer2feature,,,,,,,,,,,
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,bn,answer2feature,,,,,,,,,,,
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,sigmoid,answer2feature,,,,,,,,,,,
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,加权,answer2feature,,,,,,,,,,,
先对input数据进行bn，在进行sigmoid归一化到01，再进行一个加权平衡alpha*(1x_p)`*`x+x_p`*`x,平衡,answer2feature,,,,,,,,,,,
"x_p=tf.sigmoid(tf.layers.batch_normalization(x,center=False,scale=False,training=True))",sigmoid,answer2feature,,,,,,,,,,,
在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,batch,answer2feature,,,,,,,,,,,
在用batch_normalization的时候，需要设置traning=True，否则在做test的时候，获取不到training过程中的各batch的期望,期望,answer2feature,,,,,,,,,,,
test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),期望,answer2feature,,,,,,,,,,,
test的时候，方差计算利用的是期望的无偏估计计算方法:E(u^2)`*`m/(m1),估计,answer2feature,,,,,,,,,,,
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,deepfm,answer2feature,,,,,,,,,,,
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,FNN,answer2feature,,,,,,,,,,,
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,特征,answer2feature,,,,,,,,,,,
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,dnn,answer2feature,,,,,,,,,,,
类似deepfm和FNN等模型的高阶的特征交互来自于dnn部分，但是这样的特征交互是不可控且隐式的，难以描述的,特征,answer2feature,,,,,,,,,,,
向量级别的特征交互而不是元素级交互,向量,answer2feature,,,,,,,,,,,
向量级别的特征交互而不是元素级交互,特征,answer2feature,,,,,,,,,,,
经验上，vectorwise的方式构建的特征交叉关系比bitwise的方式更容易学习,特征,answer2feature,,,,,,,,,,,
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",deepfm,answer2feature,,,,,,,,,,,
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",拟合,answer2feature,,,,,,,,,,,
"之前用的deepfm在历史数据的拟合上出现了瓶颈：A\[""篮球"",""足球"",""健身""]，B\[""篮球"",""电脑"",""蔡徐坤""]，会给A推荐""蔡徐坤""，但是实际上不合理",推荐,answer2feature,,,,,,,,,,,
"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",attention,answer2feature,,,,,,,,,,,
"思路一：改变Memorization为attention网络，强化feature直接的关系，对B进行""电脑""与""蔡徐坤""之间的绑定而不是""篮球""和""蔡徐坤""之间的绑定",feature,answer2feature,,,,,,,,,,,
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,特征,answer2feature,,,,,,,,,,,
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,DCN,answer2feature,,,,,,,,,,,
思路二：改变Memorization为更优化更合理的低价特征交互，比如DCN或者XDeepFM,XDeepFM,answer2feature,,,,,,,,,,,
显示是可以写出feature交互的公式，隐式相反,feature,answer2feature,,,,,,,,,,,
元素级是以feature值交互，向量级是feature向量级点乘处理,feature,answer2feature,,,,,,,,,,,
元素级是以feature值交互，向量级是feature向量级点乘处理,向量,answer2feature,,,,,,,,,,,
元素级是以feature值交互，向量级是feature向量级点乘处理,feature,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,DNN,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,FM,answer2feature,,,,,,,,,,,
高阶特征是类似DNN这种多层特征交互，低阶特征交互是FM这种特征单层处理方式,特征,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,DeepFm,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,高频,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,特征,answer2feature,,,,,,,,,,,
借鉴了DeepFm的整体结构，保持了两个部分的组合：低阶特征交互+高阶特征交互，**低价特征来记忆高频历史数据场景，高阶特征交互来进行稀疏场景的泛化**,稀疏,answer2feature,,,,,,,,,,,
高阶特征交互：DNN,特征,answer2feature,,,,,,,,,,,
高阶特征交互：DNN,DNN,answer2feature,,,,,,,,,,,
低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),特征,answer2feature,,,,,,,,,,,
低价特征交互：DCN结构(![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9ihwd6wucj305q00la9v.jpg)),DCN,answer2feature,,,,,,,,,,,
这样的网络结构保证来来自X0的1，2，3...N阶的特征组合,特征,answer2feature,,,,,,,,,,,
借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),DCN,answer2feature,,,,,,,,,,,
借鉴来DCN的交叉网络的特殊结构**自动构造有限高阶交叉特征**：![](https://tva1.sinaimg.cn/large/006tNbRwgy1g9iidr9x2bj306r01lglh.jpg),特征,answer2feature,,,,,,,,,,,
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",batch,answer2feature,,,,,,,,,,,
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",DNN,answer2feature,,,,,,,,,,,
"构造\[batch,field,embedding_size]的input，分别进入DNN、CIN、Linear层",Linear,answer2feature,,,,,,,,,,,
"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",batch,answer2feature,,,,,,,,,,,
"先记录\[batch,field,embedding_size]作为X0，并切分为embedding`*`\[batch,field,1]份",batch,answer2feature,,,,,,,,,,,
设置三层隐层，单层结点数为200，单层操作如下（以X1为例）：,隐层,answer2feature,,,,,,,,,,,
"获取上一次的layerout：X0，并进行切分：embedding`*`\[batch,field,1]",batch,answer2feature,,,,,,,,,,,
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature,,,,,,,,,,,
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature,,,,,,,,,,,
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",embedding,answer2feature,,,,,,,,,,,
"进行外积：embedding`*`\[batch,field,1]`*`embedding`*`\[batch,field,1]得到\[embedding,batch,field,field]",batch,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",压缩,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",embedding,answer2feature,,,,,,,,,,,
"对\[embedding,batch,field,field]进行压缩\[embedding,batch,field`*`field]，在对结果进行\[1,field`*`field,output_layer]卷积得到缩\[embedding,batch,output_layer]",batch,answer2feature,,,,,,,,,,,
加偏置项，并进行激活函数处理，完成一轮处理,激活函数,answer2feature,,,,,,,,,,,
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",embedding,answer2feature,,,,,,,,,,,
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",维度,answer2feature,,,,,,,,,,,
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",pooling,answer2feature,,,,,,,,,,,
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",batch,answer2feature,,,,,,,,,,,
"将若干轮的处理结果按照hidden_size维进行合并，并对embedding维度进行pooling得到\[batch,embedding]的output层即为结果",embedding,answer2feature,,,,,,,,,,,
实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,输出,answer2feature,,,,,,,,,,,
实际过程中，可以对每层对结果进行采样泛化；可以通过最后层输出的残差连接保证梯度消失等等,梯度,answer2feature,,,,,,,,,,,
DCN是bitwise的，而CIN是vectorwise的,DCN,answer2feature,,,,,,,,,,,
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,DCN,answer2feature,,,,,,,,,,,
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,特征,answer2feature,,,,,,,,,,,
DCN每层是1～l+1阶特征，而CIN每层只包含l+1阶的组合特征,特征,answer2feature,,,,,,,,,,,
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,DNN,answer2feature,,,,,,,,,,,
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,神经元,answer2feature,,,,,,,,,,,
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,向量,answer2feature,,,,,,,,,,,
假设CIN和DNN每层神经元/向量个数都为H，网络深度为L,深度,answer2feature,,,,,,,,,,,
DNN:O(m`*`D`*`H+L`*`H`*`H),DNN,answer2feature,,,,,,,,,,,
input数据中只拿了近20次的点击，部分用户是没有20次的历史行为的，所以我们记录了每一个用户实际点击的次数，在做embedding的时候，我们除以的是真实的historylength,embedding,answer2feature,,,,,,,,,,,
历史上最高频的商品id/文章id,高频,answer2feature,,,,,,,,,,,
train的时候是进行负采样的,负采样,answer2feature,,,,,,,,,,,
负采样我们会进行剔除，把该次click下的同时show的样本进行剔除后采样,负采样,answer2feature,,,,,,,,,,,
均衡采样，不会根据其他样本showtime进行加权,加权,answer2feature,,,,,,,,,,,
为了尽可能多的修正全量样本，尽快达到收敛,全量,answer2feature,,,,,,,,,,,
为了尽可能多的修正全量样本，尽快达到收敛,收敛,answer2feature,,,,,,,,,,,
为了避免其他推荐产生的交叉影响,推荐,answer2feature,,,,,,,,,,,
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,初始化,answer2feature,,,,,,,,,,,
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,初始化,answer2feature,,,,,,,,,,,
是用初始化我们在进行historyclickembedding过程中使用的初始化的向量，没有在最后层重新构造一个itemembedding的结果，实测效果翻倍的要好,向量,answer2feature,,,,,,,,,,,
希望平衡样本构造时间对当前的影响,平衡,answer2feature,,,,,,,,,,,
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,nlp,answer2feature,,,,,,,,,,,
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,预测,answer2feature,,,,,,,,,,,
item对比nlp问题的时候，上下文信息更适用于文本等固定结果的探索问题（完形填空问题）；而在预测nextone这种问题下，只通过上文进行预测，更加合适和合理。浏览信息大概率都是不对称的，而常规的i2i模型的假设都是对称的，上下文都可以用的,预测,answer2feature,,,,,,,,,,,
在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,推荐,answer2feature,,,,,,,,,,,
在实际的推荐数据获取中，历史点击流收到若干种在线的推荐算法影响，并不全是像自然语言问题一样真实具有序列性,推荐,answer2feature,,,,,,,,,,,
负采样,负采样,answer2feature,,,,,,,,,,,
工程妥协，避免几百万的item每次都要计算一边，而采取的ANN的邻近搜索加快速度,搜索,answer2feature,,,,,,,,,,,
embedding,embedding,answer2feature,,,,,,,,,,,
引入了doc2vec做init,doc2vec,answer2feature,,,,,,,,,,,
权重共享，没有在softmax处重新构造,softmax,answer2feature,,,,,,,,,,,
负采样,负采样,answer2feature,,,,,,,,,,,
加快收敛,加快,answer2feature,,,,,,,,,,,
加快收敛,收敛,answer2feature,,,,,,,,,,,
click数据的预处理,预处理,answer2feature,,,,,,,,,,,
设函数f(x)满足条件：,函数,answer2feature,,,,,,,,,,,
"设函数f(x),g(x)满足条件：",函数,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,离散,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,离散,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,期望,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,收敛,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,特征,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,反映,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,期望,answer2feature,,,,,,,,,,,
离散型随机变量的一切可能的取值xi与对应的概率Pi(=xi)之积的和称为该离散型随机变量的数学期望（设级数绝对收敛），记为E(x)。随机变量最基本的数学特征之一。它反映随机变量平均取值的大小。又称期望或均值。,均值,answer2feature,,,,,,,,,,,
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature,,,,,,,,,,,
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature,,,,,,,,,,,
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,连续性,answer2feature,,,,,,,,,,,
若随机变量X的分布函数F(x)可表示成一个非负可积函数f(x)的积分，则称X为连续性随机变量，f(x)称为X的概率密度函数（分布密度函数）。,函数,answer2feature,,,,,,,,,,,
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,期望,answer2feature,,,,,,,,,,,
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,均值,answer2feature,,,,,,,,,,,
方差是各个数据与平均数之差的平方的平均数。在概率论和数理统计中，方差（英文Variance）用来度量随机变量和其数学期望（即均值）之间的偏离程度。在许多实际问题中，研究随机变量和均值之间的偏离程度有着很重要的意义。,均值,answer2feature,,,,,,,,,,,
方差刻画了随机变量的取值对于其数学期望的离散程度。,期望,answer2feature,,,,,,,,,,,
方差刻画了随机变量的取值对于其数学期望的离散程度。,离散,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,均值,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,差别,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,逼近,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,统计,answer2feature,,,,,,,,,,,
很显然，均值描述的是样本集合的中间点，它告诉我们的信息是很有限的，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均。以这两个集合为例，[0，8，12，20]和[8，9，11，12]，两个集合的均值都是10，但显然两个集合差别是很大的，计算两者的标准差，前者是8.3，后者是1.8，显然后者较为集中，故其标准差小一些，标准差描述的就是这种“散布度”。之所以除以n1而不是除以n，是因为这样能使我们以较小的样本集更好的逼近总体的标准差，即统计上所谓的“无偏估计”。而方差则仅仅是标准差的平方。,估计,answer2feature,,,,,,,,,,,
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,平方和,answer2feature,,,,,,,,,,,
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,平方根,answer2feature,,,,,,,,,,,
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,反映,answer2feature,,,,,,,,,,,
标准差（StandardDeviation），也称均方差（meansquareerror），是各数据偏离平均数的距离的平均数，它是离均差平方和平均后的方根，用σ表示。标准差是方差的算术平方根。标准差能反映一个数据集的离散程度。平均数相同的，标准差未必相同。,离散,answer2feature,,,,,,,,,,,
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,回归,answer2feature,,,,,,,,,,,
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,统计分析,answer2feature,,,,,,,,,,,
协方差分析是建立在方差分析和回归分析基础之上的一种统计分析方法。方差分析是从质量因子的角度探讨因素不同水平对实验指标影响的差异。一般说来，质量因子是可以人为控制的。回归分析是从数量因子的角度出发，通过建立回归方程来研究实验指标与一个（或几个）因子之间的数量关系。但大多数情况下，数量因子是不可以人为加以控制的。,回归,answer2feature,,,,,,,,,,,
在概率论和统计学中，协方差用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。,误差,answer2feature,,,,,,,,,,,
"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",相关性,answer2feature,,,,,,,,,,,
"\[1,1]之间，值越接近1，说明两个变量正相关性（线性）越强。越接近1，说明负相关性越强，当为0时，表示两个变量没有相关性",相关性,answer2feature,,,,,,,,,,,
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",离散,answer2feature,,,,,,,,,,,
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",均匀分布,answer2feature,,,,,,,,,,,
"离散随机变量的均匀分布：假设X有k个取值：x1,x2,...,xk则均匀分布的概率密度函数为:",均匀分布,answer2feature,,,,,,,,,,,
"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",参数,answer2feature,,,,,,,,,,,
"伯努利分布：参数为p∈[0,1]，设随机变量X∈{0,1}，则概率分布函数为：",函数,answer2feature,,,,,,,,,,,
期望为p，方差为p(1p),期望,answer2feature,,,,,,,,,,,
期望为np，方差为np(1p),期望,answer2feature,,,,,,,,,,,
我们在做模型训练的之后，随机变量取值范围是实数，大多数情况下都假设变量服从高斯分布，原因：,训练,answer2feature,,,,,,,,,,,
随机变量大多数情况下有若干个因素组合而成，中心极限定理表明，多个独立随机变量的和近似正态分布,正态分布,answer2feature,,,,,,,,,,,
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,正态分布,answer2feature,,,,,,,,,,,
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,熵,answer2feature,,,,,,,,,,,
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,不确定性,answer2feature,,,,,,,,,,,
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,熵,answer2feature,,,,,,,,,,,
在具有相同方差的所有可能的概率分布中，正态分布的熵最大（即不确定性最大）；熵大带来的信息量多,信息量,answer2feature,,,,,,,,,,,
典型的一维正态分布的概率密度函数为:,正态分布,answer2feature,,,,,,,,,,,
期望为u，方差为2γ^2,期望,answer2feature,,,,,,,,,,,
拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用该性质,拉普拉斯,answer2feature,,,,,,,,,,,
假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为k的概率。,泊松,answer2feature,,,,,,,,,,,
期望：λ，方差为：λ,期望,answer2feature,,,,,,,,,,,
"p(|xu|>k?)<=1/(k^2),满足k>0,u为期望,?为标准差",期望,answer2feature,,,,,,,,,,,
绝大多数数据都应该在均值附近,均值,answer2feature,,,,,,,,,,,
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),有放回,answer2feature,,,,,,,,,,,
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),抽取,answer2feature,,,,,,,,,,,
有放回的抽取，抽取m个排成一列，求不同排列总数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920jcfb42j300n00d0s5.jpg),抽取,answer2feature,,,,,,,,,,,
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),无放回,answer2feature,,,,,,,,,,,
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),抽取,answer2feature,,,,,,,,,,,
无放回的抽取，抽取m个排成一列，求不同排列总数:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g920kqhlhbj301w015we9.jpg),抽取,answer2feature,,,,,,,,,,,
均匀分布：,均匀分布,answer2feature,,,,,,,,,,,
问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,放回,answer2feature,,,,,,,,,,,
问题：抽蓝球红球，蓝结束红放回继续，平均结束游戏抽取次数,抽取,answer2feature,,,,,,,,,,,
假设设抽到蓝球的概率为p，设抽到红球的概率为q，那么抽取到的次数为：1・p+2p・q+...+np・q^(n1),抽取,answer2feature,,,,,,,,,,,
定义：f(x)在x0处的邻域内有n+1阶的导数，在x0的邻域内的任x，x和x0之间至少存在一个ζ，使得,导数,answer2feature,,,,,,,,,,,
1范数：各列绝对值和的最大值,范数,answer2feature,,,,,,,,,,,
1范数：各列绝对值和的最大值,最大值,answer2feature,,,,,,,,,,,
"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",范数,answer2feature,,,,,,,,,,,
"2范数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zwforqmj302i00n0si.jpg),矩阵$A^TA$的最大特征值开平方根",特征值,answer2feature,,,,,,,,,,,
特征值分解可以得到特征值与特征向量,特征值,answer2feature,,,,,,,,,,,
特征值分解可以得到特征值与特征向量,分解,answer2feature,,,,,,,,,,,
特征值分解可以得到特征值与特征向量,特征值,answer2feature,,,,,,,,,,,
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征值,answer2feature,,,,,,,,,,,
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征,answer2feature,,,,,,,,,,,
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么,特征,answer2feature,,,,,,,,,,,
"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",特征值,answer2feature,,,,,,,,,,,
"矩阵A的特征值与其特征向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zxnzwesj300h00h0rz.jpg),特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g91zy0ybcpj300a00c0pd.jpg)满足：",特征值,answer2feature,,,,,,,,,,,
其中，Q为特征向量组成的矩阵，∑为特征值由大到小组成的矩阵,特征值,answer2feature,,,,,,,,,,,
矩阵的特征值大于等于0，半正定,特征值,answer2feature,,,,,,,,,,,
矩阵的特征值大于0，正定,特征值,answer2feature,,,,,,,,,,,
Hessian矩阵正定性在梯度下降的应用,梯度,answer2feature,,,,,,,,,,,
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",函数,answer2feature,,,,,,,,,,,
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",偏导恒,answer2feature,,,,,,,,,,,
"若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解",函数,answer2feature,,,,,,,,,,,
在svm中核函数构造的基本假设,svm,answer2feature,,,,,,,,,,,
在svm中核函数构造的基本假设,核,answer2feature,,,,,,,,,,,
在svm中核函数构造的基本假设,函数,answer2feature,,,,,,,,,,,
数据需要服从正态分布,正态分布,answer2feature,,,,,,,,,,,
基于正态分布的离群点检测方法,正态分布,answer2feature,,,,,,,,,,,
多元高斯分布检测：,多元,answer2feature,,,,,,,,,,,
假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),均值,answer2feature,,,,,,,,,,,
假设n维的数据集合![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9e4sjkoj303000it8l.jpg)，可以计算n维的均值向量![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9ewbb1cj304b00igli.jpg),向量,answer2feature,,,,,,,,,,,
假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,均值,answer2feature,,,,,,,,,,,
假设![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9kw2qf3j300900awec.jpg)是均值向量，其中S是协方差矩阵。,向量,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9rdqiqjj300g00h0sl.jpg)统计检验,统计,answer2feature,,,,,,,,,,,
"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",均值,answer2feature,,,,,,,,,,,
"![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sgjwk1j300d00awec.jpg)是a在第i维上的取值,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8m9sljfhpj300g00e0sl.jpg)是所有对象在第i维的均值，n是维度",维度,answer2feature,,,,,,,,,,,
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,均值,answer2feature,,,,,,,,,,,
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,特征值,answer2feature,,,,,,,,,,,
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,特征值,answer2feature,,,,,,,,,,,
去除均值后的协方差矩阵对应的特征值和特征向量，按照特征值排序，topN个特征向量组成新的低维空间,排序,answer2feature,,,,,,,,,,,
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,特征,answer2feature,,,,,,,,,,,
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,原始数据,answer2feature,,,,,,,,,,,
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,维度,answer2feature,,,,,,,,,,,
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,空间,answer2feature,,,,,,,,,,,
核心：在于组合原始的特征，使得新的原始数据在新的低维度空间中的方差更大，特征更有区分力,特征,answer2feature,,,,,,,,,,,
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,空间,answer2feature,,,,,,,,,,,
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,优化,answer2feature,,,,,,,,,,,
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,压缩,answer2feature,,,,,,,,,,,
问题是没有做到剔除，只是对空间上的表现进行了优化，尽可能的压缩异常点在新空间中作用,空间,answer2feature,,,,,,,,,,,
假设dataMat是一个p维的数据集合，有N个样本，它的协方差矩阵是X。那么协方差矩阵就通过奇异值分解写成：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8man1o3trj302h00hmx1.jpg),分解,answer2feature,,,,,,,,,,,
"其中P是一个(p,p)维的正交矩阵，它的每一列都是X的特征向量。D是一个(p,p)维的对角矩阵，包含了特征值![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8manhjc7qj301n00ga9x.jpg)。",特征值,answer2feature,,,,,,,,,,,
最后还需要拉回原空间：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mb1hmrmnj306w00i747.jpg),空间,answer2feature,,,,,,,,,,,
特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,特征值,answer2feature,,,,,,,,,,,
特征向量所对应的特征值反映了这批数据在这个方向上的拉伸程度,反映,answer2feature,,,,,,,,,,,
两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature,,,,,,,,,,,
两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,空间,answer2feature,,,,,,,,,,,
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature,,,,,,,,,,,
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,向量,answer2feature,,,,,,,,,,,
矩阵点乘向量的意义是将右边的向量变换到左边矩阵中每一行行向量为基所表示的空间中去。,空间,answer2feature,,,,,,,,,,,
定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,DBSCAN,answer2feature,,,,,,,,,,,
定义密度为到k个最近邻的平均距离的倒数。如果该距离小，则密度高，反之亦然。另一种密度定义是使用DBSCAN聚类算法使用的密度定义，即一个对象周围的密度等于该对象指定距离d内对象的个数。,聚类,answer2feature,,,,,,,,,,,
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),深度,answer2feature,,,,,,,,,,,
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),ceiling,answer2feature,,,,,,,,,,,
经验1：每棵树的最大深度limitlength=ceiling(log2(样本大小)),log2,answer2feature,,,,,,,,,,,
经验2：树的个数在256棵以下,树,answer2feature,,,,,,,,,,,
需要人为选择阈值,阈值,answer2feature,,,,,,,,,,,
一个对象是基于聚类的离群点，如果该对象不强属于任何簇，那么该对象属于离群点。,聚类,answer2feature,,,,,,,,,,,
缺点也就是聚类的缺点，包括初始点对结果的影响，数据是否保持凸型对结果对影响，簇的个数的选择,聚类,answer2feature,,,,,,,,,,,
平均值修正：可用前后两个观测值的平均值修正该异常值；,平均值,answer2feature,,,,,,,,,,,
平均值修正：可用前后两个观测值的平均值修正该异常值；,平均值,answer2feature,,,,,,,,,,,
生成列新特征：category异常,特征,answer2feature,,,,,,,,,,,
不处理：直接在具有异常值的数据集上进行数据挖掘；,数据挖掘,answer2feature,,,,,,,,,,,
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",下采样,answer2feature,,,,,,,,,,,
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",高维,answer2feature,,,,,,,,,,,
"下采样：克服高维特征以及大量数据导致的问题,有助于降低成本,缩短时间甚至提升效果",特征,answer2feature,,,,,,,,,,,
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,上采样,answer2feature,,,,,,,,,,,
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,平衡,answer2feature,,,,,,,,,,,
上采样：均衡正负样本的数据，避免数据不平衡导致分类器对正负样本的有偏训练,训练,answer2feature,,,,,,,,,,,
比如99%为正样本，1%为负样本，如果分类器把所以样本预测为正样本则准确率高达99%，显然不符合实际情况,预测,answer2feature,,,,,,,,,,,
采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,原始数据,answer2feature,,,,,,,,,,,
采样前后会对原始数据的分布进行改变，可能导致泛化能力大大下降,泛化能力,answer2feature,,,,,,,,,,,
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,过拟合,answer2feature,,,,,,,,,,,
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,原始数据,answer2feature,,,,,,,,,,,
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,训练,answer2feature,,,,,,,,,,,
采样有一定概率会造成过拟合，当原始数据过少而采样量又很大当时候，造成大量数据被重复，造成模型训练的结果有一定的过拟合,过拟合,answer2feature,,,,,,,,,,,
无放回的简单抽样：每条样本被采到的概率相等且都为1/N,无放回,answer2feature,,,,,,,,,,,
无放回的简单抽样：每条样本被采到的概率相等且都为1/N,抽样,answer2feature,,,,,,,,,,,
有放回的简单抽样：每条样本可能多次被选中,有放回,answer2feature,,,,,,,,,,,
有放回的简单抽样：每条样本可能多次被选中,抽样,answer2feature,,,,,,,,,,,
上采样：即合理地增加少数类的样本,上采样,answer2feature,,,,,,,,,,,
下采样：欠抽样技术是将数据从原始数据集中移除,下采样,answer2feature,,,,,,,,,,,
下采样：欠抽样技术是将数据从原始数据集中移除,抽样,answer2feature,,,,,,,,,,,
下采样：欠抽样技术是将数据从原始数据集中移除,原始数据,answer2feature,,,,,,,,,,,
平衡采样：考虑正负样本比,平衡,answer2feature,,,,,,,,,,,
分层采样：通过某一些feature对数据进行切分，按照切分后的占比分别进行采样,feature,answer2feature,,,,,,,,,,,
"整体采样：先将数据集T中的数据分组成G个互斥的簇,然后再从G个簇中简单随机采样s个簇作为样本集",互斥,answer2feature,,,,,,,,,,,
"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",上采样,answer2feature,,,,,,,,,,,
"相对于采样随机的方法进行上采样,还有两种比较流行的上采样的改进方式：",上采样,answer2feature,,,,,,,,,,,
ADASYN,ADASYN,answer2feature,,,,,,,,,,,
平衡欠采样,平衡,answer2feature,,,,,,,,,,,
EasyEnsemble，利用模型融合的方法（Ensemble）,EasyEnsemble,answer2feature,,,,,,,,,,,
EasyEnsemble，利用模型融合的方法（Ensemble）,融合,answer2feature,,,,,,,,,,,
EasyEnsemble，利用模型融合的方法（Ensemble）,Ensemble,answer2feature,,,,,,,,,,,
少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,训练,answer2feature,,,,,,,,,,,
少样本不变，多样本拆分成N份，分别组合进行模型训练后进行模型融合,融合,answer2feature,,,,,,,,,,,
BalanceCascade，利用模型融合的方法（Boost）,BalanceCascade,answer2feature,,,,,,,,,,,
BalanceCascade，利用模型融合的方法（Boost）,融合,answer2feature,,,,,,,,,,,
BalanceCascade，利用模型融合的方法（Boost）,Boost,answer2feature,,,,,,,,,,,
每次剔除预测正确的多数样本，加入新的未预测的多数样本,预测,answer2feature,,,,,,,,,,,
每次剔除预测正确的多数样本，加入新的未预测的多数样本,预测,answer2feature,,,,,,,,,,,
NearMiss,NearMiss,answer2feature,,,,,,,,,,,
选择离各种情况下的少数样本位置最远的多数样本进行训练,训练,answer2feature,,,,,,,,,,,
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,训练,answer2feature,,,,,,,,,,,
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,回归,answer2feature,,,,,,,,,,,
可以通过修改模型训练中的loss权重，比如罗辑回归中进行case_weight的调整，adaboost中对样本错分权重的改变等等。,adaboost,answer2feature,,,,,,,,,,,
通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,特征,answer2feature,,,,,,,,,,,
通常我们引入的特征不仅仅是连续变量，在分类变量上合成采样表现并不优秀,分类,answer2feature,,,,,,,,,,,
合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,平衡,answer2feature,,,,,,,,,,,
合成采样往往无法与后序模型进行结合使用，比如随机采样可以引入模型交叉，比如平衡欠采样可以引入模型融合,融合,answer2feature,,,,,,,,,,,
避免异常点：比如对连续变量进行份桶离散化,离散,answer2feature,,,,,,,,,,,
可解释性或者需要连续输出：比如评分卡模型中的iv+woe,输出,answer2feature,,,,,,,,,,,
使得原始数据的信息量更大：比如log/sqrt变换,原始数据,answer2feature,,,,,,,,,,,
使得原始数据的信息量更大：比如log/sqrt变换,信息量,answer2feature,,,,,,,,,,,
缩放仅仅跟最大、最小值的差别有关，只是一个去量纲的过程,差别,answer2feature,,,,,,,,,,,
标准化(zscore),标准化,answer2feature,,,,,,,,,,,
缩放和所有点都相关，数据相对分布不会改变，集中的数据标准化后依旧集中,标准化,answer2feature,,,,,,,,,,,
更快的收敛,收敛,answer2feature,,,,,,,,,,,
异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,特征,answer2feature,,,,,,,,,,,
异常点角度：特征数据上下限明显异常的时候使用标准化方法，简单归一化会造成数据差异模糊，整体方差下降,标准化,answer2feature,,,,,,,,,,,
分布角度：使用标准化之前，要求数据需要近似满足高斯分布，不然会改变数据的分布，尤其是对数据分布有强假设的情况下,标准化,answer2feature,,,,,,,,,,,
上线变动角度：归一化在上线的时候需要考虑上下约束届是否需要变动，标准化则不需要考虑变动,标准化,answer2feature,,,,,,,,,,,
值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,标准化,answer2feature,,,,,,,,,,,
值域范围角度：归一化对数据范围约定较为固定，而标准化的输出上下届则不定,输出,answer2feature,,,,,,,,,,,
模型角度：一般涉及距离计算，协方差计算，数据满足高斯分布的情况下用标准化，其他归一化或其他变换,标准化,answer2feature,,,,,,,,,,,
knn：计算距离，不去量冈则结果受值域范围影响大,knn,answer2feature,,,,,,,,,,,
neuralnetwork：梯度异常问题+激活函数问题,梯度,answer2feature,,,,,,,,,,,
neuralnetwork：梯度异常问题+激活函数问题,激活函数,answer2feature,,,,,,,,,,,
截断,截断,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",截断,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature,,,,,,,,,,,
"连续型的数值进行截断或者对长尾数据进行对数后截断(保留重要信息的前提下对特征进行截断,截断后的特征也可以看作是类别特征)",特征,answer2feature,,,,,,,,,,,
参考异常点里面的outlier识别，以最大值填充或者以None,最大值,answer2feature,,,,,,,,,,,
数据分布过于不平衡,平衡,answer2feature,,,,,,,,,,,
分桶,分桶,answer2feature,,,,,,,,,,,
离散化,离散,answer2feature,,,,,,,,,,,
zscore标准化,标准化,answer2feature,,,,,,,,,,,
范数归一化:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8mf5xdj2sj3031011jr6.jpg),范数,answer2feature,,,,,,,,,,,
L1范数,L1,answer2feature,,,,,,,,,,,
L1范数,范数,answer2feature,,,,,,,,,,,
L2范数,L2,answer2feature,,,,,,,,,,,
L2范数,范数,answer2feature,,,,,,,,,,,
平方根缩放,平方根,answer2feature,,,,,,,,,,,
它将大端长尾压缩为短尾，并将小端进行延伸,压缩,answer2feature,,,,,,,,,,,
可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),较差,answer2feature,,,,,,,,,,,
可以把类似较差的特征线性化，比如x1x2/y，log变换后变成了log(x1)+log(x2)log(y),特征,answer2feature,,,,,,,,,,,
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,因变量,answer2feature,,,,,,,,,,,
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,自变量,answer2feature,,,,,,,,,,,
通过因变量的变换，使得变换后的y(λ)与自变量具有线性依托关系。因此，BoxCox变换是通过参数的适当选择，达到对原来数据的“综合治理”，使其满足一个线性模型条件,参数,answer2feature,,,,,,,,,,,
特征交叉,特征,answer2feature,,,,,,,,,,,
人为分段交叉,分段,answer2feature,,,,,,,,,,,
提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,拟合,answer2feature,,,,,,,,,,,
提升模型的拟合能力，使基向量更有表示能力。比如，本来是在二维空间解释一个点的意义，现在升维到三维后解释,向量,answer2feature,,,,,,,,,,,
离散变量的交并补,离散,answer2feature,,,,,,,,,,,
连续变量的点积，attention类似,点积,answer2feature,,,,,,,,,,,
连续变量的点积，attention类似,attention,answer2feature,,,,,,,,,,,
交叉中需要并行特征筛选的步骤,特征,answer2feature,,,,,,,,,,,
交叉中需要并行特征筛选的步骤,筛选,answer2feature,,,,,,,,,,,
FM/FFM中的矩阵点积,FM,answer2feature,,,,,,,,,,,
FM/FFM中的矩阵点积,FFM,answer2feature,,,,,,,,,,,
FM/FFM中的矩阵点积,点积,answer2feature,,,,,,,,,,,
NeuralNetwork里面的dense,dense,answer2feature,,,,,,,,,,,
通过树或者类似的特征组合模型去做最低熵的特征选择,树,answer2feature,,,,,,,,,,,
通过树或者类似的特征组合模型去做最低熵的特征选择,特征,answer2feature,,,,,,,,,,,
通过树或者类似的特征组合模型去做最低熵的特征选择,熵,answer2feature,,,,,,,,,,,
通过树或者类似的特征组合模型去做最低熵的特征选择,特征选择,answer2feature,,,,,,,,,,,
非线性编码,非线性,answer2feature,,,,,,,,,,,
核向量进行升维,核,answer2feature,,,,,,,,,,,
核向量进行升维,向量,answer2feature,,,,,,,,,,,
树模型的叶子结点的stack,树,answer2feature,,,,,,,,,,,
谱聚类/pca/svd等信息抽取编码,聚类,answer2feature,,,,,,,,,,,
谱聚类/pca/svd等信息抽取编码,pca,answer2feature,,,,,,,,,,,
谱聚类/pca/svd等信息抽取编码,svd,answer2feature,,,,,,,,,,,
谱聚类/pca/svd等信息抽取编码,抽取,answer2feature,,,,,,,,,,,
lda/EM等分布拟合表示,lda,answer2feature,,,,,,,,,,,
lda/EM等分布拟合表示,EM,answer2feature,,,,,,,,,,,
lda/EM等分布拟合表示,拟合,answer2feature,,,,,,,,,,,
计数编码,计数,answer2feature,,,,,,,,,,,
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",特征,answer2feature,,,,,,,,,,,
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",计数,answer2feature,,,,,,,,,,,
"将类别特征用其对应的计数来代替,这对线性和非线性模型都有效",非线性,answer2feature,,,,,,,,,,,
"对异常值比较敏感,特征取值有可能冲突",特征,answer2feature,,,,,,,,,,,
计数排名编码,计数,answer2feature,,,,,,,,,,,
Embedding,Embedding,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",离散,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",欠拟合,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",过拟合,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",稀疏,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",目标,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",特征,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",分类,answer2feature,,,,,,,,,,,
"对于基数(类别变量所有可能不同取值的个数)很大的离散特征，简单模型任意欠拟合,而复杂模型任意过拟合;对于独热编码,得到的特征矩阵太稀疏.对于高基数类别变量,有效方式是基于目标变量对类别特征进行编码,即有监督的编码方式,适用于分类和回归问题",回归,answer2feature,,,,,,,,,,,
类别特征之间交叉组合,特征,answer2feature,,,,,,,,,,,
类别特征和数值特征之间交叉组合,特征,answer2feature,,,,,,,,,,,
类别特征和数值特征之间交叉组合,特征,answer2feature,,,,,,,,,,,
均值、中位数、标准差、最大值和最小值,均值,answer2feature,,,,,,,,,,,
均值、中位数、标准差、最大值和最小值,中位数,answer2feature,,,,,,,,,,,
均值、中位数、标准差、最大值和最小值,最大值,answer2feature,,,,,,,,,,,
分位数、方差、vif值、分段冲量,分段,answer2feature,,,,,,,,,,,
预处理手段有哪些？,预处理,answer2feature,,,,,,,,,,,
分词,分词,answer2feature,,,,,,,,,,,
LDA,LDA,answer2feature,,,,,,,,,,,
词干提取,词干,answer2feature,,,,,,,,,,,
文档特征,特征,answer2feature,,,,,,,,,,,
文本向量化,向量,answer2feature,,,,,,,,,,,
word2vec,word2vec,answer2feature,,,,,,,,,,,
glove,glove,answer2feature,,,,,,,,,,,
bert,bert,answer2feature,,,,,,,,,,,
文本相似性,相似性,answer2feature,,,,,,,,,,,
分词过程中会考虑哪些方面？,分词,answer2feature,,,,,,,,,,,
词性标注,词性,answer2feature,,,,,,,,,,,
词形还原和词干提取,词形,answer2feature,,,,,,,,,,,
词形还原和词干提取,词干,answer2feature,,,,,,,,,,,
词形还原为了通用性特征的提取,词形,answer2feature,,,,,,,,,,,
词形还原为了通用性特征的提取,特征,answer2feature,,,,,,,,,,,
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,词干,answer2feature,,,,,,,,,,,
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,训练,answer2feature,,,,,,,,,,,
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,注意力,answer2feature,,,,,,,,,,,
词干提取为了去除干扰词把训练注意力集中在关键词上，同时提高速度；缺点是不一定词干代表完整句义,词干,answer2feature,,,,,,,,,,,
文本中的统计信息一般有哪些？,统计,answer2feature,,,,,,,,,,,
直接统计值：,统计,answer2feature,,,,,,,,,,,
不同词性个数,词性,answer2feature,,,,,,,,,,,
直接统计值的统计信息：,统计,answer2feature,,,,,,,,,,,
直接统计值的统计信息：,统计,answer2feature,,,,,,,,,,,
最小最大均值方差标准差,均值,answer2feature,,,,,,,,,,,
直接对文本特征进行整理手段有哪些？,特征,answer2feature,,,,,,,,,,,
将文本转换为连续序列，扩充样本特征,特征,answer2feature,,,,,,,,,,,
"权重评分，去除掉一些低重要性的词，比如每篇文章都出现的""的""，""了""",重要性,answer2feature,,,,,,,,,,,
LDA,LDA,answer2feature,,,,,,,,,,,
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,主题,answer2feature,,,,,,,,,,,
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,抽取,answer2feature,,,,,,,,,,,
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,狄利克雷,answer2feature,,,,,,,,,,,
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,拟合,answer2feature,,,,,,,,,,,
主题抽取，用狄利克雷分布去拟合出文章和主题之间的关系,主题,answer2feature,,,,,,,,,,,
用来衡量概率分布之间的相似性,相似性,answer2feature,,,,,,,,,,,
向量化,向量,answer2feature,,,,,,,,,,,
word2vec,word2vec,answer2feature,,,,,,,,,,,
glove,glove,answer2feature,,,,,,,,,,,
bert,bert,answer2feature,,,,,,,,,,,
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,attention,answer2feature,,,,,,,,,,,
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,bert,answer2feature,,,,,,,,,,,
建议不要上来就transfer+attention+bert+xlnet，挖了坑要跳的,xlnet,answer2feature,,,,,,,,,,,
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,特征,answer2feature,,,,,,,,,,,
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,特征,answer2feature,,,,,,,,,,,
耗时：特征个数越多，分析特征、训练模型所需的时间就越长。,训练,answer2feature,,,,,,,,,,,
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,过拟合,answer2feature,,,,,,,,,,,
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,特征,answer2feature,,,,,,,,,,,
过拟合：特征个数越多，容易引起“维度灾难”，模型也会越复杂，其推广能力会下降。,维度,answer2feature,,,,,,,,,,,
共线性：单因子对目标的作用被稀释，解释力下降,目标,answer2feature,,,,,,,,,,,
方差，是的feature内的方向更大，对目标区分度提高更高贡献,feature,answer2feature,,,,,,,,,,,
方差，是的feature内的方向更大，对目标区分度提高更高贡献,目标,answer2feature,,,,,,,,,,,
相关性，与区分目标有高相关的特征才有意义,相关性,answer2feature,,,,,,,,,,,
相关性，与区分目标有高相关的特征才有意义,目标,answer2feature,,,,,,,,,,,
相关性，与区分目标有高相关的特征才有意义,特征,answer2feature,,,,,,,,,,,
移除低方差特征,特征,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,阈值,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征值,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,特征,answer2feature,,,,,,,,,,,
移除低方差特征是指移除那些方差低于某个阈值，即特征值变动幅度小于某个范围的特征，这一部分特征的区分度较差，我们进行移除,较差,answer2feature,,,,,,,,,,,
相关性,相关性,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征选择,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,目标,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,重要性,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,重要性,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature,,,,,,,,,,,
单变量特征选择：单变量特征是基于单一变量和目标y之间的关系，通过计算某个能够度量特征重要性的指标，然后选出重要性Top的K个特征。但是这种方式有一个缺点就是忽略了特征组合的情况,特征,answer2feature,,,,,,,,,,,
卡方检验,卡方,answer2feature,,,,,,,,,,,
熵检验,熵,answer2feature,,,,,,,,,,,
互信息熵,熵,answer2feature,,,,,,,,,,,
"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",相关性,answer2feature,,,,,,,,,,,
"度量两个变量之间的相关性,互信息越大表明两个变量相关性越高;互信息为0,两个变量越独立",相关性,answer2feature,,,,,,,,,,,
KL散度,KL,answer2feature,,,,,,,,,,,
KL散度,散度,answer2feature,,,,,,,,,,,
相对熵,熵,answer2feature,,,,,,,,,,,
均值,均值,answer2feature,,,,,,,,,,,
中位数,中位数,answer2feature,,,,,,,,,,,
有效性存疑，取决于特征列数,有效性,answer2feature,,,,,,,,,,,
有效性存疑，取决于特征列数,存疑,answer2feature,,,,,,,,,,,
有效性存疑，取决于特征列数,特征,answer2feature,,,,,,,,,,,
生成的插值来源于其他列的特征，是不是意味着插值的结果已经是和其他列的组合高相关,特征,answer2feature,,,,,,,,,,,
离散特征新增缺失的category,离散,answer2feature,,,,,,,,,,,
离散特征新增缺失的category,特征,answer2feature,,,,,,,,,,,
把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,embedding,answer2feature,,,,,,,,,,,
把全部结果都embedding化，对空值或者缺失值按照一定规则生成若干个hold位，以hold位的向量结果作为缺失值的结果,向量,answer2feature,,,,,,,,,,,
可以参考YouTube中的新商品向量生成逻辑,YouTube,answer2feature,,,,,,,,,,,
可以参考YouTube中的新商品向量生成逻辑,向量,answer2feature,,,,,,,,,,,
可以参考YouTube中的新商品向量生成逻辑,逻辑,answer2feature,,,,,,,,,,,
bert中的\[UNK]向量，\[unused]向量,bert,answer2feature,,,,,,,,,,,
bert中的\[UNK]向量，\[unused]向量,向量,answer2feature,,,,,,,,,,,
bert中的\[UNK]向量，\[unused]向量,向量,answer2feature,,,,,,,,,,,
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,原始数据,answer2feature,,,,,,,,,,,
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,高维,answer2feature,,,,,,,,,,,
如果我们能在原始数据上发现明显规律，比如整体数据满足高维多元高斯分布，则可以通过未知列补全缺失列的值,多元,answer2feature,,,,,,,,,,,
实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,机器学习,answer2feature,,,,,,,,,,,
实际机器学习工程中，直接删除、众数填充和直接离散化方法用的最多,离散,answer2feature,,,,,,,,,,,
快速,快速,answer2feature,,,,,,,,,,,
对原始数据的前提假设最少，也不会影响到非缺失列,原始数据,answer2feature,,,,,,,,,,,
在深度学习中，hold位填充方法用的最多,深度,answer2feature,,,,,,,,,,,
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,拟合,answer2feature,,,,,,,,,,,
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,向量,answer2feature,,,,,,,,,,,
在大量数据的拟合条件下，能保证这些未知数据处的向量也能得到收敛,收敛,answer2feature,,,,,,,,,,,
而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,向量,answer2feature,,,,,,,,,,,
而且通过随机构造的特性，保证了缺失处的\[UNK]向量，\[unused]向量的通配性,向量,answer2feature,,,,,,,,,,,
|模型|ID3|C4.5|CART|,ID3,answer2feature,,,,,,,,,,,
|模型|ID3|C4.5|CART|,CART,answer2feature,,,,,,,,,,,
|特征选择|信息增益|信息增益率|Gini系数/均方差|,特征选择,answer2feature,,,,,,,,,,,
|特征选择|信息增益|信息增益率|Gini系数/均方差|,增益,answer2feature,,,,,,,,,,,
|特征选择|信息增益|信息增益率|Gini系数/均方差|,增益,answer2feature,,,,,,,,,,,
1.构建根节点，将所有训练数据都放在根节点,训练,answer2feature,,,,,,,,,,,
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,特征,answer2feature,,,,,,,,,,,
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,特征,answer2feature,,,,,,,,,,,
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,训练,answer2feature,,,,,,,,,,,
2.选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类,分类,answer2feature,,,,,,,,,,,
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,递归,answer2feature,,,,,,,,,,,
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,训练,answer2feature,,,,,,,,,,,
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,分类,answer2feature,,,,,,,,,,,
3.如果子集非空，或子集容量未小于最少数量，递归1，2步骤，直到所有训练数据子集都被正确分类或没有合适的特征为止,特征,answer2feature,,,,,,,,,,,
其中，D为数据全集，C为不同因变量类别k上的子集(传统意义上的y的种类),因变量,answer2feature,,,,,,,,,,,
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,特征,answer2feature,,,,,,,,,,,
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,分类,answer2feature,,,,,,,,,,,
条件信息熵：在特征A给定的条件下对数据集D分类的不确定性：,不确定性,answer2feature,,,,,,,,,,,
信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,增益,answer2feature,,,,,,,,,,,
信息增益：知道特征A的信息而使类D的信息的不确定减少的程度（对称）：,特征,answer2feature,,,,,,,,,,,
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature,,,,,,,,,,,
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,信息量,answer2feature,,,,,,,,,,,
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature,,,,,,,,,,,
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,特征,answer2feature,,,,,,,,,,,
简而言之，就是在特征A下找到最合适的切分，使得在该切分下信息量的变换最大，更加稳定；但是这个有一个问题，对于类别天生较多的特征，模型更容易选中，因为特征类别较多，切分后的信息增益天生更大，更容易满足我们的原始假设,增益,answer2feature,,,,,,,,,,,
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",增益,answer2feature,,,,,,,,,,,
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",熵,answer2feature,,,,,,,,,,,
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",平衡,answer2feature,,,,,,,,,,,
"在信息增益计算的基础不变的情况下得到的：I(D,A)=H(D)H(D/A)，同时还考虑了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g925yr9z1vj306601f745.jpg),用划分的子集数上的熵来平衡了分类数过多的问题。",分类,answer2feature,,,,,,,,,,,
信息增益率：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9260xx5wej304b017q2r.jpg),增益,answer2feature,,,,,,,,,,,
对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),特征,answer2feature,,,,,,,,,,,
对于样本D，如果根据特征A的某个值，把D分成D1和D2，则在特征A的条件下，D的基尼系数为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9274yiwhoj30iq02wmx8.jpg),特征,answer2feature,,,,,,,,,,,
存在偏向于选择取值较多的特征问题,特征,answer2feature,,,,,,,,,,,
主动进行的连续的特征离散化,特征,answer2feature,,,,,,,,,,,
主动进行的连续的特征离散化,离散,answer2feature,,,,,,,,,,,
比如m个样本的连续特征A有m个，先set在order，再两两组合求中间值，以该值点作为划分的待选点,特征,answer2feature,,,,,,,,,,,
**连续特征可以再后序特征划分中仍可继续参与计算**,特征,answer2feature,,,,,,,,,,,
**连续特征可以再后序特征划分中仍可继续参与计算**,特征,answer2feature,,,,,,,,,,,
缺失问题优化,优化,answer2feature,,,,,,,,,,,
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,训练,answer2feature,,,,,,,,,,,
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,增益,answer2feature,,,,,,,,,,,
训练：用所有未缺失的样本，和之前一样，计算每个属性的信息增益，但是这里的信息增益需要乘以一个系数（未缺失样本/总样本）,增益,answer2feature,,,,,,,,,,,
预测：直接跳过该节点，并将此样本划入所有子节点，划分后乘以系数计算，系数为不缺失部分的样本分布,预测,answer2feature,,,,,,,,,,,
分类情况下的变量特征选择,分类,answer2feature,,,,,,,,,,,
分类情况下的变量特征选择,特征选择,answer2feature,,,,,,,,,,,
离散变量：二分划分,离散,answer2feature,,,,,,,,,,,
回归情况下，连续变量不再采取中间值划分，采用最小方差法,回归,answer2feature,,,,,,,,,,,
我们的算法从根节点开始，用训练集递归的建立CART树。,训练,answer2feature,,,,,,,,,,,
我们的算法从根节点开始，用训练集递归的建立CART树。,递归,answer2feature,,,,,,,,,,,
我们的算法从根节点开始，用训练集递归的建立CART树。,CART,answer2feature,,,,,,,,,,,
我们的算法从根节点开始，用训练集递归的建立CART树。,树,answer2feature,,,,,,,,,,,
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,阈值,answer2feature,,,,,,,,,,,
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,特征,answer2feature,,,,,,,,,,,
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,决策,answer2feature,,,,,,,,,,,
对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归,递归,answer2feature,,,,,,,,,,,
计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,阈值,answer2feature,,,,,,,,,,,
计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归,递归,answer2feature,,,,,,,,,,,
计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,特征,answer2feature,,,,,,,,,,,
计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数,特征值,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征,answer2feature,,,,,,,,,,,
在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2,特征值,answer2feature,,,,,,,,,,,
递归1～4,递归,answer2feature,,,,,,,,,,,
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征,answer2feature,,,,,,,,,,,
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征,answer2feature,,,,,,,,,,,
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),特征值,answer2feature,,,,,,,,,,,
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),均值,answer2feature,,,,,,,,,,,
对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点，其中c1为D1的均值，c2为D2的均值：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g928jep1nkj309a00q745.jpg),均值,answer2feature,,,,,,,,,,,
回归树：利用最终叶子的均值或者中位数来作为输出结果,回归,answer2feature,,,,,,,,,,,
回归树：利用最终叶子的均值或者中位数来作为输出结果,树,answer2feature,,,,,,,,,,,
回归树：利用最终叶子的均值或者中位数来作为输出结果,均值,answer2feature,,,,,,,,,,,
回归树：利用最终叶子的均值或者中位数来作为输出结果,中位数,answer2feature,,,,,,,,,,,
回归树：利用最终叶子的均值或者中位数来作为输出结果,输出,answer2feature,,,,,,,,,,,
分类树：利用最终叶子的大概率的分类类别来作为输出结果,分类,answer2feature,,,,,,,,,,,
分类树：利用最终叶子的大概率的分类类别来作为输出结果,树,answer2feature,,,,,,,,,,,
分类树：利用最终叶子的大概率的分类类别来作为输出结果,分类,answer2feature,,,,,,,,,,,
分类树：利用最终叶子的大概率的分类类别来作为输出结果,输出,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,目标,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,函数,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,参数,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,线性回归,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,训练,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,预测,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,误差,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,分类,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,树,answer2feature,,,,,,,,,,,
目标函数为：????(????)=??(????)+??|????|，其中，α为正则化参数，这和线性回归的正则化一样。??(????)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|????|是子树T的叶子节点的数量,回归,answer2feature,,,,,,,,,,,
当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,CART,answer2feature,,,,,,,,,,,
当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature,,,,,,,,,,,
当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,CART,answer2feature,,,,,,,,,,,
当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature,,,,,,,,,,,
当??=0时，即没有正则化，原始的生成的CART树即为最优子树。当??=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。,树,answer2feature,,,,,,,,,,,
当然，这是两种极端情况。一般来说，??越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的??，一定存在使损失函数????(??)最小的唯一子树。,剪枝,answer2feature,,,,,,,,,,,
当然，这是两种极端情况。一般来说，??越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的??，一定存在使损失函数????(??)最小的唯一子树。,损失,answer2feature,,,,,,,,,,,
当然，这是两种极端情况。一般来说，??越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的??，一定存在使损失函数????(??)最小的唯一子树。,函数,answer2feature,,,,,,,,,,,
"由枝剪到根结点及不枝剪两种情况可得：??=(??(??)???(????))/(|????|?1),C(T)为根结点误差",误差,answer2feature,,,,,,,,,,,
计算出每个子树是否剪枝的阈值??,剪枝,answer2feature,,,,,,,,,,,
计算出每个子树是否剪枝的阈值??,阈值,answer2feature,,,,,,,,,,,
选择阈值??集合中的最小值,阈值,answer2feature,,,,,,,,,,,
分别针对不同的最小值??所对应的剪枝后的最优子树做交叉验证,剪枝,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,分类,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,回归,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,离散,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,梯度,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,导数,answer2feature,,,,,,,,,,,
无论是分类树还是回归树，无论是连续变量还是离散变量，树模型一直想找的是最优切分点，不存在梯度导数等计算，数值缩放不影响分裂点位置，对树模型的结构不造成影响。,树,answer2feature,,,,,,,,,,,
缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,特征,answer2feature,,,,,,,,,,,
缺失值不敏感，对特征的宽容程度高，可缺失可连续可离散,离散,answer2feature,,,,,,,,,,,
可以解决线性及非线性问题,非线性,answer2feature,,,,,,,,,,,
有特征选择等辅助功能,特征选择,answer2feature,,,,,,,,,,,
正负量级有偏样本的样本效果较差,较差,answer2feature,,,,,,,,,,,
单棵树的拟合效果欠佳，容易过拟合,拟合,answer2feature,,,,,,,,,,,
单棵树的拟合效果欠佳，容易过拟合,过拟合,answer2feature,,,,,,,,,,,
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2・||W||・||W||)subjectto：y(wx+b)>=1，其中||・||为2范数,分类,answer2feature,,,,,,,,,,,
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2・||W||・||W||)subjectto：y(wx+b)>=1，其中||・||为2范数,优化,answer2feature,,,,,,,,,,,
从分类平面，到求两类间的最大间隔![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fr1xbmyj3034018mwy.jpg)，到转化为求间隔分之一等优化问题：loss=min(1/2・||W||・||W||)subjectto：y(wx+b)>=1，其中||・||为2范数,范数,answer2feature,,,,,,,,,,,
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature,,,,,,,,,,,
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature,,,,,,,,,,,
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,优化,answer2feature,,,,,,,,,,,
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,拉格朗日,answer2feature,,,,,,,,,,,
然后就是优化问题的解决办法，首先是用拉格拉日乘子把约束优化转化为无约束优化，对各个变量求导令其为零，得到的式子带入拉格朗日式子从而转化为对偶问题,对偶,answer2feature,,,,,,,,,,,
最后再利用SMO（序列最小优化）来解决这个对偶问题,优化,answer2feature,,,,,,,,,,,
最后再利用SMO（序列最小优化）来解决这个对偶问题,对偶,answer2feature,,,,,,,,,,,
在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。换句话说，就是超平面附近决定超平面位置的那些参与计算锁定平面位置的点,向量,answer2feature,,,,,,,,,,,
支持向量的添加才会提高，否则无效,向量,answer2feature,,,,,,,,,,,
可以，把loss函数变为：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93jrdscfrj309l011dfo.jpg),函数,answer2feature,,,,,,,,,,,
非线性问题,非线性,answer2feature,,,,,,,,,,,
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,SVM,answer2feature,,,,,,,,,,,
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,拉格朗日,answer2feature,,,,,,,,,,,
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,核,answer2feature,,,,,,,,,,,
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,函数,answer2feature,,,,,,,,,,,
SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以用smo算法解出非线性分类器,非线性,answer2feature,,,,,,,,,,,
硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,SVM,answer2feature,,,,,,,,,,,
硬SVM分类器（线性可分）：当训练数据可分时，通过间隔最大化，直接得到线性表分类器,训练,answer2feature,,,,,,,,,,,
软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,SVM,answer2feature,,,,,,,,,,,
软SVM分类器（线性可分）：当训练数据近似可分时，通过软间隔最大化，得到线性表分类器,训练,answer2feature,,,,,,,,,,,
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,训练,answer2feature,,,,,,,,,,,
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,核,answer2feature,,,,,,,,,,,
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,函数,answer2feature,,,,,,,,,,,
kernelSVM：当训练数据线性不可分时，通过核函数+软间隔的技巧，得到一个非线性的分类器,非线性,answer2feature,,,,,,,,,,,
svm原问题是：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),svm,answer2feature,,,,,,,,,,,
svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),svm,answer2feature,,,,,,,,,,,
svm对偶问题：求解![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg),对偶,answer2feature,,,,,,,,,,,
"拉格朗日乘子法：求f的最小值时，有h=0的限制条件，那么就构造∑λh+f=Loss,作为新loss",拉格朗日,answer2feature,,,,,,,,,,,
引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,拉格朗日,answer2feature,,,,,,,,,,,
引入松弛变量α的目的是构造满足拉格朗日条件的限制性条件,限制性,answer2feature,,,,,,,,,,,
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,求偏,answer2feature,,,,,,,,,,,
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,求偏,answer2feature,,,,,,,,,,,
在对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数,导数,answer2feature,,,,,,,,,,,
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,限制性,answer2feature,,,,,,,,,,,
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,规划,answer2feature,,,,,,,,,,,
因为原问题是带有限制性条件的凸二次规划问题不方便求解，转换为对偶问题更加高效,对偶,answer2feature,,,,,,,,,,,
引入了核函数,核,answer2feature,,,,,,,,,,,
引入了核函数,函数,answer2feature,,,,,,,,,,,
原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,限制性,answer2feature,,,,,,,,,,,
原问题是要考虑限制性条件的最优，而对偶问题考虑的是类似分情况讨论的解析问题,对偶,answer2feature,,,,,,,,,,,
因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0,向量,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,求偏,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,求偏,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,导数,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,联立方程,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,不等式,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,优化,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,优化,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93g16v0syj309d01gglh.jpg)分别对原来的w求偏导之外再对新构造的乘子λ和松弛变量α求偏导数，并且都等于0后的联立方程便是不等式约束优化优化问题的KKT(KarushKuhnTucker)条件,KKT,answer2feature,,,,,,,,,,,
KKT乘子λ>=0,KKT,answer2feature,,,,,,,,,,,
原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),损失,answer2feature,,,,,,,,,,,
原损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93fy1gm79j309b0173yd.jpg),函数,answer2feature,,,,,,,,,,,
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),优化,answer2feature,,,,,,,,,,,
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),损失,answer2feature,,,,,,,,,,,
优化后的损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93gp9vlbjj30c401gt8m.jpg),函数,answer2feature,,,,,,,,,,,
要求KKT乘子λ>=0,KKT,answer2feature,,,,,,,,,,,
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,核,answer2feature,,,,,,,,,,,
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,函数,answer2feature,,,,,,,,,,,
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,特征,answer2feature,,,,,,,,,,,
核函数能够将特征从低维空间映射到高维空间，这个映射可以把低维空间中不可分的两类点变成高维线性可分的,高维,answer2feature,,,,,,,,,,,
线性核函数：主要用于线性可分的情形。参数少，速度快。,核,answer2feature,,,,,,,,,,,
线性核函数：主要用于线性可分的情形。参数少，速度快。,函数,answer2feature,,,,,,,,,,,
线性核函数：主要用于线性可分的情形。参数少，速度快。,参数,answer2feature,,,,,,,,,,,
多项式核函数：,多项式,answer2feature,,,,,,,,,,,
多项式核函数：,核,answer2feature,,,,,,,,,,,
多项式核函数：,函数,answer2feature,,,,,,,,,,,
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,核,answer2feature,,,,,,,,,,,
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,函数,answer2feature,,,,,,,,,,,
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,参数,answer2feature,,,,,,,,,,,
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,分类,answer2feature,,,,,,,,,,,
高斯核函数：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。,参数,answer2feature,,,,,,,,,,,
sigmoid核函数：,sigmoid,answer2feature,,,,,,,,,,,
sigmoid核函数：,核,answer2feature,,,,,,,,,,,
sigmoid核函数：,函数,answer2feature,,,,,,,,,,,
拉普拉斯核函数：,拉普拉斯,answer2feature,,,,,,,,,,,
拉普拉斯核函数：,核,answer2feature,,,,,,,,,,,
拉普拉斯核函数：,函数,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,特征,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,核,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,函数,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,特征,answer2feature,,,,,,,,,,,
我用的比较多的是线性核函数和高斯核函数，线性用于特征多，线性问题的时候，高斯核函数用于特征少，非线性问题需要升维的时候,非线性,answer2feature,,,,,,,,,,,
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,机器学习,answer2feature,,,,,,,,,,,
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,核,answer2feature,,,,,,,,,,,
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,函数,answer2feature,,,,,,,,,,,
在机器学习中常用的核函数，一般有这么几类，也就是LibSVM中自带的这几类：,LibSVM,answer2feature,,,,,,,,,,,
"2)多项式：K(v1,v2)=(r<v1,v2>+c)^n",多项式,answer2feature,,,,,,,,,,,
"3)Radialbasisfunction：K(v1,v2)=exp(r||v1v2||^2)",exp,answer2feature,,,,,,,,,,,
"4)Sigmoid：tanh(r<v1,v2>+c)",Sigmoid,answer2feature,,,,,,,,,,,
Mercer定理：核函数矩阵是对称半正定的,核,answer2feature,,,,,,,,,,,
Mercer定理：核函数矩阵是对称半正定的,函数,answer2feature,,,,,,,,,,,
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),非线性,answer2feature,,,,,,,,,,,
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),分类,answer2feature,,,,,,,,,,,
计算非线性分类问题下，需要利用到SMO方法求解，该方法复杂度高O(n^2),复杂度,answer2feature,,,,,,,,,,,
在使用核函数的时候参数假设全靠试，时间成本过高,核,answer2feature,,,,,,,,,,,
在使用核函数的时候参数假设全靠试，时间成本过高,函数,answer2feature,,,,,,,,,,,
在使用核函数的时候参数假设全靠试，时间成本过高,参数,answer2feature,,,,,,,,,,,
e的n次方的泰勒展开得到了一个无穷维度的映射,泰勒展开,answer2feature,,,,,,,,,,,
e的n次方的泰勒展开得到了一个无穷维度的映射,维度,answer2feature,,,,,,,,,,,
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,svm,answer2feature,,,,,,,,,,,
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,svm,answer2feature,,,,,,,,,,,
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,margin,answer2feature,,,,,,,,,,,
如果在svm容忍范围内或者在svm的margin外，则不受影响；否则决策边界会发生调整,决策,answer2feature,,,,,,,,,,,
逻辑回归，线性svm,逻辑,answer2feature,,,,,,,,,,,
逻辑回归，线性svm,回归,answer2feature,,,,,,,,,,,
逻辑回归，线性svm,svm,answer2feature,,,,,,,,,,,
非线性：,非线性,answer2feature,,,,,,,,,,,
贝叶斯，决策树，核svm，DNN,核,answer2feature,,,,,,,,,,,
贝叶斯，决策树，核svm，DNN,svm,answer2feature,,,,,,,,,,,
贝叶斯，决策树，核svm，DNN,DNN,answer2feature,,,,,,,,,,,
数据量大特征多：,特征,answer2feature,,,,,,,,,,,
逻辑回归,逻辑,answer2feature,,,,,,,,,,,
逻辑回归,回归,answer2feature,,,,,,,,,,,
数据量少特征少：,特征,answer2feature,,,,,,,,,,,
核svm,核,answer2feature,,,,,,,,,,,
核svm,svm,answer2feature,,,,,,,,,,,
树模型,树,answer2feature,,,,,,,,,,,
LR是参数模型，SVM为非参数模型。,LR,answer2feature,,,,,,,,,,,
LR是参数模型，SVM为非参数模型。,参数,answer2feature,,,,,,,,,,,
LR是参数模型，SVM为非参数模型。,SVM,answer2feature,,,,,,,,,,,
LR是参数模型，SVM为非参数模型。,参数,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,LR,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,损失,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,函数,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,logisticalloss,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,SVM,answer2feature,,,,,,,,,,,
LR采用的损失函数为logisticalloss，而SVM采用的是hingeloss。,hingeloss,answer2feature,,,,,,,,,,,
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,SVM,answer2feature,,,,,,,,,,,
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,分类,answer2feature,,,,,,,,,,,
在学习分类器的时候，SVM只考虑与分类最相关的少数支持向量点。,向量,answer2feature,,,,,,,,,,,
LR的模型相对简单，在进行大规模线性分类时比较方便。,LR,answer2feature,,,,,,,,,,,
LR的模型相对简单，在进行大规模线性分类时比较方便。,分类,answer2feature,,,,,,,,,,,
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",mse,answer2feature,,,,,,,,,,,
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",均方,answer2feature,,,,,,,,,,,
"mse,最小均方误差:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg)",误差,answer2feature,,,,,,,,,,,
最小二乘,二乘,answer2feature,,,,,,,,,,,
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),损失,answer2feature,,,,,,,,,,,
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93458dklvj306l011t8j.jpg),函数,answer2feature,,,,,,,,,,,
使右侧为0可得：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934a6q83tj304300kdfm.jpg),右侧,answer2feature,,,,,,,,,,,
如果X点乘X的转置可逆则有唯一解，否则无法如此求解,可逆,answer2feature,,,,,,,,,,,
梯度下降,梯度,answer2feature,,,,,,,,,,,
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),损失,answer2feature,,,,,,,,,,,
损失函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g933uf4aimj305u01fa9w.jpg),函数,answer2feature,,,,,,,,,,,
求导可得梯度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934mwtnodj307401fdfp.jpg),梯度,answer2feature,,,,,,,,,,,
加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),l2,answer2feature,,,,,,,,,,,
加上l2的线性回归：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934px37g4j305b017glf.jpg),线性回归,answer2feature,,,,,,,,,,,
在用最小二乘推导的过程和上面一样，最后在结果上进行了平滑，保证有解：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g934t64beij304w00kmwy.jpg),二乘,answer2feature,,,,,,,,,,,
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature,,,,,,,,,,,
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,稀疏,answer2feature,,,,,,,,,,,
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,线性关系,answer2feature,,,,,,,,,,,
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature,,,,,,,,,,,
特征过多，稀疏线性关系，目的为了在一堆特征里面找出主要的特征,特征,answer2feature,,,,,,,,,,,
kmeans是两个步骤交替进行，可以分别看成E步和M步,kmeans,answer2feature,,,,,,,,,,,
M步中将每类的中心更新为分给该类各点的均值，可以认为是在「各类分布均为单位方差的高斯分布」的假设下，最大化似然值；,均值,answer2feature,,,,,,,,,,,
E步中将每个点分给中心距它最近的类（硬分配），可以看成是EM算法中E步（软分配）的近似,EM,answer2feature,,,,,,,,,,,
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,参数,answer2feature,,,,,,,,,,,
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,MSE,answer2feature,,,,,,,,,,,
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,MSE,answer2feature,,,,,,,,,,,
M步中的最大化似然值，更新参数依赖的是MSE，MSE至少存在局部最优解，必然收敛,收敛,answer2feature,,,,,,,,,,,
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,聚类,answer2feature,,,,,,,,,,,
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,kmeans,answer2feature,,,,,,,,,,,
先层次聚类，再在不同层次上选取初始点进行kmeans聚类,聚类,answer2feature,,,,,,,,,,,
贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),贝叶斯公式,answer2feature,,,,,,,,,,,
贝叶斯公式是完整的数学公式P(A/B)=P(A)P(B/A)/P(B),数学公式,answer2feature,,,,,,,,,,,
"朴素贝叶斯=贝叶斯公式+条件独立假设，在实际使用过程中，朴素贝叶斯完全只需要关注P(A,B)=P(A)P(B/A)即可",贝叶斯公式,answer2feature,,,,,,,,,,,
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,多项式,answer2feature,,,,,,,,,,,
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,多项式,answer2feature,,,,,,,,,,,
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,离散,answer2feature,,,,,,,,,,,
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,特征,answer2feature,,,,,,,,,,,
多项式：多项式模型适用于离散特征情况，在文本领域应用广泛，其基本思想是：我们将重复的词语视为其出现多次,领域,answer2feature,,,,,,,,,,,
因为统计次数，所以会出现0次可能，所以实际中进行了平滑操作,统计,answer2feature,,,,,,,,,,,
两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,特征,answer2feature,,,,,,,,,,,
两者形式非常像，区别就在先验平滑分母考虑的是平滑类别y个数，后验平滑分母考虑的是平滑特征对应特征x可选的个数,特征,answer2feature,,,,,,,,,,,
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),特征,answer2feature,,,,,,,,,,,
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),高斯公式,answer2feature,,,,,,,,,,,
高斯：高斯模型适合连续特征情况，[高斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/数学/概率密度分布/概率密度分布.md#L1),blob,answer2feature,,,,,,,,,,,
高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,特征,answer2feature,,,,,,,,,,,
高斯模型假设在对应类别下的每一维特征都服从高斯分布（正态分布）,正态分布,answer2feature,,,,,,,,,,,
伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,离散,answer2feature,,,,,,,,,,,
伯努利：伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次,特征,answer2feature,,,,,,,,,,,
拉普拉斯平滑,拉普拉斯,answer2feature,,,,,,,,,,,
优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,分类,answer2feature,,,,,,,,,,,
优点：对小规模数据表现很好，适合多分类任务，适合增量式训练,训练,answer2feature,,,,,,,,,,,
缺点：对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）,离散,answer2feature,,,,,,,,,,,
生成模型和判别模型,判别,answer2feature,,,,,,,,,,,
分布函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93b9whhwuj306z01amwz.jpg),函数,answer2feature,,,,,,,,,,,
密度函数：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93bdnikzbj306e01pjr8.jpg),函数,answer2feature,,,,,,,,,,,
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,参数,answer2feature,,,,,,,,,,,
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,参数,answer2feature,,,,,,,,,,,
其中，μ表示位置参数，γ为形状参数。**logistic分布比正太分布有更长的尾部且波峰更尖锐**,logistic,answer2feature,,,,,,,,,,,
y=sigmoid(f(x)),sigmoid,answer2feature,,,,,,,,,,,
可以看作是一次线性拟合+一次sigmoid的非线性变化,拟合,answer2feature,,,,,,,,,,,
可以看作是一次线性拟合+一次sigmoid的非线性变化,sigmoid,answer2feature,,,,,,,,,,,
可以看作是一次线性拟合+一次sigmoid的非线性变化,非线性,answer2feature,,,,,,,,,,,
对于lr来说事情只有发生和不发生两种可能，对于已知样本来说，满足伯努利的概率假设：,lr,answer2feature,,,,,,,,,,,
第i个样本正确预测的概率如上可得,预测,answer2feature,,,,,,,,,,,
几率odds,odds,answer2feature,,,,,,,,,,,
数据特征下属于正例及反例的比值,特征,answer2feature,,,,,,,,,,,
极大似然,极大似然,answer2feature,,,,,,,,,,,
第i个样本正确预测的概率如上可得每条样本的情况下,预测,answer2feature,,,,,,,,,,,
综合全部样本发生的概率都要最大的话，采取极大似然连乘可得：,极大似然,answer2feature,,,,,,,,,,,
损失函数,损失,answer2feature,,,,,,,,,,,
损失函数,函数,answer2feature,,,,,,,,,,,
通常会对极大似然取对数，得到损失函数，方便计算,极大似然,answer2feature,,,,,,,,,,,
通常会对极大似然取对数，得到损失函数，方便计算,损失,answer2feature,,,,,,,,,,,
通常会对极大似然取对数，得到损失函数，方便计算,函数,answer2feature,,,,,,,,,,,
梯度下降,梯度,answer2feature,,,,,,,,,,,
损失函数求偏导，更新θ,损失,answer2feature,,,,,,,,,,,
损失函数求偏导，更新θ,函数,answer2feature,,,,,,,,,,,
损失函数求偏导，更新θ,求偏,answer2feature,,,,,,,,,,,
首先需要理解梯度下降的更新公式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cn8ok1fj307a01ft8k.jpg),梯度,answer2feature,,,,,,,,,,,
同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,特征,answer2feature,,,,,,,,,,,
同一条样本不同特征维度进行拆分，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)处并行，把![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93csl2l0pj301400ia9t.jpg)内的xi和Wi拆分成块分别计算后合并，再把外层![](https://tva1.sinaimg.cn/large/006y8mN6gy1g93cnjne2dj303200ia9u.jpg)同样拆分成若干块进行计算,维度,answer2feature,,,,,,,,,,,
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,特征,answer2feature,,,,,,,,,,,
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,拟合,answer2feature,,,,,,,,,,,
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,比率,answer2feature,,,,,,,,,,,
观测样本中该特征在正负类中出现概率的比值满足线性条件，用的是线性拟合比率值，所以叫回归,回归,answer2feature,,,,,,,,,,,
1.点击行为为正向，未点击行为为负向，ctr需要得到点击行为的概率，lr可以产出正向行为的概率，完美match,lr,answer2feature,,,,,,,,,,,
2.实现简单，方便并行，计算迭代速度很快,迭代,answer2feature,,,,,,,,,,,
3.可解释性强，可结合正则化等优化方法,优化方法,answer2feature,,,,,,,,,,,
特征之间尽可能独立,特征,answer2feature,,,,,,,,,,,
不独立所以我们把不独立的特征交叉了,特征,answer2feature,,,,,,,,,,,
还记得FM的思路？,FM,answer2feature,,,,,,,,,,,
离散特征,离散,answer2feature,,,,,,,,,,,
离散特征,特征,answer2feature,,,,,,,,,,,
连续特征通常没有特别含义，31岁和32岁差在哪？,特征,answer2feature,,,,,,,,,,,
离散特征方便交叉考虑,离散,answer2feature,,,,,,,,,,,
离散特征方便交叉考虑,特征,answer2feature,,,,,,,,,,,
使的lr满足分布假设,lr,answer2feature,,,,,,,,,,,
在某种确定分类上的特征分布满足高斯分布,分类,answer2feature,,,,,,,,,,,
在某种确定分类上的特征分布满足高斯分布,特征,answer2feature,,,,,,,,,,,
C1和C2为正负类，观测样本中该特征在正负类中出现概率的比值满足线性条件的前提就是P服从正太分布,特征,answer2feature,,,,,,,,,,,
实际中不满足的很多，不满足我们通常就离散化，oneHotEncode,离散,answer2feature,,,,,,,,,,,
此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,贝叶斯公式,answer2feature,,,,,,,,,,,
此处就用到了全概率公式推导，有可能会回到[写出全概率公式&贝叶斯公式](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/先验概率和后验概率/先验概率和后验概率.md#L96)的问题中,blob,answer2feature,,,,,,,,,,,
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,lr,answer2feature,,,,,,,,,,,
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,odds,answer2feature,,,,,,,,,,,
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,线性回归,answer2feature,,,,,,,,,,,
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,odds,answer2feature,,,,,,,,,,,
思路一：lr的前提假设就是几率odds满足线性回归，odds又为正负样本的log比，参见`满足什么样条件的数据用LR最好？`中第三点公式的展开,LR,answer2feature,,,,,,,,,,,
思路二：Exponentialmodel的形式是这样的：假设第i个特征对第k类的贡献是![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmq11x6zj300s00f3y9.jpg)，则数据点![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmqpotqfj302700imwx.jpg)属于第k类的概率正比于![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wmrp4qv4j305b00i0sj.jpg)。,特征,answer2feature,,,,,,,,,,,
化简即为sigmoid,sigmoid,answer2feature,,,,,,,,,,,
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,glm,answer2feature,,,,,,,,,,,
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,lr,answer2feature,,,,,,,,,,,
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,sigmoid,answer2feature,,,,,,,,,,,
思路三：glm有满足指数族的性质，而作为lr作为y满足伯努利分布的的线性条件，伯努利分布的指数族形式就是sigmoid，或者也叫连接函数,函数,answer2feature,,,,,,,,,,,
直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,分类,answer2feature,,,,,,,,,,,
直接对分类模型进行建模，前提假设为非常弱的指定类别上自变量的条件分布满足高斯,自变量,answer2feature,,,,,,,,,,,
由预测0/1的类别扩展到了预测01的概率值,预测,answer2feature,,,,,,,,,,,
由预测0/1的类别扩展到了预测01的概率值,预测,answer2feature,,,,,,,,,,,
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,线性回归,answer2feature,,,,,,,,,,,
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,全量,answer2feature,,,,,,,,,,,
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,敏感度,answer2feature,,,,,,,,,,,
线性回归在全量数据上的敏感度一致，sigmoid在分界点0.5处更加敏感,sigmoid,answer2feature,,,,,,,,,,,
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,sigmoid,answer2feature,,,,,,,,,,,
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,逻辑,answer2feature,,,,,,,,,,,
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,回归,answer2feature,,,,,,,,,,,
sigmoid在逻辑回归的参数更新中也不起影响，避免了更新速度不稳定的问题,参数,answer2feature,,,,,,,,,,,
更新速度只与真实的x和y相关，与激活函数无关，更新平稳,激活函数,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,mse,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,激活函数,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,sigmoid,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,sigmoid,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,函数,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,梯度,answer2feature,,,,,,,,,,,
比如mse就会导致更新速度与激活函数sigmoid挂钩，而sigmoid函数在定义域内的梯度大小都比较小(0.25>x)，不利于快速更新,快速,answer2feature,,,,,,,,,,,
mse下的lr损失函数非凸，难以得到解析解,mse,answer2feature,,,,,,,,,,,
mse下的lr损失函数非凸，难以得到解析解,lr,answer2feature,,,,,,,,,,,
mse下的lr损失函数非凸，难以得到解析解,损失,answer2feature,,,,,,,,,,,
mse下的lr损失函数非凸，难以得到解析解,函数,answer2feature,,,,,,,,,,,
way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,sigmoid,answer2feature,,,,,,,,,,,
way1:把01的sigmoid的lr结果Y映射为2y1，推导不变,lr,answer2feature,,,,,,,,,,,
"way2:把激活函数换成tanh，因为tanh的值域范围为\[1,1],满足结果，推导不变",激活函数,answer2feature,,,,,,,,,,,
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,sigmoid,answer2feature,,,,,,,,,,,
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,函数,answer2feature,,,,,,,,,,,
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,函数,answer2feature,,,,,,,,,,,
way3:依旧以sigmoid函数的话，似然函数(likelihood)模型是：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wngwzstcj30iu01edfw.jpg)，重复极大似然计算即可,极大似然,answer2feature,,,,,,,,,,,
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,损失,answer2feature,,,,,,,,,,,
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,函数,answer2feature,,,,,,,,,,,
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,收敛,answer2feature,,,,,,,,,,,
如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果,特征,answer2feature,,,,,,,,,,,
每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,特征,answer2feature,,,,,,,,,,,
每一个特征都是原来特征权重值的百分之一，线性可能解释性优点也消失了,特征,answer2feature,,,,,,,,,,,
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,训练,answer2feature,,,,,,,,,,,
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,收敛,answer2feature,,,,,,,,,,,
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,收敛,answer2feature,,,,,,,,,,,
增加训练收敛的难度及耗时，有限次数下可能共线性变量无法收敛，系数估计变得不可靠,估计,answer2feature,,,,,,,,,,,
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,泛化能力,answer2feature,,,,,,,,,,,
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,训练,answer2feature,,,,,,,,,,,
泛化能力变差，训练是两列特征可能会共线性，当线上数据加入噪声后共线性消失，效果可能变差,特征,answer2feature,,,,,,,,,,,
结论：可以，加l2正则项后可用,l2,answer2feature,,,,,,,,,,,
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,核,answer2feature,,,,,,,,,,,
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,逻辑,answer2feature,,,,,,,,,,,
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,回归,answer2feature,,,,,,,,,,,
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,拟合,answer2feature,,,,,,,,,,,
核逻辑回归，需要把拟合参数w表示成z的线性组合及representertheorem理论。这边比较复杂，待更新，需要了解：,参数,answer2feature,,,,,,,,,,,
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,L2,answer2feature,,,,,,,,,,,
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,核,answer2feature,,,,,,,,,,,
凡是进行L2正则化的线性问题我们都能使用核函数的技巧的证明,函数,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",L1,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",参数,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",拉普拉斯,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",lr,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",损失,answer2feature,,,,,,,,,,,
"L1正则项：为模型加了一个先验知识，未知参数w满足拉普拉斯分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpllyoblj303s011gle.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpimx0juj301g0133ya.jpg)项",函数,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",L2,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",参数,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",均值,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",lr,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",损失,answer2feature,,,,,,,,,,,
"L2正则项：为模型加了一个先验知识，未知参数w满足0均值正太分布，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpayd8u6j307k0190sl.jpg),u为0。在lr模型损失函数中新增了![](https://tva1.sinaimg.cn/large/006y8mN6gy1g8wpet47boj3011014mwx.jpg)项",函数,answer2feature,,,,,,,,,,,
这个问题还可以换一个说法，l1和l2的各自作用。,l1,answer2feature,,,,,,,,,,,
这个问题还可以换一个说法，l1和l2的各自作用。,l2,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l1,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,参数,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,拉普拉斯,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l2,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,参数,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,均值,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,拉普拉斯,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,特征,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l1,answer2feature,,,,,,,,,,,
刚才我们说到l1对未知参数w有个前提假设满足拉普拉斯分布，l2对未知参数的假设则是正太分布，且都是零均值，单纯从图像上我们就可以发现，拉普拉斯对w的规约到0的可能性更高，所以对于特征约束强的需求下l1合适，否则l2,l2,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,训练,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,误差,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,最小化,answer2feature,,,,,,,,,,,
结构风险最小化：在经验风险最小化的基础上（也就是训练误差最小化），尽可能采用简单的模型，以此提高泛化预测精度。,预测,answer2feature,,,,,,,,,,,
特征交叉，类似fm,特征,answer2feature,,,,,,,,,,,
核逻辑回归，类似svm,核,answer2feature,,,,,,,,,,,
核逻辑回归，类似svm,逻辑,answer2feature,,,,,,,,,,,
核逻辑回归，类似svm,回归,answer2feature,,,,,,,,,,,
核逻辑回归，类似svm,svm,answer2feature,,,,,,,,,,,
线性变换+非线性激活，类似neuralnetwork,非线性,answer2feature,,,,,,,,,,,
**模型中对数据对处理一般都有一个标答是提升数据表达能力，也就是使数据含有的可分信息量更大**,信息量,answer2feature,,,,,,,,,,,
加速收敛,加速,answer2feature,,,,,,,,,,,
加速收敛,收敛,answer2feature,,,,,,,,,,,
提高计算效率,效率,answer2feature,,,,,,,,,,,
梯度下降过程稳定,梯度,answer2feature,,,,,,,,,,,
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),标准化,answer2feature,,,,,,,,,,,
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),blob,answer2feature,,,,,,,,,,,
[归一化和标准化之间的关系](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/特征提取/数据变换.md#L6),预处理,answer2feature,,,,,,,,,,,
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,离散,answer2feature,,,,,,,,,,,
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,非线性,answer2feature,,,,,,,,,,,
原来的单变量可扩展到n个离散变量，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合,拟合,answer2feature,,,,,,,,,,,
离散后结合正则化可以进行特征筛选，更好防止过拟合,离散,answer2feature,,,,,,,,,,,
离散后结合正则化可以进行特征筛选，更好防止过拟合,特征,answer2feature,,,,,,,,,,,
离散后结合正则化可以进行特征筛选，更好防止过拟合,筛选,answer2feature,,,,,,,,,,,
离散后结合正则化可以进行特征筛选，更好防止过拟合,过拟合,answer2feature,,,,,,,,,,,
离散变量的计算相对于连续变量更快,离散,answer2feature,,,,,,,,,,,
lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,lr,answer2feature,,,,,,,,,,,
lr的output是彼此之间相对谁的可能性更高，而不是概率，概率是事情发生的可能，lr的output不代表可能,lr,answer2feature,,,,,,,,,,,
lr和线性回归,lr,answer2feature,,,,,,,,,,,
lr和线性回归,线性回归,answer2feature,,,,,,,,,,,
lr解用的极大似然，线性回归用的最小二乘,lr,answer2feature,,,,,,,,,,,
lr解用的极大似然，线性回归用的最小二乘,极大似然,answer2feature,,,,,,,,,,,
lr解用的极大似然，线性回归用的最小二乘,线性回归,answer2feature,,,,,,,,,,,
lr解用的极大似然，线性回归用的最小二乘,二乘,answer2feature,,,,,,,,,,,
lr用于分类，线性回归用于回归,lr,answer2feature,,,,,,,,,,,
lr用于分类，线性回归用于回归,分类,answer2feature,,,,,,,,,,,
lr用于分类，线性回归用于回归,线性回归,answer2feature,,,,,,,,,,,
lr用于分类，线性回归用于回归,回归,answer2feature,,,,,,,,,,,
但两者都是广义线性回归GLM问题,线性回归,answer2feature,,,,,,,,,,,
但两者都是广义线性回归GLM问题,GLM,answer2feature,,,,,,,,,,,
两者对非线性问题的处理能力都是欠佳的,非线性,answer2feature,,,,,,,,,,,
lr和最大熵,lr,answer2feature,,,,,,,,,,,
lr和最大熵,熵,answer2feature,,,,,,,,,,,
lr和svm,lr,answer2feature,,,,,,,,,,,
lr和svm,svm,answer2feature,,,,,,,,,,,
都可分类，都是判别式模型思路,分类,answer2feature,,,,,,,,,,,
都可分类，都是判别式模型思路,判别式,answer2feature,,,,,,,,,,,
lr是交叉熵，svm是HingeLoss,lr,answer2feature,,,,,,,,,,,
lr是交叉熵，svm是HingeLoss,熵,answer2feature,,,,,,,,,,,
lr是交叉熵，svm是HingeLoss,svm,answer2feature,,,,,,,,,,,
lr是交叉熵，svm是HingeLoss,HingeLoss,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,lr,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,全量,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,拟合,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,svm,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,向量,answer2feature,,,,,,,,,,,
lr是全量数据拟合，svm是支持向量拟合,拟合,answer2feature,,,,,,,,,,,
lr是参数估计有参数的前提假设，svm没有,lr,answer2feature,,,,,,,,,,,
lr是参数估计有参数的前提假设，svm没有,参数,answer2feature,,,,,,,,,,,
lr是参数估计有参数的前提假设，svm没有,svm,answer2feature,,,,,,,,,,,
lr依赖的是极大似然，svm依赖的是距离,lr,answer2feature,,,,,,,,,,,
lr依赖的是极大似然，svm依赖的是距离,极大似然,answer2feature,,,,,,,,,,,
lr依赖的是极大似然，svm依赖的是距离,svm,answer2feature,,,,,,,,,,,
lr和朴素贝叶斯,lr,answer2feature,,,,,,,,,,,
如果朴素贝叶斯也有在某一类上的数据x满足高斯分布的假设前提，lr和朴素贝叶斯一致,lr,answer2feature,,,,,,,,,,,
lr是判别模型，朴素贝叶斯是生成模型,lr,answer2feature,,,,,,,,,,,
lr是判别模型，朴素贝叶斯是生成模型,判别,answer2feature,,,,,,,,,,,
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,lr,answer2feature,,,,,,,,,,,
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,feature,answer2feature,,,,,,,,,,,
lr没有明确feature条件独立(但是不能共线性，理由之前讲了)，朴素贝叶斯要求feature条件独立,feature,answer2feature,,,,,,,,,,,
lr和最大熵模型,lr,answer2feature,,,,,,,,,,,
lr和最大熵模型,熵,answer2feature,,,,,,,,,,,
最大熵模型在解决二分类问题就是逻辑回归,熵,answer2feature,,,,,,,,,,,
最大熵模型在解决二分类问题就是逻辑回归,逻辑,answer2feature,,,,,,,,,,,
最大熵模型在解决二分类问题就是逻辑回归,回归,answer2feature,,,,,,,,,,,
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,熵,answer2feature,,,,,,,,,,,
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,分类,answer2feature,,,,,,,,,,,
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,逻辑,answer2feature,,,,,,,,,,,
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,回归,answer2feature,,,,,,,,,,,
最大熵模型在解决多分类问题的时候就是多项逻辑回归回归,回归,answer2feature,,,,,,,,,,,
随机梯度下降,梯度,answer2feature,,,,,,,,,,,
批梯度下降,梯度,answer2feature,,,,,,,,,,,
mini批梯度下降,梯度,answer2feature,,,,,,,,,,,
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,梯度,answer2feature,,,,,,,,,,,
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,参数,answer2feature,,,,,,,,,,,
除此之外，比如ada和冲量梯度下降法会对下降的速率速度进行控制，也会对不同更新速度的参数进行控制，等等，多用于深度学习中,深度,answer2feature,,,,,,,,,,,
简单，易部署，训练速度快,训练,answer2feature,,,,,,,,,,,
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),平衡,answer2feature,,,,,,,,,,,
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),blob,answer2feature,,,,,,,,,,,
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),预处理,answer2feature,,,,,,,,,,,
数据不平衡需要人为处理，weight_class/[有哪些常见的采样方法](https://github.com/sladesha/Reflection_Summary/blob/master/数据预处理/数据平衡/采样.md#L11),平衡,answer2feature,,,,,,,,,,,
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature,,,,,,,,,,,
特征筛选，特征的系数决定该特征的重要性,筛选,answer2feature,,,,,,,,,,,
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature,,,,,,,,,,,
特征筛选，特征的系数决定该特征的重要性,特征,answer2feature,,,,,,,,,,,
特征筛选，特征的系数决定该特征的重要性,重要性,answer2feature,,,,,,,,,,,
sklearn.linear_model.LogisticRegression,sklearn,answer2feature,,,,,,,,,,,
sklearn.linear_model.LogisticRegression,LogisticRegression,answer2feature,,,,,,,,,,,
看部分参数的解释,参数,answer2feature,,,,,,,,,,,
比如输出值的形式，输出的格式,输出,answer2feature,,,,,,,,,,,
比如输出值的形式，输出的格式,输出,answer2feature,,,,,,,,,,,
penalty是正则化，solver是函数优化方法,函数,answer2feature,,,,,,,,,,,
penalty是正则化，solver是函数优化方法,优化方法,answer2feature,,,,,,,,,,,
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,l1,answer2feature,,,,,,,,,,,
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,l2,answer2feature,,,,,,,,,,,
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,坐标轴,answer2feature,,,,,,,,,,,
penalty包含l1和l2两种，solver包含坐标轴下降、牛顿、随机梯度下降等,梯度,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,梯度,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l1,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,损失,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,函数,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,一阶,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,导数,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,坐标轴,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l1,answer2feature,,,,,,,,,,,
牛顿法，拟牛顿法和随机梯度下降都不能使用l1，因为他们都需要损失函数的一阶二阶导数，而坐标轴下降法不限制这些，l1和l2都可行。,l2,answer2feature,,,,,,,,,,,
l1和l2选择参考上面讲的正则化部分,l1,answer2feature,,,,,,,,,,,
l1和l2选择参考上面讲的正则化部分,l2,answer2feature,,,,,,,,,,,
随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,梯度,answer2feature,,,,,,,,,,,
随机梯度下降在数据较少的时候最好别用，但是速度比较快。默认的是坐标轴下降法,坐标轴,answer2feature,,,,,,,,,,,
首先，决定是否为多分类的参数是multi_class,分类,answer2feature,,,,,,,,,,,
首先，决定是否为多分类的参数是multi_class,参数,answer2feature,,,,,,,,,,,
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,LabelEncoder,answer2feature,,,,,,,,,,,
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,目标值,answer2feature,,,,,,,,,,,
在真正执行multi的时候，会通过LabelEncoder把目标值y离散化，不停的选择两类去做ovr的计算直到取完所有情况,离散,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,逻辑,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,回归,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,特征,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,函数,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,梯度,answer2feature,,,,,,,,,,,
逻辑回归假设观测样本中该特征在正负类中出现结果服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的,参数,answer2feature,,,,,,,,,,,
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,逻辑,answer2feature,,,,,,,,,,,
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,回归,answer2feature,,,,,,,,,,,
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,非线性,answer2feature,,,,,,,,,,,
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,核,answer2feature,,,,,,,,,,,
逻辑回归本质是线性模型，只能解决线性相关的问题，非线性相关用核或者svm等,svm,answer2feature,,,,,,,,,,,
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,逻辑,answer2feature,,,,,,,,,,,
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,回归,answer2feature,,,,,,,,,,,
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,特征,answer2feature,,,,,,,,,,,
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,核,answer2feature,,,,,,,,,,,
逻辑回归不需要特征的条件独立，但是不能共线性，需要核线性回归一样，做共线性检验,线性回归,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,逻辑,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,鲁棒,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,SVM,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,logistic,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,向量,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,判别,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,logistic,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,回归,answer2feature,,,,,,,,,,,
逻辑回归对样本噪声是鲁棒对，SVM对噪声比较敏感，而logistic回归对噪声不是很敏感，是因为如果噪声点落在了支持向量上，将会直接影响判别面的方程。而logistic回归通过最大似然求解模型参数，将会弱化噪声的影响,参数,answer2feature,,,,,,,,,,,
随机森林=bagging+决策树,随机森林,answer2feature,,,,,,,,,,,
随机森林=bagging+决策树,bagging,answer2feature,,,,,,,,,,,
随机：特征选择随机+数据采样随机,特征选择,answer2feature,,,,,,,,,,,
特征随机是在决策树**每个结点上选择的时候随机**，并不是在每棵树创建的时候随机,特征,answer2feature,,,,,,,,,,,
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,特征选择,answer2feature,,,,,,,,,,,
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,全量,answer2feature,,,,,,,,,,,
每个结点上对特征选择都是从全量特征中进行采样对，**不会剔除已利用的**,特征,answer2feature,,,,,,,,,,,
数据采样，是有放回的采样,有放回,answer2feature,,,,,,,,,,,
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,回归,answer2feature,,,,,,,,,,,
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,回归,answer2feature,,,,,,,,,,,
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,输出,answer2feature,,,,,,,,,,,
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,分类,answer2feature,,,,,,,,,,,
可分类可回归，回归是对输出值进行简单平均，分类是对输出值进行简单投票,输出,answer2feature,,,,,,,,,,,
CART树,CART,answer2feature,,,,,,,,,,,
CART树,树,answer2feature,,,,,,,,,,,
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature,,,,,,,,,,,
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature,,,,,,,,,,,
从M个输入特征里随机选择m个输入特征，然后从这m个输入特征里选择一个最好的进行分裂,特征,answer2feature,,,,,,,,,,,
不需要剪枝，直到该节点的所有训练样例都属于同一类,剪枝,answer2feature,,,,,,,,,,,
不需要剪枝，直到该节点的所有训练样例都属于同一类,训练,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),离散,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),分类,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),回归,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature,,,,,,,,,,,
在连续值和离散值上：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",分类,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",RF,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",CART,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",分类,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",树,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",标准,answer2feature,,,,,,,,,,,
"分类RF对应的CART分类树默认是基尼系数gini,另一个可选择的标准是信息增益",增益,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,回归,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,RF,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,CART,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,回归,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,树,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,mse,answer2feature,,,,,,,,,,,
回归RF对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae,标准,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),损失,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),函数,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),分类,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),CART,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),回归,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),树,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),blob,answer2feature,,,,,,,,,,,
参考决策树的损失函数即可：[CART分类树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164)和[CART回归树建立算法的具体流程](https://github.com/sladesha/Reflection_Summary/blob/master/机器学习/决策树/决策树.md#L164),机器学习,answer2feature,,,,,,,,,,,
增加树的数量,树,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),bagging,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),期望,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),期望,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),Bagging,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),Boosting,answer2feature,,,,,,,,,,,
bagging算法中，基模型的期望与整体期望一致，参考[就理论角度论证Bagging、Boosting的方差偏差问题](https://github.com/sladesha/Reflection_Summary/blob/master/基础概念/方差与偏差/方差与偏差.md#L7),blob,answer2feature,,,,,,,,,,,
随着基模型数（m）的增多，整体模型的方差减少，从而防止过拟合的能力增强，模型的准确度得到提高,过拟合,answer2feature,,,,,,,,,,,
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征选择,answer2feature,,,,,,,,,,,
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature,,,,,,,,,,,
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature,,,,,,,,,,,
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,差别,answer2feature,,,,,,,,,,,
特征选择方向：对于某个特征，如果用另外一个随机值替代它之后的表现比之前更差，则表明该特征比较重要，所占的权重应该较大，不能用一个随机值替代。相反，如果随机值替代后的表现没有太大差别，则表明该特征不那么重要，可有可无,特征,answer2feature,,,,,,,,,,,
通过permutation的方式将原来的所有N个样本的第i个特征值重新打乱分布（相当于重新洗牌）,特征值,answer2feature,,,,,,,,,,,
是使用uniform或者gaussian抽取随机值替换原特征,抽取,answer2feature,,,,,,,,,,,
是使用uniform或者gaussian抽取随机值替换原特征,特征,answer2feature,,,,,,,,,,,
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,随机森林,answer2feature,,,,,,,,,,,
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,特征,answer2feature,,,,,,,,,,,
除了直接让随机森林选择特征，还有自行构造组合特征带入模型，是的randomForestsubspace变成randomForestcombination,特征,answer2feature,,,,,,,,,,,
要调整的参数主要是n_estimators和max_features,参数,answer2feature,,,,,,,,,,,
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,树,answer2feature,,,,,,,,,,,
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,树,answer2feature,,,,,,,,,,,
n_estimators是森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好,临界值,answer2feature,,,,,,,,,,,
max_features是分割节点时考虑的特征的随机子集的大小。这个值越低，方差减小得越多，但是偏差的增大也越多,特征,answer2feature,,,,,,,,,,,
回归：max_features=n_features,回归,answer2feature,,,,,,,,,,,
分类：max_features=sqrt(n_features),分类,answer2feature,,,,,,,,,,,
其他参数中,参数,answer2feature,,,,,,,,,,,
max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,修剪,answer2feature,,,,,,,,,,,
max_depth=None和min_samples_split=2结合，为不限制生成一个不修剪的完全树,树,answer2feature,,,,,,,,,,,
不同决策树可以由不同主机并行训练生成，效率很高,训练,answer2feature,,,,,,,,,,,
不同决策树可以由不同主机并行训练生成，效率很高,效率,answer2feature,,,,,,,,,,,
随机森林算法继承了CART的优点,随机森林,answer2feature,,,,,,,,,,,
随机森林算法继承了CART的优点,CART,answer2feature,,,,,,,,,,,
将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,bagging,answer2feature,,,,,,,,,,,
将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题,过拟合,answer2feature,,,,,,,,,,,
初始化训练一个弱学习器，初始化下的各条样本的权重一致,初始化,answer2feature,,,,,,,,,,,
初始化训练一个弱学习器，初始化下的各条样本的权重一致,训练,answer2feature,,,,,,,,,,,
初始化训练一个弱学习器，初始化下的各条样本的权重一致,初始化,answer2feature,,,,,,,,,,,
基于调整后的样本及样本权重训练下一个弱学习器,训练,answer2feature,,,,,,,,,,,
预测时直接串联综合各学习器的加权结果,预测,answer2feature,,,,,,,,,,,
预测时直接串联综合各学习器的加权结果,串联,answer2feature,,,,,,,,,,,
预测时直接串联综合各学习器的加权结果,加权,answer2feature,,,,,,,,,,,
回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,回归,answer2feature,,,,,,,,,,,
回归树在每个切分后的结点上都会有一个预测值，这个预测值就是结点上所有值的均值,均值,answer2feature,,,,,,,,,,,
分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,分枝,answer2feature,,,,,,,,,,,
分枝时遍历所有的属性进行二叉划分，挑选使平方误差最小的划分属性作为本节点的划分属性,误差,answer2feature,,,,,,,,,,,
属性上有多个值，则需要遍历所有可能的属性值，挑选使平方误差最小的划分属性值作为本属性的划分值,误差,answer2feature,,,,,,,,,,,
递归重复以上步骤，直到满足叶子结点上值的要求,递归,answer2feature,,,,,,,,,,,
adaboost，gbdt等等,adaboost,answer2feature,,,,,,,,,,,
adaboost，gbdt等等,gbdt,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,boostingtree,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,拟合,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,mse,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,回归,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,损失,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,函数,answer2feature,,,,,,,,,,,
boostingtree利用基模型学习器，拟合的是mse（回归）或者指数损失函数（分类）,分类,answer2feature,,,,,,,,,,,
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,gbdt,answer2feature,,,,,,,,,,,
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,拟合,answer2feature,,,,,,,,,,,
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,损失,answer2feature,,,,,,,,,,,
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,函数,answer2feature,,,,,,,,,,,
gbdt利用基模型学习器，拟合的是当前模型与标签值的损失函数的负梯度,梯度,answer2feature,,,,,,,,,,,
Carttree，但是都是回归树,回归,answer2feature,,,,,,,,,,,
Carttree，但是都是回归树,树,answer2feature,,,,,,,,,,,
mse:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bmopzfjj303s011t8i.jpg),mse,answer2feature,,,,,,,,,,,
负梯度：yh(x),梯度,answer2feature,,,,,,,,,,,
初始模型F0由目标变量的平均值给出,目标,answer2feature,,,,,,,,,,,
初始模型F0由目标变量的平均值给出,平均值,answer2feature,,,,,,,,,,,
绝对损失:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94bne7cipj303400q742.jpg),损失,answer2feature,,,,,,,,,,,
负梯度：sign(yh(x)),梯度,answer2feature,,,,,,,,,,,
初始模型F0由目标变量的中值给出,目标,answer2feature,,,,,,,,,,,
Huber损失：mse和绝对损失的结合,损失,answer2feature,,,,,,,,,,,
Huber损失：mse和绝对损失的结合,mse,answer2feature,,,,,,,,,,,
Huber损失：mse和绝对损失的结合,损失,answer2feature,,,,,,,,,,,
负梯度：yh(x)和sign(yh(x))分段函数,梯度,answer2feature,,,,,,,,,,,
负梯度：yh(x)和sign(yh(x))分段函数,分段,answer2feature,,,,,,,,,,,
负梯度：yh(x)和sign(yh(x))分段函数,函数,answer2feature,,,,,,,,,,,
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,MSE,answer2feature,,,,,,,,,,,
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,损失,answer2feature,,,,,,,,,,,
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,损失,answer2feature,,,,,,,,,,,
它是MSE和绝对损失的组合形式，对于远离中心的异常点，采用绝对损失，其他的点采用MSE，这个界限一般用分位数点度量,MSE,answer2feature,,,,,,,,,,,
对数似然损失函数,损失,answer2feature,,,,,,,,,,,
对数似然损失函数,函数,answer2feature,,,,,,,,,,,
负梯度：y/(1+??????(?????(??))),梯度,answer2feature,,,,,,,,,,,
多元：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94cj1yce6j306x01iglg.jpg),多元,answer2feature,,,,,,,,,,,
"指数损失函数:??(??,??(??))=??????(?????(??))",损失,answer2feature,,,,,,,,,,,
"指数损失函数:??(??,??(??))=??????(?????(??))",函数,answer2feature,,,,,,,,,,,
负梯度：y・??????(?????(??)),梯度,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,梯度,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,梯度,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,拟合,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,搜索,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,多元,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,分类,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,分类,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,GBDT,answer2feature,,,,,,,,,,,
除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同,回归,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,函数,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,均方,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,误差,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,gbdt,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,梯度,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,boostingtree,answer2feature,,,,,,,,,,,
当loss函数为均方误差![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ajgymkuj303w011jr6.jpg)，gbdt中的残差的负梯度的结果yH(x)正好与boostingtree的拟合残差一致,拟合,answer2feature,,,,,,,,,,,
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",损失,answer2feature,,,,,,,,,,,
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",函数,answer2feature,,,,,,,,,,,
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",梯度,answer2feature,,,,,,,,,,,
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",回归,answer2feature,,,,,,,,,,,
"利用![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94afa5h1lj3060017mx0.jpg)可以计算得到x对应的损失函数的负梯度![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94ars964uj301h00ijr5.jpg),据此我们可以构造出第t棵回归树，其对应的叶子结点区域![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94atb0k7zj300p00i3y9.jpg)j为叶子结点位置",树,answer2feature,,,,,,,,,,,
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),回归,answer2feature,,,,,,,,,,,
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),树,answer2feature,,,,,,,,,,,
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),特征,answer2feature,,,,,,,,,,,
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),均方,answer2feature,,,,,,,,,,,
构建回归树的过程中，需要考虑找到特征A中最合适的切分点，使得切分后的数据集D1和D2的均方误差最小![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b0klia2j30ee017aa0.jpg),误差,answer2feature,,,,,,,,,,,
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值??????,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",损失,answer2feature,,,,,,,,,,,
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值??????,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",函数,answer2feature,,,,,,,,,,,
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值??????,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",拟合,answer2feature,,,,,,,,,,,
"针对每一个叶子节点里的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值??????,![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94b5ffnyoj308g0173yd.jpg)",输出,answer2feature,,,,,,,,,,,
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,feature,answer2feature,,,,,,,,,,,
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,损失,answer2feature,,,,,,,,,,,
首先，根据feature切分后的损失均方差大小，选取最优的特征切分,特征,answer2feature,,,,,,,,,,,
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,feature,answer2feature,,,,,,,,,,,
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,损失,answer2feature,,,,,,,,,,,
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,函数,answer2feature,,,,,,,,,,,
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,拟合,answer2feature,,,,,,,,,,,
其次，根据选定的feature切分后的叶子结点数据集，选取最使损失函数最小，也就是拟合叶子节点最好的输出值,输出,answer2feature,,,,,,,,,,,
本轮最终得到的强学习器的表达式如下：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94binx5prj307o01k0sl.jpg),表达式,answer2feature,,,,,,,,,,,
泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),泰勒展开,answer2feature,,,,,,,,,,,
泰勒展开的一阶形式：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94h6rkexqj305x00ijr7.jpg),一阶,answer2feature,,,,,,,,,,,
"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",泰勒展开,answer2feature,,,,,,,,,,,
"对![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hak9e26j303n00idfm.jpg)进行泰勒展开：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg),其中m1轮对残差梯度为![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hey2xksj3052017a9w.jpg)",梯度,answer2feature,,,,,,,,,,,
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",拟合,answer2feature,,,,,,,,,,,
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",梯度,answer2feature,,,,,,,,,,,
"我们拟合了残差的负梯度，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hi3epnaj302r00kmwx.jpg),所以![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94hd5sz8vj309400kglg.jpg)内会让损失向下降对方向前进",损失,answer2feature,,,,,,,,,,,
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（??）,梯度,answer2feature,,,,,,,,,,,
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（??）,梯度,answer2feature,,,,,,,,,,,
前者不用残差的负梯度而是使用残差，是全局最优值，后者使用的是局部最优方向（负梯度）*步长（??）,步长,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,优化,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,反映,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,优化,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,回归,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,梯度,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,损失,answer2feature,,,,,,,,,,,
依赖残差进行优化，损失函数一般固定为反映残差的均方差损失函数，因此当均方差损失函数失效（该损失函数对异常值敏感）的时候，换了其他一般的损失函数，便很难得到优化的结果。同时，因为损失函数的问题，BoostingTree也很难处理回归之外问题。而后者使用梯度下降的方法，对于任意可以求导的损失函数它都可以处理,函数,answer2feature,,,,,,,,,,,
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,逼近,answer2feature,,,,,,,,,,,
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,逼近,answer2feature,,,,,,,,,,,
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,树,answer2feature,,,,,,,,,,,
每次走一小步逐渐逼近结果的效果，要比每次迈一大步很快逼近结果的方式更容易得到精确值，即它不完全信任每一棵残差树，认为每棵树只学到了真理的一部分累加的时候只累加了一小部分多学几棵树来弥补不足。这个技巧类似于梯度下降里的学习率,梯度,answer2feature,,,,,,,,,,,
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,特征,answer2feature,,,,,,,,,,,
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,特征,answer2feature,,,,,,,,,,,
会，同时因为特征会进行多次使用，特征用的越多，则该特征的重要性越大,重要性,answer2feature,,,,,,,,,,,
每一棵树基于原始原本的一个子集进行训练,训练,answer2feature,,,,,,,,,,,
rf是有放回采样，gbdt是无放回采样,有放回,answer2feature,,,,,,,,,,,
rf是有放回采样，gbdt是无放回采样,gbdt,answer2feature,,,,,,,,,,,
rf是有放回采样，gbdt是无放回采样,无放回,answer2feature,,,,,,,,,,,
特征子采样可以来控制模型整体的方差,特征,answer2feature,,,,,,,,,,,
利用Shrinkage收缩，控制每一棵子树的贡献度,树,answer2feature,,,,,,,,,,,
每棵Cart树的枝剪,树,answer2feature,,,,,,,,,,,
对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,预处理,answer2feature,,,,,,,,,,,
对数据的要求比较低，不需要强假设，不需要数据预处理，连续离散都可以，缺失值也能接受,离散,answer2feature,,,,,,,,,,,
bagging，关注于提升分类器的泛化能力,bagging,answer2feature,,,,,,,,,,,
bagging，关注于提升分类器的泛化能力,泛化能力,answer2feature,,,,,,,,,,,
boosting，关注于提升分类器的精度,boosting,answer2feature,,,,,,,,,,,
数据要求比较低，不需要前提假设，能处理缺失值，连续值，离散值,离散,answer2feature,,,,,,,,,,,
使用一些健壮的损失函数，对异常值的鲁棒性非常强,损失,answer2feature,,,,,,,,,,,
使用一些健壮的损失函数，对异常值的鲁棒性非常强,函数,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,RF,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,迭代,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,训练,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,有放回,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,抽样,answer2feature,,,,,,,,,,,
RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本,GBDT,answer2feature,,,,,,,,,,,
gbdt对异常值比rf更加敏感,gbdt,answer2feature,,,,,,,,,,,
gbdt是串行，rf是并行,gbdt,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,gbdt,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,回归,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,树,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,分类,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,回归,answer2feature,,,,,,,,,,,
gbdt是cart回归树，rf是cart分类回归树都可以,树,answer2feature,,,,,,,,,,,
gbdt是提高降低偏差提高性能，rf是通过降低方差提高性能,gbdt,answer2feature,,,,,,,,,,,
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,gbdt,answer2feature,,,,,,,,,,,
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,输出,answer2feature,,,,,,,,,,,
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,加权,answer2feature,,,,,,,,,,,
gbdt对输出值是进行加权求和，rf对输出值是进行投票或者平均,输出,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,线性回归,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,逻辑,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,回归,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,GBDT,answer2feature,,,,,,,,,,,
从决策边界来说，线性回归的决策边界是一条直线，逻辑回归的决策边界是一条曲线，而GBDT的决策边界可能是很多条线,决策,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,高维,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,稀疏,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,特征,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,LR,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,GBDT,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,LR,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,参数,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,GBDT,answer2feature,,,,,,,,,,,
当在高维稀疏特征的场景下，LR的效果一般会比GBDT好。因为LR有参数惩罚，GBDT容易造成过拟合,过拟合,answer2feature,,,,,,,,,,,
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,迭代,answer2feature,,,,,,,,,,,
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature,,,,,,,,,,,
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature,,,,,,,,,,,
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature,,,,,,,,,,,
每轮迭代时，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间,训练,answer2feature,,,,,,,,,,,
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,排序,answer2feature,,,,,,,,,,,
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,特征值,answer2feature,,,,,,,,,,,
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,特征,answer2feature,,,,,,,,,,,
预排序方法需要保存特征值，及特征排序后的索引结果，占用空间,排序,answer2feature,,,,,,,,,,,
levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,训练,answer2feature,,,,,,,,,,,
levelwise，在训练的时候哪怕新增的分裂点对loss增益没有提升也会先达到预定的层数,增益,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,直方图,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,特征选择,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,直方图,answer2feature,,,,,,,,,,,
将连续的浮点特征离散成k个离散值，具体过程是首先确定对于每一个特征需要多少的桶bin，然后均分，将属于该桶的样本数据更新为bin的值，最后用直方图表示。在进行特征选择时，只需要根据直方图的离散值，遍历寻找最优的分割点,离散,answer2feature,,,,,,,,,,,
每次从当前所有叶子中找到分裂增益最大（一般也是数据量最大）的一个叶子，然后分裂，如此循环,增益,answer2feature,,,,,,,,,,,
直方图做差加速,直方图,answer2feature,,,,,,,,,,,
直方图做差加速,加速,answer2feature,,,,,,,,,,,
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,梯度,answer2feature,,,,,,,,,,,
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,梯度,answer2feature,,,,,,,,,,,
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,损失,answer2feature,,,,,,,,,,,
单边梯度采样GradientbasedOneSideSampling(GOSS)：排除**大部分**小梯度的样本，仅用剩下的样本计算损失增益,增益,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",互斥,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",稀疏,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",互斥,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
"互斥稀疏特征绑定ExclusiveFeatureBundling(EFB)：从减少特征角度，把尽可能互斥的特征进行合并，比如特征A\[0,10],特征B\[0,20],可以把B+10后与A合并，得到新特征A+B\[0,30]",特征,answer2feature,,,,,,,,,,,
显示的把树模型复杂度作为正则项加到优化目标中,树,answer2feature,,,,,,,,,,,
显示的把树模型复杂度作为正则项加到优化目标中,复杂度,answer2feature,,,,,,,,,,,
显示的把树模型复杂度作为正则项加到优化目标中,优化,answer2feature,,,,,,,,,,,
显示的把树模型复杂度作为正则项加到优化目标中,目标,answer2feature,,,,,,,,,,,
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,优化,answer2feature,,,,,,,,,,,
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,目标,answer2feature,,,,,,,,,,,
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,泰勒展开,answer2feature,,,,,,,,,,,
优化目标计算中用到二阶泰勒展开代替一阶，更加准确,一阶,answer2feature,,,,,,,,,,,
近似算法（分桶）,分桶,answer2feature,,,,,,,,,,,
更加高效和快速,快速,answer2feature,,,,,,,,,,,
数据事先排序并且以block形式存储，有利于并行计算,排序,answer2feature,,,,,,,,,,,
数据事先排序并且以block形式存储，有利于并行计算,block,answer2feature,,,,,,,,,,,
基于分布式通信框架rabit，可以运行在MPI和yarn上,分布式,answer2feature,,,,,,,,,,,
实现做了面向体系结构的优化，针对cache和内存做了性能优化,优化,answer2feature,,,,,,,,,,,
实现做了面向体系结构的优化，针对cache和内存做了性能优化,优化,answer2feature,,,,,,,,,,,
模型优化上：,优化,answer2feature,,,,,,,,,,,
基模型的优化：,优化,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),gbdt,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),树,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),xgboost,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),L1,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),L2,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),逻辑,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),分类,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),线性回归,answer2feature,,,,,,,,,,,
gbdt用的是cart回归树作为基模型，xgboost还可以用线性模型，加上天生的正则项，就是带L1和L2逻辑回归(分类)和线性回归(回归),回归,answer2feature,,,,,,,,,,,
损失函数上的优化：,损失,answer2feature,,,,,,,,,,,
损失函数上的优化：,函数,answer2feature,,,,,,,,,,,
损失函数上的优化：,优化,answer2feature,,,,,,,,,,,
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,gbdt,answer2feature,,,,,,,,,,,
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,一阶,answer2feature,,,,,,,,,,,
gbdt对loss是泰勒一阶展开，xgboost是泰勒二阶展开,xgboost,answer2feature,,,,,,,,,,,
gbdt没有在loss中带入结点个数和预测值的正则项,gbdt,answer2feature,,,,,,,,,,,
特征选择上的优化：,特征选择,answer2feature,,,,,,,,,,,
特征选择上的优化：,优化,answer2feature,,,,,,,,,,,
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,加速,answer2feature,,,,,,,,,,,
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,gbdt,answer2feature,,,,,,,,,,,
实现了一种分裂节点寻找的近似算法，用于加速和减小内存消耗，而不是gbdt的暴力搜索,搜索,answer2feature,,,,,,,,,,,
节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,gbdt,answer2feature,,,,,,,,,,,
节点分裂算法解决了缺失值方向的问题，gbdt则是沿用了cart的方法进行加权,加权,answer2feature,,,,,,,,,,,
正则化的优化：,优化,answer2feature,,,,,,,,,,,
特征采样,特征,answer2feature,,,,,,,,,,,
工程优化上：,优化,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,xgboost,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,block,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,排序,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,增益,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,特征,answer2feature,,,,,,,,,,,
xgboost在对特征进行了分block预排序，使得在做特征分裂的时候，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行,增益,answer2feature,,,,,,,,,,,
支持分布式计算可以运行在MPI，YARN上，得益于底层支持容错的分布式通信框架rabit,分布式,answer2feature,,,,,,,,,,,
![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mnp3fd7j301700idfl.jpg)为泰勒一阶展开，![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mrtqxv0j30480173yc.jpg),一阶,answer2feature,,,,,,,,,,,
MAE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mxhhvg8j303l011q2q.jpg),MAE,answer2feature,,,,,,,,,,,
MAPE:![](https://tva1.sinaimg.cn/large/006y8mN6gy1g94mx7uyuej303f0170sj.jpg),MAPE,answer2feature,,,,,,,,,,,
利用可导的函数逼近MAE或MAPE,函数,answer2feature,,,,,,,,,,,
利用可导的函数逼近MAE或MAPE,逼近,answer2feature,,,,,,,,,,,
利用可导的函数逼近MAE或MAPE,MAE,answer2feature,,,,,,,,,,,
利用可导的函数逼近MAE或MAPE,MAPE,answer2feature,,,,,,,,,,,
mse,mse,answer2feature,,,,,,,,,,,
法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,特征,answer2feature,,,,,,,,,,,
法尝试所有特征和所有分裂位置，从而求得最优分裂点。当样本太大且特征为连续值时，这种暴力做法的计算量太大,特征,answer2feature,,,,,,,,,,,
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征值,answer2feature,,,,,,,,,,,
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征值,answer2feature,,,,,,,,,,,
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,统计,answer2feature,,,,,,,,,,,
近似算法寻找最优分裂点时不会枚举所有的特征值，而是对特征值进行聚合统计，然后形成若干个桶。然后仅仅将桶边界上的特征的值作为分裂点的候选，从而获取计算性能的提升,特征,answer2feature,,,,,,,,,,,
离散值直接分桶,离散,answer2feature,,,,,,,,,,,
离散值直接分桶,分桶,answer2feature,,,,,,,,,,,
连续值分位数分桶,分桶,answer2feature,,,,,,,,,,,
训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,训练,answer2feature,,,,,,,,,,,
训练时：缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个,损失,answer2feature,,,,,,,,,,,
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,预测,answer2feature,,,,,,,,,,,
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,训练,answer2feature,,,,,,,,,,,
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,预测,answer2feature,,,,,,,,,,,
预测时：如果训练中没有数据缺失，预测时出现了数据缺失，那么默认被分类到右子树,分类,answer2feature,,,,,,,,,,,
特征预排序,特征,answer2feature,,,,,,,,,,,
特征预排序,排序,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,特征,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,排序,answer2feature,,,,,,,,,,,
按特征进行存储，每一个block代表一个特征的值，样本在该block中按照它在该特征的值排好序。这些block只需要在程序开始的时候计算一次，后续排序只需要线性扫描这些block即可,block,answer2feature,,,,,,,,,,,
block可以仅存放样本的索引，而不是样本本身，这样节省了大量的存储空间,block,answer2feature,,,,,,,,,,,
’weight‘：代表着某个特征被选作分裂结点的次数；,特征,answer2feature,,,,,,,,,,,
’gain‘：使用该特征作为分类结点的信息增益；,特征,answer2feature,,,,,,,,,,,
’gain‘：使用该特征作为分类结点的信息增益；,分类,answer2feature,,,,,,,,,,,
’gain‘：使用该特征作为分类结点的信息增益；,增益,answer2feature,,,,,,,,,,,
’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,特征,answer2feature,,,,,,,,,,,
’cover‘：某特征作为划分结点，覆盖样本总数的平均值；,平均值,answer2feature,,,,,,,,,,,
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,目标,answer2feature,,,,,,,,,,,
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,函数,answer2feature,,,,,,,,,,,
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,树,answer2feature,,,,,,,,,,,
在目标函数中增加了正则项：叶子结点树+叶子结点权重的L2模的平方,L2,answer2feature,,,,,,,,,,,
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,阈值,answer2feature,,,,,,,,,,,
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,目标,answer2feature,,,,,,,,,,,
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,函数,answer2feature,,,,,,,,,,,
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,增益,answer2feature,,,,,,,,,,,
在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂,阈值,answer2feature,,,,,,,,,,,
当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂,阈值,answer2feature,,,,,,,,,,,
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,XGBoost,answer2feature,,,,,,,,,,,
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,树,answer2feature,,,,,,,,,,,
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,深度,answer2feature,,,,,,,,,,,
XGBoost先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝,剪枝,answer2feature,,,,,,,,,,,
降低树的深度,树,answer2feature,,,,,,,,,,,
降低树的深度,深度,answer2feature,,,,,,,,,,,
减小learningrate，提高estimator,learningrate,answer2feature,,,,,,,,,,,
减小learningrate，提高estimator,estimator,answer2feature,,,,,,,,,,,
先确定learningrate和estimator,learningrate,answer2feature,,,,,,,,,,,
先确定learningrate和estimator,estimator,answer2feature,,,,,,,,,,,
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,增益,answer2feature,,,,,,,,,,,
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,参数,answer2feature,,,,,,,,,,,
再确定全局信息：比如最小分裂增益，子采样参数，正则参数,参数,answer2feature,,,,,,,,,,,
重新降低learningrate，得到最优解,learningrate,answer2feature,,,,,,,,,,,
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,RNN,answer2feature,,,,,,,,,,,
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,RNN,answer2feature,,,,,,,,,,,
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,决策,answer2feature,,,,,,,,,,,
对比RNN的是，RNN是基于马尔可夫决策过程，决策链路太短，且单向,决策,answer2feature,,,,,,,,,,,
对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,CNN,answer2feature,,,,,,,,,,,
对比CNN的是，CNN基于的是窗口式捕捉，没有受限于窗口大小，局部信息获取，且无序,CNN,answer2feature,,,,,,,,,,,
首先，我们可以理解为Attention把input重新进行了一轮编码，获得一个新的序列,Attention,answer2feature,,,,,,,,,,,
除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,平衡,answer2feature,,,,,,,,,,,
除以![](https://tva1.sinaimg.cn/large/006y8mN6ly1g98734eqn7j300y00na9t.jpg)的目的是为了平衡qk的值，避免softmax之后过小,softmax,answer2feature,,,,,,,,,,,
qk除了点击还可以直接拼接再内接一个参数变量等等,参数,answer2feature,,,,,,,,,,,
MultiAttention只是重复了h次的Attention，最后把结果进行拼接,Attention,answer2feature,,,,,,,,,,,
可以直接随机初始化,初始化,answer2feature,,,,,,,,,,,
也可以参考Google的sin/cos位置初始化方法,初始化,answer2feature,,,,,,,,,,,
Q：指的是query，相当于decoder的内容,decoder,answer2feature,,,,,,,,,,,
K：指的是key，相当于encoder的内容,encoder,answer2feature,,,,,,,,,,,
V：指的是value，相当于encoder的内容,encoder,answer2feature,,,,,,,,,,,
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,解码,answer2feature,,,,,,,,,,,
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,注意力,answer2feature,,,,,,,,,,,
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,encoder,answer2feature,,,,,,,,,,,
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,attention,answer2feature,,,,,,,,,,,
q和k对齐了解码端和编码端的信息相似度，相似度的值进行归一化后会生成对齐概率值（注意力值）。V对应的是encoder的内容，刚说了attention是对encoder对重编码，qk完成权重重新计算，v复制重编码,encoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,seq2seq,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Encoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,向量,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,预测,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,token,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,损失,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Encoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,向量,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature,,,,,,,,,,,
seq2seq最大的问题在于将Encoder端的所有信息压缩到一个固定长度的向量中，并将其作为Decoder端首个隐藏状态的输入，来预测Decoder端第一个单词(token)的隐藏状态。在输入序列比较长的时候，这样做显然会损失Encoder端的很多信息，而且这样一股脑的把该固定向量送入Decoder端，Decoder端不能够关注到其想要关注的信息,Decoder,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,目标,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,目标,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,embedding,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,FFN,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,Transformer,answer2feature,,,,,,,,,,,
selfattention让源序列和目标序列首先“自关联”起来，这样的话，源序列和目标序列自身的embedding表示所蕴含的信息更加丰富，而且后续的FFN层也增强了模型的表达能力，并且Transformer并行计算的能力是远远超过seq2seq系列的模型,seq2seq,answer2feature,,,,,,,,,,,
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,向量,answer2feature,,,,,,,,,,,
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,均值,answer2feature,,,,,,,,,,,
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,点积,answer2feature,,,,,,,,,,,
假设向量q和k的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积qk的均值是0，方差是dk,均值,answer2feature,,,,,,,,,,,
"针对Q和K中的每一维i都有qi和ki相互独立且均值0方差1，不妨记![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envjoy8oj301h00gdfl.jpg),![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9envuw3p0j301g00ga9t.jpg)",均值,answer2feature,,,,,,,,,,,
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,维度,answer2feature,,,,,,,,,,,
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,均值,answer2feature,,,,,,,,,,,
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,维度,answer2feature,,,,,,,,,,,
所以k维度上的qk方差会为dk，均值为0，用维度的根号来放缩，使得标准化,标准化,answer2feature,,,,,,,,,,,
按batch进行期望和标准差计算,batch,answer2feature,,,,,,,,,,,
按batch进行期望和标准差计算,期望,answer2feature,,,,,,,,,,,
对整体数据进行标准化,标准化,answer2feature,,,,,,,,,,,
对标准化的数据进行线性变换,标准化,answer2feature,,,,,,,,,,,
out=alhpa*(XXmu)/np.sqrt(var+eps)+beta,beta,answer2feature,,,,,,,,,,,
和bn过程近似，只是作用的方向是在维度上，而不是batch上,bn,answer2feature,,,,,,,,,,,
和bn过程近似，只是作用的方向是在维度上，而不是batch上,维度,answer2feature,,,,,,,,,,,
和bn过程近似，只是作用的方向是在维度上，而不是batch上,batch,answer2feature,,,,,,,,,,,
这样做的好处就是不会受到batch大小不一致的影响,batch,answer2feature,,,,,,,,,,,
常见结构，CV里面用的比较多,CV,answer2feature,,,,,,,,,,,
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,求偏,answer2feature,,,,,,,,,,,
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,导数,answer2feature,,,,,,,,,,,
虽然是对![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is8rtnxuj300l00k3y9.jpg)求偏导数，但是存在一项只和![](https://tva1.sinaimg.cn/large/006tNbRwly1g9is984bi0j300l00k3y9.jpg)相关的项，之间避免了何中间权重矩阵变换导致梯度消失的问题,梯度,answer2feature,,,,,,,,,,,
防止梯度消失,梯度,answer2feature,,,,,,,,,,,
对输出的变化更敏感,输出,answer2feature,,,,,,,,,,,
我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,xdeepfm,answer2feature,,,,,,,,,,,
我在做xdeepfm的输出层的时候做到了，因为当时做CIN的时候，我设置了layers为5层，担心层数过深造成网络退化，在output的时候加了残差网络,输出,answer2feature,,,,,,,,,,,
Bert和Transform中attention部分残差网络用的比较频繁,Bert,answer2feature,,,,,,,,,,,
Bert和Transform中attention部分残差网络用的比较频繁,Transform,answer2feature,,,,,,,,,,,
Bert和Transform中attention部分残差网络用的比较频繁,attention,answer2feature,,,,,,,,,,,
mask+attention，mask的word结合全部其他encoderword的信息,mask,answer2feature,,,,,,,,,,,
mask+attention，mask的word结合全部其他encoderword的信息,attention,answer2feature,,,,,,,,,,,
mask+attention，mask的word结合全部其他encoderword的信息,mask,answer2feature,,,,,,,,,,,
mask+attention，mask的word结合全部其他encoderword的信息,word,answer2feature,,,,,,,,,,,
MLM：将完整句子中的部分字mask，预测该mask词,mask,answer2feature,,,,,,,,,,,
MLM：将完整句子中的部分字mask，预测该mask词,预测,answer2feature,,,,,,,,,,,
MLM：将完整句子中的部分字mask，预测该mask词,mask,answer2feature,,,,,,,,,,,
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,训练,answer2feature,,,,,,,,,,,
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,语料库,answer2feature,,,,,,,,,,,
NSP：为每个训练前的例子选择句子A和B时，50%的情况下B是真的在A后面的下一个句子，50%的情况下是来自语料库的随机句子，进行二分预测是否为真实下一句,预测,answer2feature,,,,,,,,,,,
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature,,,,,,,,,,,
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature,,,,,,,,,,,
mask只会出现在构造句子中，当真实场景下是不会出现mask的，全mask不match句型了,mask,answer2feature,,,,,,,,,,,
随机替换也帮助训练修正了\[unused]和\[UNK],训练,answer2feature,,,,,,,,,,,
input_id是语义表达，和传统的w2v一样，方法也一样的lookup,w2v,answer2feature,,,,,,,,,,,
input_id是语义表达，和传统的w2v一样，方法也一样的lookup,lookup,answer2feature,,,,,,,,,,,
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",BERT,answer2feature,,,,,,,,,,,
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",向量,answer2feature,,,,,,,,,,,
"segment_id是辅助BERT区别句子对中的两个句子的向量表示，从\[1,embedding_size]里面lookup",lookup,answer2feature,,,,,,,,,,,
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",有序,answer2feature,,,,,,,,,,,
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",词袋模型,answer2feature,,,,,,,,,,,
"position_id是为了获取文本天生的有序信息，否则就和传统词袋模型一样了，从\[511,embedding_size]里面lookup",lookup,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",encoder,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",输出,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",分类,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",输出,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",向量,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",维度,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",softmax,answer2feature,,,,,,,,,,,
"MLM:在encoder的输出上添加一个分类层,用嵌入矩阵乘以输出向量，将其转换为词汇的维度,用softmax计算mask中每个单词的概率",mask,answer2feature,,,,,,,,,,,
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",分类,answer2feature,,,,,,,,,,,
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",输出,answer2feature,,,,,,,,,,,
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",向量,answer2feature,,,,,,,,,,,
"NSP:用一个简单的分类层将\[CLS]标记的输出变换为2×1形状的向量,用softmax计算IsNextSequence的概率",softmax,answer2feature,,,,,,,,,,,
MLM+NSP即为最后的损失,损失,answer2feature,,,,,,,,,,,
"tf.multal(tf.nn.softmax(tf.multiply(tf.multal(q,k,transpose_b=True),1/math.sqrt(float(size_per_head)))),v)",softmax,answer2feature,,,,,,,,,,,
headonly：保存前510个token（留两个位置给\[CLS]和\[SEP]）,token,answer2feature,,,,,,,,,,,
tailonly：保存最后510个token,token,answer2feature,,,,,,,,,,,
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature,,,,,,,,,,,
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature,,,,,,,,,,,
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature,,,,,,,,,,,
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,token,answer2feature,,,,,,,,,,,
head+tail：选择前128个token和最后382个token（文本在800以内）或者前256个token+后254个token（文本大于800tokens）,tokens,answer2feature,,,,,,,,,,,
"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",tokens,answer2feature,,,,,,,,,,,
"首先定义处理好输入的tokens的对应的id作为input_id,因为不是训练所以input_mask和segment_id都是采取默认的1即可",训练,answer2feature,,,,,,,,,,,
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,Transform,answer2feature,,,,,,,,,,,
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,transformer,answer2feature,,,,,,,,,,,
进入Transform模块，后循环调用transformer的前向过程，次数为隐藏层个数，每次前向过程都包含self_attention_layer、add_and_norm、feed_forward和add_and_norm四个步骤,隐藏层,answer2feature,,,,,,,,,,,
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],输出,answer2feature,,,,,,,,,,,
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature,,,,,,,,,,,
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature,,,,,,,,,,,
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],输出,answer2feature,,,,,,,,,,,
输出结果为句向量则取\[cls]对应的向量（需要处理成embedding_size），否则也可以取最后一层的输出作为每个词的向量组合all_encoder_layers\[1],向量,answer2feature,,,,,,,,,,,
BasicTokenizer：根据空格等进行普通的分词,BasicTokenizer,answer2feature,,,,,,,,,,,
BasicTokenizer：根据空格等进行普通的分词,分词,answer2feature,,,,,,,,,,,
包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,预处理,answer2feature,,,,,,,,,,,
包括了一些预处理的方法：去除无意义词，跳过'\t'这些词，unicode变换，中文字符筛选等等,筛选,answer2feature,,,,,,,,,,,
WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,WordpieceTokenizer,answer2feature,,,,,,,,,,,
WordpieceTokenizer：前者的结果再细粒度的切分为WordPiece,WordPiece,answer2feature,,,,,,,,,,,
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,token,answer2feature,,,,,,,,,,,
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,输出,answer2feature,,,,,,,,,,,
get_sentence_out代表了这个获取每个token的output输出，用的是cls向量,向量,answer2feature,,,,,,,,,,,
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,Transform,answer2feature,,,,,,,,,,,
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,dense,answer2feature,,,,,,,,,,,
Transform模块中：在残差连接之前，对output_layer进行了dense+dropout后再合并input_layer进行的layer_norm得到的attention_output,dropout,answer2feature,,,,,,,,,,,
所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,dense,answer2feature,,,,,,,,,,,
所有attention_output得到并合并后，也是先进行了全连接，而后再进行了dense+dropout再合并的attention_output之后才进行layer_norm得到最终的layer_output,dropout,answer2feature,,,,,,,,,,,
残差结构能够很好的消除层数加深所带来的信息损失问题,损失,answer2feature,,,,,,,,,,,
在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,预测,answer2feature,,,,,,,,,,,
在线预测只能一条一条的入参，实际上在可承受的计算量内batch越大整体的计算性能性价比越高,batch,answer2feature,,,,,,,,,,,
mask机制,mask,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,elmo,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,GPT,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,bert,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,elmo,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,静态,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,向量,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,LSTM,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,GPT,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,bert,answer2feature,,,,,,,,,,,
特征提取器：elmo采用LSTM进行提取，GPT和bert则采用Transformer进行提取。很多任务表明Transformer特征提取能力强于LSTM，elmo采用1层静态向量+2层LSTM，多层提取能力有限，而GPT和bert中的Transformer可采用多层，并行计算能力强。,Transformer,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,GPT,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,elmo,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,bert,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,elmo,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,融合,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,特征,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,bert,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,融合,answer2feature,,,,,,,,,,,
单/双向语言模型：GPT采用单向语言模型，elmo和bert采用双向语言模型。但是elmo实际上是两个单向语言模型（方向相反）的拼接，这种融合特征的能力比bert一体化融合特征方式弱。,特征,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,GPT,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,bert,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,Transformer,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,Transformer,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,GPT,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,decoder,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,decoder,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,bert,answer2feature,,,,,,,,,,,
GPT和bert都采用Transformer，Transformer是encoderdecoder结构，GPT的单向语言模型采用decoder部分，decoder的部分见到的都是不完整的句子；bert的双向语言模型则采用encoder部分，采用了完整句子,encoder,answer2feature,,,,,,,,,,,
则条件概率分布P(Y|X)为条件随机场CRF,条件随机场,answer2feature,,,,,,,,,,,
则条件概率分布P(Y|X)为条件随机场CRF,CRF,answer2feature,,,,,,,,,,,
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",CRF,answer2feature,,,,,,,,,,,
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",判别,answer2feature,,,,,,,,,,,
"CRF是判别模型求的是p(Y/X),HMM是生成模型求的是P(X,Y)",HMM,answer2feature,,,,,,,,,,,
CRF是无向图，HMM是有向图,CRF,answer2feature,,,,,,,,,,,
CRF是无向图，HMM是有向图,HMM,answer2feature,,,,,,,,,,,
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,CRF,answer2feature,,,,,,,,,,,
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,输出,answer2feature,,,,,,,,,,,
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,HMM,answer2feature,,,,,,,,,,,
CRF全局最优输出节点的条件概率，HMM对转移概率和表现概率直接建模，统计共现概率,统计,answer2feature,,,,,,,,,,,
Bert把中文文本进行了embedding，得到每个字的表征向量,Bert,answer2feature,,,,,,,,,,,
Bert把中文文本进行了embedding，得到每个字的表征向量,embedding,answer2feature,,,,,,,,,,,
Bert把中文文本进行了embedding，得到每个字的表征向量,向量,answer2feature,,,,,,,,,,,
dense操作得到了每个文本文本对应的未归一化的tag概率,dense,answer2feature,,,,,,,,,,,
CRF在选择每个词的tag的过程其实就是一个最优Tag路径的选择过程,CRF,answer2feature,,,,,,,,,,,
CRF层能从训练数据中获得约束性的规则,CRF,answer2feature,,,,,,,,,,,
CRF层能从训练数据中获得约束性的规则,训练,answer2feature,,,,,,,,,,,
"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",向量,answer2feature,,,,,,,,,,,
"其实，一句话解释就是想构造一个向量表征方式，使得向量的点击和共现矩阵中的对应关系一致。因为共现矩阵中的对应关系证明了，存在i，k，j三个不同的文本，如果i和k相关，j和k相关，那么p(i,j)=p(j,k)近似于1，其他情况都过大和过小。",向量,answer2feature,,,,,,,,,,,
公现矩阵，NXN的，N为词袋量,词袋,answer2feature,,,,,,,,,,,
W2V的工程实现结果相对来说支持的更多，比如most_similarty等功能,W2V,answer2feature,,,,,,,,,,,
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",词性,answer2feature,,,,,,,,,,,
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",训练,answer2feature,,,,,,,,,,,
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",词性,answer2feature,,,,,,,,,,,
"按照词性进行已知词替换，\[unknown],\[unknowa],\[unknowv]...，然后再进行训练。实际去用的时候，判断词性后直接使用对应的unknown?向量替代",向量,answer2feature,,,,,,,,,,,
从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),狄利克雷,answer2feature,,,,,,,,,,,
从狄利克雷分布α中取样生成文档i的主题分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg),主题,answer2feature,,,,,,,,,,,
多项式分布的共轭分布是狄利克雷分布,多项式,answer2feature,,,,,,,,,,,
多项式分布的共轭分布是狄利克雷分布,狄利克雷,answer2feature,,,,,,,,,,,
二项式分布的共轭分布是Beta分布,二项式,answer2feature,,,,,,,,,,,
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),主题,answer2feature,,,,,,,,,,,
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),多项式,answer2feature,,,,,,,,,,,
从主题的多项式![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vd6yi8j300c00f0qn.jpg)分布中取样生成文档i第j个词的主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg),主题,answer2feature,,,,,,,,,,,
从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),狄利克雷,answer2feature,,,,,,,,,,,
从狄利克雷分布β中取样生成主题![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8vq647gj300i00e0r3.jpg)对应的词语分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg),主题,answer2feature,,,,,,,,,,,
从词语的多项式分布![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8u1wbj1j300q00j3y9.jpg)中采样最终生成词语![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b8uydisuj300n00e0s0.jpg),多项式,answer2feature,,,,,,,,,,,
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,EM,answer2feature,,,,,,,,,,,
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,主题,answer2feature,,,,,,,,,,,
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,主题,answer2feature,,,,,,,,,,,
采用EM方法修正词主题矩阵+主题文档矩阵直至收敛,收敛,answer2feature,,,,,,,,,,,
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,LDA,answer2feature,,,,,,,,,,,
这个问题很难说清楚，一般会揪着细节问，不会在乎你的公式写的是不是完全一致。这部分是LDA的核心，是考验一个nlp工程师的最基础最基础的知识点,nlp,answer2feature,,,,,,,,,,,
"	先随机给每个词附上主题",主题,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",多项式,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",狄利克雷,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",狄利克雷,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",主题,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",多项式,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",狄利克雷,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",主题,answer2feature,,,,,,,,,,,
"	因为多项式分布的共轭分布是狄利克雷分布，可以根据狄利克雷分布先验分布结合每个词实际的主题满足的多项式分布得到后验狄利克雷分布分布，从而积分得到一文档的主题条件分布，词同理，从而得到每篇文章的主题和词的联合概率分布",主题,answer2feature,,,,,,,,,,,
"	有了联合概率分布，去除词wi后，就可以得到其他词主题条件概率分布",主题,answer2feature,,,,,,,,,,,
"	根据条件概率分布使用坐标轮换的吉布斯采样方法，得到词对应的平稳矩阵及词对应的主题",主题,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",收敛,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",统计,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",主题,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",主题,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",统计,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",主题,answer2feature,,,,,,,,,,,
"	收敛后统计文章的词对应的主题，得到文章的主题分布；统计词对应的主题，得到不同主题下词的分布",主题,answer2feature,,,,,,,,,,,
"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",特征,answer2feature,,,,,,,,,,,
"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",优化,answer2feature,,,,,,,,,,,
"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",特征,answer2feature,,,,,,,,,,,
"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",维度,answer2feature,,,,,,,,,,,
"	吉布斯采样是怎么做的？（基于MCMC思想，面对多维特征优化一维特征固定其他维度不变，满足细致平稳性，坐标变换以加快样本集生成速度）",加快,answer2feature,,,,,,,,,,,
"	MCMC中什么叫做蒙特卡洛方法？",蒙特卡洛,answer2feature,,,,,,,,,,,
"	马尔科夫链收敛性质？",收敛,answer2feature,,,,,,,,,,,
"		连通性，不能有断点",连通性,answer2feature,,,,,,,,,,,
"		连通性，不能有断点",断点,answer2feature,,,,,,,,,,,
"		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵",迭代,answer2feature,,,,,,,,,,,
"		先得到转移矩阵P在N次迭代下收敛到不变的平稳矩阵",收敛,answer2feature,,,,,,,,,,,
"			给定任意的转移矩阵Q，已知π(i)p(i,j)=π(j)p(j,i)，近似拟合π(i)Q(i,j)a(i,j)=π(j)Q(j,i)a(j,i)",拟合,answer2feature,,,,,,,,,,,
"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",加快,answer2feature,,,,,,,,,,,
"			左右同乘缩放，更新a(i,j)的计算公式，加快收敛速度",收敛,answer2feature,,,,,,,,,,,
"		Gibbs采样",Gibbs,answer2feature,,,,,,,,,,,
"			同上，差别在固定n?1个特征在某一个特征采样及坐标轮换采样",差别,answer2feature,,,,,,,,,,,
"			同上，差别在固定n?1个特征在某一个特征采样及坐标轮换采样",特征,answer2feature,,,,,,,,,,,
"			同上，差别在固定n?1个特征在某一个特征采样及坐标轮换采样",特征,answer2feature,,,,,,,,,,,
"		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路",回归,answer2feature,,,,,,,,,,,
"		其为一对样本，有点像Lasso回归中的固定n1维特征求一维特征求极值的思路",特征,answer2feature,,,,,,,,,,,
变分推断EM算法,EM,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",LDA,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",主题,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",主题,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",beta,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",期望,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",期望,answer2feature,,,,,,,,,,,
"	整体上过程是，LDA中存在隐藏变量主题分布，词分布，实际主题，和模型超参alpha，beta，需要E步求出隐藏变量基于条件概率的期望，在M步最大化这个期望，从而得到alpha，beta",beta,answer2feature,,,,,,,,,,,
"	变分推断在于隐藏变量没法直接求，**用三个独立分布的变分分步去拟合三个隐藏变量的条件分布**",拟合,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",kl,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",散度,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",最小化,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",KL,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",散度,answer2feature,,,,,,,,,,,
"		实际去做的时候，用的是kl散度衡量分布之间的相似度，最小化KL散度及相对熵",熵,answer2feature,,,,,,,,,,,
"	EM过程",EM,answer2feature,,,,,,,,,,,
"		E：最小化相对熵，偏导为0得到变分参数",最小化,answer2feature,,,,,,,,,,,
"		E：最小化相对熵，偏导为0得到变分参数",熵,answer2feature,,,,,,,,,,,
"		E：最小化相对熵，偏导为0得到变分参数",偏导,answer2feature,,,,,,,,,,,
"		E：最小化相对熵，偏导为0得到变分参数",参数,answer2feature,,,,,,,,,,,
"		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值",参数,answer2feature,,,,,,,,,,,
"		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值",梯度,answer2feature,,,,,,,,,,,
"		M：固定变分参数，梯度下降法，牛顿法得到alpha和beta的值",beta,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,狄利克雷,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,LDA,answer2feature,,,,,,,,,,,
以多项式分布狄利克雷分布为例，我们的多项式分布θ先验分布π(θ)，及加了多项式分布的样本信息x后的后验分布π(θ/x)都满足狄利克雷分布，则称狄利克雷分布为LDA场景下多项式分布的共轭分布,多项式,answer2feature,,,,,,,,,,,
LDA是加了狄利克雷先验的PLSA,LDA,answer2feature,,,,,,,,,,,
LDA是加了狄利克雷先验的PLSA,狄利克雷,answer2feature,,,,,,,,,,,
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,EM,answer2feature,,,,,,,,,,,
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,估计,answer2feature,,,,,,,,,,,
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,LDA,answer2feature,,,,,,,,,,,
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,狄利克雷,answer2feature,,,,,,,,,,,
PLSA的p(z/d)和p(w/z)都是直接EM估计的，而LDA都是通过狄利克雷给出的多项式分布参数估计出来的,多项式,answer2feature,,,,,,,,,,,
LDA是贝叶斯思想，PLSA是MLE,LDA,answer2feature,,,,,,,,,,,
LDA是贝叶斯思想，PLSA是MLE,MLE,answer2feature,,,,,,,,,,,
困惑度越小，越容易过拟合,过拟合,answer2feature,,,,,,,,,,,
某个词属于某个主题的困惑度：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b7zjns8uj305i012jr7.jpg)，某个文章的困惑度即为词的连乘：![](https://tva1.sinaimg.cn/large/006y8mN6gy1g9b83z3d22j304q01dweb.jpg),主题,answer2feature,,,,,,,,,,,
LDA比较是doc，word2vec是词,LDA,answer2feature,,,,,,,,,,,
LDA比较是doc，word2vec是词,word2vec,answer2feature,,,,,,,,,,,
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,LDA,answer2feature,,,,,,,,,,,
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,主题,answer2feature,,,,,,,,,,,
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,Word2Vec,answer2feature,,,,,,,,,,,
LDA是生成的每篇文章对k个主题对概率分布，Word2Vec生成的是每个词的特征表示,特征,answer2feature,,,,,,,,,,,
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,LDA,answer2feature,,,,,,,,,,,
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,主题,answer2feature,,,,,,,,,,,
LDA的文章之间的联系是主题，Word2Vec的词之间的联系是词本身的信息,Word2Vec,answer2feature,,,,,,,,,,,
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,LDA,answer2feature,,,,,,,,,,,
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,word,answer2feature,,,,,,,,,,,
LDA依赖的是doc和word共现得到的结果，Word2Vec依赖的是文本上下文得到的结果,Word2Vec,answer2feature,,,,,,,,,,,
通常alpha为1/k，k为类别数，beta一般为0.01,beta,answer2feature,,,,,,,,,,,
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature,,,,,,,,,,,
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature,,,,,,,,,,,
alpha越小，文档属于某一个主题的概率很大，接近于1，属于其他主题的概率就很小，文章的主题比较明确,主题,answer2feature,,,,,,,,,,,
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,beta,answer2feature,,,,,,,,,,,
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,beta,answer2feature,,,,,,,,,,,
beta同理，但是一般不会刻意去改beta，主要是压缩alpha到一定小的程度,压缩,answer2feature,,,,,,,,,,,
chucksize大一些更新的过程比较平稳，收敛更加平稳,收敛,answer2feature,,,,,,,,,,,
迭代次数一般不超过2000次，200万doc大约在2300次收敛,迭代,answer2feature,,,,,,,,,,,
迭代次数一般不超过2000次，200万doc大约在2300次收敛,收敛,answer2feature,,,,,,,,,,,
层次softmax,softmax,answer2feature,,,,,,,,,,,
负采样,负采样,answer2feature,,,,,,,,,,,
最大化对数似然函数,函数,answer2feature,,,,,,,,,,,
输入层：是上下文的词语的词向量,向量,answer2feature,,,,,,,,,,,
投影层：对其求和，所谓求和，就是简单的向量加法,向量,answer2feature,,,,,,,,,,,
输出层：输出最可能的word,输出,answer2feature,,,,,,,,,,,
输出层：输出最可能的word,输出,answer2feature,,,,,,,,,,,
输出层：输出最可能的word,word,answer2feature,,,,,,,,,,,
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,树,answer2feature,,,,,,,,,,,
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,logistics,answer2feature,,,,,,,,,,,
沿着哈夫曼树找到对应词，每一次节点选择就是一次logistics选择过程，连乘即为似然函数,函数,answer2feature,,,,,,,,,,,
对每层每个变量求偏导，参考sgd,求偏,answer2feature,,,,,,,,,,,
对每层每个变量求偏导，参考sgd,sgd,answer2feature,,,,,,,,,,,
统计每个词出现对概率，丢弃词频过低对词,统计,answer2feature,,,,,,,,,,,
统计每个词出现对概率，丢弃词频过低对词,词频,answer2feature,,,,,,,,,,,
每次选择softmax的负样本的时候，从丢弃之后的词库里选择（选择是需要参考出现概率的）,softmax,answer2feature,,,,,,,,,,,
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,负采样,answer2feature,,,,,,,,,,,
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,负采样,answer2feature,,,,,,,,,,,
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,输出,answer2feature,,,,,,,,,,,
负采样的核心思想是：利用负采样后的输出分布来模拟真实的输出分布,输出,answer2feature,,,,,,,,,,,
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,Mikolov,answer2feature,,,,,,,,,,,
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,CBOW,answer2feature,,,,,,,,,,,
**Mikolov的原论文，Skipgram在处理少量数据时效果很好，可以很好地表示低频单词。而CBOW的学习速度更快，对高频单词有更好的表示**,高频,answer2feature,,,,,,,,,,,
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",复杂度,answer2feature,,,,,,,,,,,
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",CBOW,answer2feature,,,,,,,,,,,
"Skipgram的时间复杂度是o(kv),CBOW的时间复杂度o(v)",复杂度,answer2feature,,,,,,,,,,,
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,item2vec,answer2feature,,,,,,,,,,,
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,向量,answer2feature,,,,,,,,,,,
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,判别,answer2feature,,,,,,,,,,,
从item2vec得到的词向量中随机抽出一部分进行人工判别可靠性。即人工判断各维度item与标签item的相关程度，判断是否合理，序列是否相关,维度,answer2feature,,,,,,,,,,,
对item2vec得到的词向量进行聚类或者可视化,item2vec,answer2feature,,,,,,,,,,,
对item2vec得到的词向量进行聚类或者可视化,向量,answer2feature,,,,,,,,,,,
对item2vec得到的词向量进行聚类或者可视化,聚类,answer2feature,,,,,,,,,,,
word2vec是基于邻近词共现，glove是基于全文共现,word2vec,answer2feature,,,,,,,,,,,
word2vec是基于邻近词共现，glove是基于全文共现,glove,answer2feature,,,,,,,,,,,
word2vec利用了负采样或者层次softmax加速，相对更快,word2vec,answer2feature,,,,,,,,,,,
word2vec利用了负采样或者层次softmax加速，相对更快,负采样,answer2feature,,,,,,,,,,,
word2vec利用了负采样或者层次softmax加速，相对更快,softmax,answer2feature,,,,,,,,,,,
word2vec利用了负采样或者层次softmax加速，相对更快,加速,answer2feature,,,,,,,,,,,
glove用了全局共现矩阵，更占内存资源,glove,answer2feature,,,,,,,,,,,
word2vec是“predictive”的模型，而GloVe是“countbased”的模型,word2vec,answer2feature,,,,,,,,,,,
对于中文依赖分词结果的好坏,分词,answer2feature,,,,,,,,,,,
基础概念,AutoML,root2first,,,,,,,,,,,
AutoML,AutoML问题构成?,second2question,,,,,,,,,,,
AutoML,特征工程选择思路？,second2question,,,,,,,,,,,
AutoML,模型相关的选择思路?,second2question,,,,,,,,,,,
AutoML,常见优化算法思路？,second2question,,,,,,,,,,,
AutoML,AutoML参数选择所使用的方法？,second2question,,,,,,,,,,,
AutoML,讲讲贝叶斯优化如何在automl上应用？,second2question,,,,,,,,,,,
AutoML,以高斯过程为例，超参搜索的f的最优解求解acquisition function有哪些？,second2question,,,,,,,,,,,
基础概念,先验概率和后验概率,root2first,,,,,,,,,,,
先验概率和后验概率,写出全概率公式&贝叶斯公式,second2question,,,,,,,,,,,
先验概率和后验概率,说说你怎么理解为什么有全概率公式&贝叶斯公式,second2question,,,,,,,,,,,
先验概率和后验概率,什么是先验概率,second2question,,,,,,,,,,,
先验概率和后验概率,什么是后验概率,second2question,,,,,,,,,,,
先验概率和后验概率,经典概率题,second2question,,,,,,,,,,,
基础概念,方差与偏差,root2first,,,,,,,,,,,
方差与偏差,解释方差：,second2question,,,,,,,,,,,
方差与偏差,解释偏差：,second2question,,,,,,,,,,,
方差与偏差,模型训练为什么要引入偏差和方差？请理论论证。,second2question,,,,,,,,,,,
方差与偏差,什么情况下引发高方差？,second2question,,,,,,,,,,,
方差与偏差,如何解决高方差问题？,second2question,,,,,,,,,,,
方差与偏差,以上方法是否一定有效？,second2question,,,,,,,,,,,
方差与偏差,如何解决高偏差问题？,second2question,,,,,,,,,,,
方差与偏差,以上方法是否一定有效？,second2question,,,,,,,,,,,
方差与偏差,遇到过的机器学习中的偏差与方差问题？,second2question,,,,,,,,,,,
方差与偏差,就理论角度论证Bagging、Boosting的方差偏差问题,second2question,,,,,,,,,,,
方差与偏差,遇到过的深度学习中的偏差与方差问题？,second2question,,,,,,,,,,,
方差与偏差,方差、偏差与模型的复杂度之间的关系？,second2question,,,,,,,,,,,
基础概念,生成与判别模型,root2first,,,,,,,,,,,
生成与判别模型,什么叫生成模型？,second2question,,,,,,,,,,,
生成与判别模型,什么叫判别模型？,second2question,,,,,,,,,,,
生成与判别模型,什么时候会选择生成/判别模型？,second2question,,,,,,,,,,,
生成与判别模型,CRF/朴素贝叶斯/EM/最大熵模型/马尔科夫随机场/混合高斯模型各自是什么模型？,second2question,,,,,,,,,,,
生成与判别模型,我的理解：,second2question,,,,,,,,,,,
基础概念,频率概率,root2first,,,,,,,,,,,
频率概率,极大似然估计 - MLE,second2question,,,,,,,,,,,
频率概率,最大后验估计 - MAP,second2question,,,,,,,,,,,
频率概率,极大似然估计与最大后验概率的区别？,second2question,,,,,,,,,,,
频率概率,到底什么是似然什么是概率估计？,second2question,,,,,,,,,,,
推荐,DeepFM,root2first,,,,,,,,,,,
DeepFM,DNN与DeepFM之间的区别?,second2question,,,,,,,,,,,
DeepFM,Wide&Deep与DeepFM之间的区别?,second2question,,,,,,,,,,,
DeepFM,你在使用deepFM的时候是如何处理欠拟合和过拟合问题的？,second2question,,,,,,,,,,,
DeepFM,DeepFM怎么优化的？,second2question,,,,,,,,,,,
DeepFM,不定长文本数据如何输入deepFM？,second2question,,,,,,,,,,,
DeepFM,deepfm的embedding初始化有什么值得注意的地方吗？,second2question,,,,,,,,,,,
推荐,DIN,root2first,,,,,,,,,,,
DIN,主要使用了什么机制?,second2question,,,,,,,,,,,
DIN,activation unit的作用,second2question,,,,,,,,,,,
DIN,DICE怎么设计的,second2question,,,,,,,,,,,
DIN,DICE使用的过程中，有什么需要注意的地方,second2question,,,,,,,,,,,
推荐,XDeepFM,root2first,,,,,,,,,,,
XDeepFM,选用的原因？,second2question,,,,,,,,,,,
XDeepFM,什么叫显示隐式？什么叫元素级/向量级？什么叫做高阶/低阶特征交互？,second2question,,,,,,,,,,,
XDeepFM,简单介绍一下XDeepFm的思想？,second2question,,,,,,,,,,,
XDeepFM,和DCN比，有哪些核心的变化？,second2question,,,,,,,,,,,
XDeepFM,时间复杂度多少？,second2question,,,,,,,,,,,
推荐,YouTubeNet,root2first,,,,,,,,,,,
YouTubeNet,变长数据如何处理的？,second2question,,,,,,,,,,,
YouTubeNet,input是怎么构造的,second2question,,,,,,,,,,,
YouTubeNet,最后一次点击实际如何处理的？,second2question,,,,,,,,,,,
YouTubeNet,output的是时候train和predict如何处理的,second2question,,,,,,,,,,,
YouTubeNet,如何进行负采样的？,second2question,,,,,,,,,,,
YouTubeNet,item向量在softmax的时候你们怎么选择的？,second2question,,,,,,,,,,,
YouTubeNet,Example Age的理解？,second2question,,,,,,,,,,,
YouTubeNet,什么叫做不对称的共同浏览（asymmetric co-watch）问题？,second2question,,,,,,,,,,,
YouTubeNet,为什么不采取类似RNN的Sequence model？,second2question,,,,,,,,,,,
YouTubeNet,YouTube如何避免百万量级的softmax问题的？,second2question,,,,,,,,,,,
YouTubeNet,serving过程中，YouTube为什么不直接采用训练时的model进行预测，而是采用了一种最近邻搜索的方法？,second2question,,,,,,,,,,,
YouTubeNet,Youtube的用户对新视频有偏好，那么在模型构建的过程中如何引入这个feature？,second2question,,,,,,,,,,,
YouTubeNet,在处理测试集的时候，YouTube为什么不采用经典的随机留一法（random holdout），而是一定要把用户最近的一次观看行为作为测试集？,second2question,,,,,,,,,,,
YouTubeNet,整个过程中有什么亮点？有哪些决定性的提升？,second2question,,,,,,,,,,,
数学,gcd,root2first,,,,,,,,,,,
gcd,辗转相除法,second2question,,,,,,,,,,,
gcd,其他方法,second2question,,,,,,,,,,,
数学,导数,root2first,,,,,,,,,,,
导数,四则运算,second2question,,,,,,,,,,,
导数,常见导数,second2question,,,,,,,,,,,
导数,复合函数的运算法则,second2question,,,,,,,,,,,
导数,莱布尼兹公式,second2question,,,,,,,,,,,
数学,平面曲线的切线和法线,root2first,,,,,,,,,,,
平面曲线的切线和法线,切线方程,second2question,,,,,,,,,,,
平面曲线的切线和法线,法线方程,second2question,,,,,,,,,,,
数学,微分中值定理,root2first,,,,,,,,,,,
微分中值定理,费马定理,second2question,,,,,,,,,,,
微分中值定理,拉格朗日中值定理,second2question,,,,,,,,,,,
微分中值定理,柯西中值定理,second2question,,,,,,,,,,,
数学,期望、方差、标准差和协方差,root2first,,,,,,,,,,,
期望、方差、标准差和协方差,期望,second2question,,,,,,,,,,,
期望、方差、标准差和协方差,方差,second2question,,,,,,,,,,,
期望、方差、标准差和协方差,标准差,second2question,,,,,,,,,,,
期望、方差、标准差和协方差,协方差,second2question,,,,,,,,,,,
期望、方差、标准差和协方差,相关系数,second2question,,,,,,,,,,,
数学,概率密度分布,root2first,,,,,,,,,,,
概率密度分布,均匀分布,second2question,,,,,,,,,,,
概率密度分布,伯努利分布,second2question,,,,,,,,,,,
概率密度分布,二项分布,second2question,,,,,,,,,,,
概率密度分布,高斯分布,second2question,,,,,,,,,,,
概率密度分布,拉普拉斯分布,second2question,,,,,,,,,,,
概率密度分布,泊松分布,second2question,,,,,,,,,,,
数学,概率论,root2first,,,,,,,,,,,
概率论,条件概率,second2question,,,,,,,,,,,
概率论,独立,second2question,,,,,,,,,,,
概率论,概率基础公式,second2question,,,,,,,,,,,
概率论,全概率：,second2question,,,,,,,,,,,
概率论,贝叶斯,second2question,,,,,,,,,,,
概率论,切比雪夫不等式,second2question,,,,,,,,,,,
概率论,抽球,second2question,,,,,,,,,,,
概率论,纸牌问题,second2question,,,,,,,,,,,
概率论,棍子/绳子问题,second2question,,,,,,,,,,,
概率论,贝叶斯,second2question,,,,,,,,,,,
概率论,选择时间问题,second2question,,,,,,,,,,,
概率论,0~1均匀分布的随机器如何变化成均值为0，方差为1的随机器,second2question,,,,,,,,,,,
概率论,抽红蓝球球,second2question,,,,,,,,,,,
数学,欧拉公式,root2first,,,,,,,,,,,
数学,泰勒公式,root2first,,,,,,,,,,,
泰勒公式,泰勒公式,second2question,,,,,,,,,,,
泰勒公式,常见泰勒公式,second2question,,,,,,,,,,,
数学,牛顿法,root2first,,,,,,,,,,,
牛顿法,迭代公式推导,second2question,,,,,,,,,,,
牛顿法,实现它,second2question,,,,,,,,,,,
数学,矩阵,root2first,,,,,,,,,,,
矩阵,范数,second2question,,,,,,,,,,,
矩阵,特征值分解，特征向量,second2question,,,,,,,,,,,
矩阵,正定性,second2question,,,,,,,,,,,
数据预处理,异常点识别,root2first,,,,,,,,,,,
异常点识别,统计方法,second2question,,,,,,,,,,,
异常点识别,矩阵分解方法,second2question,,,,,,,,,,,
异常点识别,特征值和特征向量的本质是什么？,second2question,,,,,,,,,,,
异常点识别,矩阵乘法的实际意义？,second2question,,,,,,,,,,,
异常点识别,密度的离群点检测,second2question,,,,,,,,,,,
异常点识别,聚类的离群点检测,second2question,,,,,,,,,,,
异常点识别,如何处理异常点？,second2question,,,,,,,,,,,
数据预处理,数据平衡,root2first,,,,,,,,,,,
数据平衡,为什么要对数据进行采样平衡,second2question,,,,,,,,,,,
数据平衡,是否一定需要对原始数据进行采样平衡,second2question,,,,,,,,,,,
数据平衡,有哪些常见的采样方法？,second2question,,,,,,,,,,,
数据平衡,能否避免采样？,second2question,,,,,,,,,,,
数据平衡,你平时怎么用采样方法？,second2question,,,,,,,,,,,
数据预处理,特征提取,root2first,,,,,,,,,,,
特征提取,为什么需要对数据进行变换？,second2question,,,,,,,,,,,
特征提取,归一化和标准化之间的关系？,second2question,,,,,,,,,,,
特征提取,连续特征常用方法,second2question,,,,,,,,,,,
特征提取,离散特征常用方法,second2question,,,,,,,,,,,
特征提取,文本特征,second2question,,,,,,,,,,,
特征提取,画一个最简单的最快速能实现的框架,second2question,,,,,,,,,,,
数据预处理,特征选择,root2first,,,,,,,,,,,
特征选择,为什么要做特征选择？,second2question,,,,,,,,,,,
特征选择,从哪些方面可以做特征选择？,second2question,,,,,,,,,,,
特征选择,既然说了两个方向，分别介绍一些吧,second2question,,,,,,,,,,,
数据预处理,缺失值处理,root2first,,,,,,,,,,,
缺失值处理,是不是一定需要对缺失值处理？,second2question,,,,,,,,,,,
缺失值处理,直接填充方法有哪些？,second2question,,,,,,,,,,,
缺失值处理,模型插值方法有哪些？及方法的问题,second2question,,,,,,,,,,,
缺失值处理,如何直接离散化？,second2question,,,,,,,,,,,
缺失值处理,hold位填充方法有哪些？,second2question,,,,,,,,,,,
缺失值处理,怎么理解分布补全？,second2question,,,,,,,,,,,
缺失值处理,random方法,second2question,,,,,,,,,,,
缺失值处理,总结,second2question,,,,,,,,,,,
机器学习,决策树,root2first,,,,,,,,,,,
决策树,常见决策树,second2question,,,,,,,,,,,
决策树,简述决策树构建过程,second2question,,,,,,,,,,,
决策树,详述信息熵计算方法及存在问题,second2question,,,,,,,,,,,
决策树,详述信息增益计算方法,second2question,,,,,,,,,,,
决策树,详述信息增益率计算方法,second2question,,,,,,,,,,,
决策树,解释Gini系数,second2question,,,,,,,,,,,
决策树,ID3存在的问题,second2question,,,,,,,,,,,
决策树,C4.5相对于ID3的改进点,second2question,,,,,,,,,,,
决策树,CART的连续特征改进点,second2question,,,,,,,,,,,
决策树,CART分类树建立算法的具体流程,second2question,,,,,,,,,,,
决策树,CART回归树建立算法的具体流程,second2question,,,,,,,,,,,
决策树,CART输出结果的逻辑？,second2question,,,,,,,,,,,
决策树,CART树算法的剪枝过程是怎么样的？,second2question,,,,,,,,,,,
决策树,树形结构为何不需要归一化？,second2question,,,,,,,,,,,
决策树,决策树的优缺点,second2question,,,,,,,,,,,
机器学习,支持向量机,root2first,,,,,,,,,,,
支持向量机,简单介绍SVM?,second2question,,,,,,,,,,,
支持向量机,什么叫最优超平面？,second2question,,,,,,,,,,,
支持向量机,什么是支持向量？,second2question,,,,,,,,,,,
支持向量机,SVM 和全部数据有关还是和局部数据有关?,second2question,,,,,,,,,,,
支持向量机,加大训练数据量一定能提高SVM准确率吗？,second2question,,,,,,,,,,,
支持向量机,如何解决多分类问题？,second2question,,,,,,,,,,,
支持向量机,可以做回归吗，怎么做？,second2question,,,,,,,,,,,
支持向量机,SVM 能解决哪些问题？,second2question,,,,,,,,,,,
支持向量机,介绍一下你知道的不同的SVM分类器？,second2question,,,,,,,,,,,
支持向量机,什么叫软间隔？,second2question,,,,,,,,,,,
支持向量机,SVM 软间隔与硬间隔表达式,second2question,,,,,,,,,,,
支持向量机,SVM原问题和对偶问题的关系/解释原问题和对偶问题？,second2question,,,,,,,,,,,
支持向量机,为什么要把原问题转换为对偶问题？,second2question,,,,,,,,,,,
支持向量机,为什么求解对偶问题更加高效？,second2question,,,,,,,,,,,
支持向量机,alpha系数有多少个？,second2question,,,,,,,,,,,
支持向量机,KKT限制条件，KKT条件有哪些，完整描述,second2question,,,,,,,,,,,
支持向量机,引入拉格朗日的优化方法后的损失函数解释,second2question,,,,,,,,,,,
支持向量机,核函数的作用是啥,second2question,,,,,,,,,,,
支持向量机,核函数的种类和应用场景,second2question,,,,,,,,,,,
支持向量机,如何选择核函数,second2question,,,,,,,,,,,
支持向量机,常用核函数的定义？,second2question,,,,,,,,,,,
支持向量机,核函数需要满足什么条件？,second2question,,,,,,,,,,,
支持向量机,为什么在数据量大的情况下常常用lr代替核SVM？,second2question,,,,,,,,,,,
支持向量机,高斯核可以升到多少维？为什么,second2question,,,,,,,,,,,
支持向量机,SVM和逻辑斯特回归对同一样本A进行训练，如果某类中增加一些数据点，那么原来的决策边界分别会怎么变化？,second2question,,,,,,,,,,,
支持向量机,"各种机器学习的应用场景分别是什么？例如，k近邻,贝叶斯，决策树，svm，逻辑斯蒂回归",second2question,,,,,,,,,,,
支持向量机,Linear SVM 和 LR 有什么异同？,second2question,,,,,,,,,,,
机器学习,线性回归,root2first,,,,,,,,,,,
线性回归,损失函数是啥,second2question,,,,,,,,,,,
线性回归,最小二乘/梯度下降手推,second2question,,,,,,,,,,,
线性回归,介绍一下岭回归,second2question,,,,,,,,,,,
线性回归,什么时候使用岭回归？,second2question,,,,,,,,,,,
线性回归,什么时候用Lasso回归？,second2question,,,,,,,,,,,
机器学习,聚类,root2first,,,,,,,,,,,
聚类,请问从EM角度理解kmeans?,second2question,,,,,,,,,,,
聚类,为什么kmeans一定会收敛?,second2question,,,,,,,,,,,
聚类,kmeans初始点除了随机选取之外的方法？,second2question,,,,,,,,,,,
机器学习,贝叶斯,root2first,,,,,,,,,,,
贝叶斯,解释一下朴素贝叶斯中考虑到的条件独立假设,second2question,,,,,,,,,,,
贝叶斯,讲一讲你眼中的贝叶斯公式和朴素贝叶斯分类差别,second2question,,,,,,,,,,,
贝叶斯,朴素贝叶斯中出现的常见模型有哪些,second2question,,,,,,,,,,,
贝叶斯,出现估计概率值为 0 怎么处理,second2question,,,,,,,,,,,
贝叶斯,朴素贝叶斯的优缺点？,second2question,,,,,,,,,,,
贝叶斯,朴素贝叶斯与 LR 区别？,second2question,,,,,,,,,,,
机器学习,逻辑回归,root2first,,,,,,,,,,,
逻辑回归,logistic分布函数和密度函数，手绘大概的图像,second2question,,,,,,,,,,,
逻辑回归,LR推导，基础5连问,second2question,,,,,,,,,,,
逻辑回归,梯度下降如何并行化？,second2question,,,,,,,,,,,
逻辑回归,LR明明是分类模型为什么叫回归？,second2question,,,,,,,,,,,
逻辑回归,为什么LR可以用来做CTR预估？,second2question,,,,,,,,,,,
逻辑回归,满足什么样条件的数据用LR最好？,second2question,,,,,,,,,,,
逻辑回归,LR为什么使用sigmoid函数作为激活函数？其他函数不行吗？,second2question,,,,,,,,,,,
逻辑回归,利用几率odds的意义在哪？,second2question,,,,,,,,,,,
逻辑回归,Sigmoid函数到底起了什么作用？,second2question,,,,,,,,,,,
逻辑回归,LR为什么要使用极大似然函数，交互熵作为损失函数？那为什么不选平方损失函数的呢,second2question,,,,,,,,,,,
逻辑回归,LR中若标签为+1和-1，损失函数如何推导？,second2question,,,,,,,,,,,
逻辑回归,如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？，为什么要避免共线性？,second2question,,,,,,,,,,,
逻辑回归,LR可以用核么？可以怎么用？,second2question,,,,,,,,,,,
逻辑回归,LR中的L1/L2正则项是啥？,second2question,,,,,,,,,,,
逻辑回归,lr加l1还是l2好？,second2question,,,,,,,,,,,
逻辑回归,正则化是依据什么理论实现模型优化？,second2question,,,,,,,,,,,
逻辑回归,LR可以用来处理非线性问题么？,second2question,,,,,,,,,,,
逻辑回归,为什么LR需要归一化或者取对数?,second2question,,,,,,,,,,,
逻辑回归,为什么LR把特征离散化后效果更好？离散化的好处有哪些？,second2question,,,,,,,,,,,
逻辑回归,逻辑回归估计参数时的目标函数逻辑回归的值表示概率吗？,second2question,,,,,,,,,,,
逻辑回归,LR对比万物？,second2question,,,,,,,,,,,
逻辑回归,LR梯度下降方法？,second2question,,,,,,,,,,,
逻辑回归,LR的优缺点？,second2question,,,,,,,,,,,
逻辑回归,除了做分类，你还会用LR做什么？,second2question,,,,,,,,,,,
逻辑回归,你有用过sklearn中的lr么？你用的是哪个包？,second2question,,,,,,,,,,,
逻辑回归,看过源码么？为什么去看？,second2question,,,,,,,,,,,
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中的penalty和solver的选择？,second2question,,,,,,,,,,,
逻辑回归,谈一下sklearn.linear_model.LogisticRegression中对多分类是怎么处理的？,second2question,,,,,,,,,,,
逻辑回归,我的总结,second2question,,,,,,,,,,,
机器学习,随机森林,root2first,,,,,,,,,,,
随机森林,解释下随机森林?,second2question,,,,,,,,,,,
随机森林,随机森林用的是什么树？,second2question,,,,,,,,,,,
随机森林,随机森林的生成过程？,second2question,,,,,,,,,,,
随机森林,解释下随机森林节点的分裂策略？,second2question,,,,,,,,,,,
随机森林,随机森林的损失函数是什么？,second2question,,,,,,,,,,,
随机森林,为了防止随机森林过拟合可以怎么做?,second2question,,,,,,,,,,,
随机森林,随机森林特征选择的过程？,second2question,,,,,,,,,,,
随机森林,是否用过随机森林，有什么技巧?,second2question,,,,,,,,,,,
随机森林,RF的参数有哪些，如何调参？,second2question,,,,,,,,,,,
随机森林,RF的优缺点 ？,second2question,,,,,,,,,,,
机器学习,集成学习,root2first,,,,,,,,,,,
集成学习,GBDT,first2second,,,,,,,,,,,
GBDT,介绍一下Boosting的思想？,second2question,,,,,,,,,,,
GBDT,最小二乘回归树的切分过程是怎么样的？,second2question,,,,,,,,,,,
GBDT,有哪些直接利用了Boosting思想的树模型？,second2question,,,,,,,,,,,
GBDT,gbdt和boostingtree的boosting分别体现在哪里？,second2question,,,,,,,,,,,
GBDT,gbdt的中的tree是什么tree？有什么特征？,second2question,,,,,,,,,,,
GBDT,常用回归问题的损失函数？,second2question,,,,,,,,,,,
GBDT,常用分类问题的损失函数？,second2question,,,,,,,,,,,
GBDT,什么是gbdt中的损失函数的负梯度？,second2question,,,,,,,,,,,
GBDT,如何用损失函数的负梯度实现gbdt？,second2question,,,,,,,,,,,
GBDT,拟合损失函数的负梯度为什么是可行的？,second2question,,,,,,,,,,,
GBDT,即便拟合损失函数负梯度是可行的，为什么不直接拟合残差？ 拟合负梯度好在哪里？,second2question,,,,,,,,,,,
GBDT,Shrinkage收缩的作用？,second2question,,,,,,,,,,,
GBDT,feature属性会被重复多次使用么？,second2question,,,,,,,,,,,
GBDT,gbdt如何进行正则化的？,second2question,,,,,,,,,,,
GBDT,为什么集成算法大多使用树类模型作为基学习器？或者说，为什么集成学习可以在树类模型上取得成功？,second2question,,,,,,,,,,,
GBDT,gbdt的优缺点？,second2question,,,,,,,,,,,
GBDT,gbdt和randomforest区别？,second2question,,,,,,,,,,,
GBDT,GBDT和LR的差异？,second2question,,,,,,,,,,,
集成学习,LightGBM,first2second,,,,,,,,,,,
LightGBM,XGboost缺点,second2question,,,,,,,,,,,
LightGBM,LightGBM对Xgboost的优化,second2question,,,,,,,,,,,
LightGBM,LightGBM亮点,second2question,,,,,,,,,,,
集成学习,Xgboost,first2second,,,,,,,,,,,
Xgboost,xgboost对比gbdt/boosting Tree有了哪些方向上的优化？,second2question,,,,,,,,,,,
Xgboost,xgboost和gbdt的区别？,second2question,,,,,,,,,,,
Xgboost,xgboost优化目标/损失函数改变成什么样？,second2question,,,,,,,,,,,
Xgboost,xgboost如何使用MAE或MAPE作为目标函数？,second2question,,,,,,,,,,,
Xgboost,xgboost如何寻找分裂节点的候选集？,second2question,,,,,,,,,,,
Xgboost,xgboost如何处理缺失值？,second2question,,,,,,,,,,,
Xgboost,xgboost在计算速度上有了哪些点上提升？,second2question,,,,,,,,,,,
Xgboost,xgboost特征重要性是如何得到的？,second2question,,,,,,,,,,,
Xgboost,XGBoost中如何对树进行剪枝？,second2question,,,,,,,,,,,
Xgboost,XGBoost模型如果过拟合了怎么解决？,second2question,,,,,,,,,,,
Xgboost,xgboost如何调参数？,second2question,,,,,,,,,,,
深度学习,Attention,root2first,,,,,,,,,,,
Attention,Attention对比RNN和CNN，分别有哪点你觉得的优势？,second2question,,,,,,,,,,,
Attention,写出Attention的公式？,second2question,,,,,,,,,,,
Attention,解释你怎么理解Attention的公式的？,second2question,,,,,,,,,,,
Attention,Attention模型怎么避免词袋模型的顺序问题的困境的？,second2question,,,,,,,,,,,
Attention,"Attention机制，里面的q,k,v分别代表什么？",second2question,,,,,,,,,,,
Attention,为什么self-attention可以替代seq2seq？,second2question,,,,,,,,,,,
Attention,维度与点积大小的关系是怎么样的，为什么使用维度的根号来放缩?,second2question,,,,,,,,,,,
深度学习,batch_normalization,root2first,,,,,,,,,,,
batch_normalization,你觉得bn过程是什么样的？,second2question,,,,,,,,,,,
batch_normalization,手写一下bn过程？,second2question,,,,,,,,,,,
batch_normalization,知道LN么？讲讲原理,second2question,,,,,,,,,,,
深度学习,残差网络,root2first,,,,,,,,,,,
残差网络,介绍残差网络,second2question,,,,,,,,,,,
残差网络,残差网络为什么能解决梯度消失的问题,second2question,,,,,,,,,,,
残差网络,残差网络残差作用,second2question,,,,,,,,,,,
残差网络,你平时有用过么？或者你在哪些地方遇到了,second2question,,,,,,,,,,,
自然语言处理,Bert,root2first,,,,,,,,,,,
Bert,Bert的双向体现在什么地方？,second2question,,,,,,,,,,,
Bert,Bert的是怎样实现mask构造的？,second2question,,,,,,,,,,,
Bert,在数据中随机选择 15% 的标记，其中80%被换位\[mask]，10%不变、10%随机替换其他单词，这样做的原因是什么？,second2question,,,,,,,,,,,
Bert,为什么BERT有3个嵌入层，它们都是如何实现的？,second2question,,,,,,,,,,,
Bert,bert的损失函数？,second2question,,,,,,,,,,,
Bert,手写一个multi-head attention？,second2question,,,,,,,,,,,
Bert,长文本预测如何构造Tokens？,second2question,,,,,,,,,,,
Bert,你用过什么模块？bert流程是怎么样的？,second2question,,,,,,,,,,,
Bert,知道分词模块：FullTokenizer做了哪些事情么？,second2question,,,,,,,,,,,
Bert,Bert中如何获得词意和句意？,second2question,,,,,,,,,,,
Bert,源码中Attention后实际的流程是如何的？,second2question,,,,,,,,,,,
Bert,为什么要在Attention后使用残差结构？,second2question,,,,,,,,,,,
Bert,平时用官方Bert包么？耗时怎么样？,second2question,,,,,,,,,,,
Bert,你觉得BERT比普通LM的新颖点？,second2question,,,,,,,,,,,
Bert,elmo、GPT、bert三者之间有什么区别？,second2question,,,,,,,,,,,
自然语言处理,CRF,root2first,,,,,,,,,,,
CRF,阐述CRF原理？,second2question,,,,,,,,,,,
CRF,线性链条件随机场的公式是？,second2question,,,,,,,,,,,
CRF,CRF与HMM区别?,second2question,,,,,,,,,,,
CRF,Bert+crf中的各部分作用详解？,second2question,,,,,,,,,,,
自然语言处理,GloVe,root2first,,,,,,,,,,,
GloVe,GolVe的损失函数？,second2question,,,,,,,,,,,
GloVe,解释GolVe的损失函数？,second2question,,,,,,,,,,,
GloVe,为什么GolVe会用的相对比W2V少？,second2question,,,,,,,,,,,
GloVe,如何处理未出现词？,second2question,,,,,,,,,,,
自然语言处理,LDA,root2first,,,,,,,,,,,
LDA,详述LDA原理？,second2question,,,,,,,,,,,
LDA,LDA中的主题矩阵如何计算?词分布矩阵如何计算？,second2question,,,,,,,,,,,
LDA,LDA的共轭分布解释下?,second2question,,,,,,,,,,,
LDA,PLSA和LDA的区别?,second2question,,,,,,,,,,,
LDA,怎么确定LDA的topic个数,second2question,,,,,,,,,,,
LDA,LDA和Word2Vec区别？LDA和Doc2Vec区别？,second2question,,,,,,,,,,,
LDA,LDA算法里面Dirichlet分布的两个参数alpha和beta怎样确定？trick?,second2question,,,,,,,,,,,
自然语言处理,Word2Vec,root2first,,,,,,,,,,,
Word2Vec,从隐藏层到输出的Softmax层的计算有哪些方法？,second2question,,,,,,,,,,,
Word2Vec,层次softmax流程？,second2question,,,,,,,,,,,
Word2Vec,负采样流程？,second2question,,,,,,,,,,,
Word2Vec,word2vec两种方法各自的优势?,second2question,,,,,,,,,,,
Word2Vec,怎么衡量学到的embedding的好坏?,second2question,,,,,,,,,,,
Word2Vec,word2vec和glove区别？,second2question,,,,,,,,,,,
Word2Vec,你觉得word2vec有哪些问题？,second2question,,,,,,,,,,,
